{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Regime Prediction - Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from model import MarketRegimeTransformer\n",
    "import psutil  # For memory monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "DATA_DIR = 'data/training'\n",
    "LOOKBACK_WINDOW = 256  # ~1 year of trading data\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 115\n",
    "NUM_EPOCHS = 80\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_WORKERS = 10  # Optimal for data loading without overloading system\n",
    "\n",
    "# Loss weights\n",
    "CLASS_WEIGHT = 1.0\n",
    "DURATION_WEIGHT_START = 0.05  # Gradually increase from this\n",
    "DURATION_WEIGHT_END = 0.4     # To this value\n",
    "\n",
    "# Model parameters\n",
    "N_FEATURES = 7  # OHLCV + RSI + BB\n",
    "D_MODEL = 256\n",
    "N_HEAD = 8\n",
    "NUM_FEATURE_LAYERS = 4\n",
    "NUM_AGGREGATE_LAYERS = 3\n",
    "DIM_FEEDFORWARD = 1024\n",
    "DROPOUT = 0.1\n",
    "NUM_CLASSES = 3  # bull, flat, bear\n",
    "\n",
    "# Scheduler parameters\n",
    "WARMUP_EPOCHS = 10\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "Memory Usage:\n",
      "  Allocated: 0.0 GB\n",
      "  Cached: 0.0 GB\n",
      "  Total VRAM: 24.0 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Usage:\")\n",
    "    print(f\"  Allocated: {round(torch.cuda.memory_allocated(0)/1024**3,1)} GB\")\n",
    "    print(f\"  Cached: {round(torch.cuda.memory_reserved(0)/1024**3,1)} GB\")\n",
    "    print(f\"  Total VRAM: {round(torch.cuda.get_device_properties(0).total_memory/1024**3,1)} GB\")\n",
    "    \n",
    "    # Enable cuDNN optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalfLoadedMarketRegimeDataset(Dataset):\n",
    "    def __init__(self, data_dir, subset='train', lookback_window=256, half='first'):\n",
    "        \"\"\"\n",
    "        Load half of the dataset into RAM at a time\n",
    "        Args:\n",
    "            data_dir: Base directory containing train/val subdirectories\n",
    "            subset: 'train' or 'val'\n",
    "            lookback_window: The lookback window size (for directory naming)\n",
    "            half: 'first' or 'second' - which half of files to load\n",
    "        \"\"\"\n",
    "        self.data_dir = os.path.join(data_dir, subset, f'n{lookback_window}')\n",
    "        self.subset = subset\n",
    "        self.half = half\n",
    "        \n",
    "        # Find all .npz files\n",
    "        all_files = sorted([f for f in os.listdir(self.data_dir) if f.endswith('.npz')])\n",
    "        if not all_files:\n",
    "            raise ValueError(f\"No .npz files found in {self.data_dir}\")\n",
    "        \n",
    "        # Split files into two halves\n",
    "        mid_point = len(all_files) // 2\n",
    "        if half == 'first':\n",
    "            self.files = all_files[:mid_point]\n",
    "        else:\n",
    "            self.files = all_files[mid_point:]\n",
    "        \n",
    "        print(f\"Loading {half} half: {len(self.files)} files from {subset} set...\")\n",
    "        \n",
    "        # Load all data from this half into memory\n",
    "        self.data_X = []\n",
    "        self.data_y_class = []\n",
    "        self.data_y_duration = []\n",
    "        \n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        initial_memory = process.memory_info().rss / 1024**3  # GB\n",
    "        \n",
    "        for file_idx, file in enumerate(self.files):\n",
    "            filepath = os.path.join(self.data_dir, file)\n",
    "            with np.load(filepath) as data:\n",
    "                self.data_X.append(data['X'].astype(np.float32))\n",
    "                self.data_y_class.append(data['y_class'].astype(np.int64))\n",
    "                self.data_y_duration.append(data['y_duration'].astype(np.float32))\n",
    "            \n",
    "            if (file_idx + 1) % 50 == 0:\n",
    "                current_memory = process.memory_info().rss / 1024**3\n",
    "                print(f\"  Loaded {file_idx + 1}/{len(self.files)} files, \"\n",
    "                      f\"Memory: {current_memory:.1f} GB (+{current_memory - initial_memory:.1f} GB)\")\n",
    "        \n",
    "        # Concatenate all data\n",
    "        self.data_X = np.concatenate(self.data_X, axis=0)\n",
    "        self.data_y_class = np.concatenate(self.data_y_class, axis=0)\n",
    "        self.data_y_duration = np.concatenate(self.data_y_duration, axis=0)\n",
    "        \n",
    "        self.total_samples = len(self.data_X)\n",
    "        \n",
    "        final_memory = process.memory_info().rss / 1024**3\n",
    "        print(f\"Loaded {self.total_samples} samples. Memory usage: {final_memory:.1f} GB \"\n",
    "              f\"(+{final_memory - initial_memory:.1f} GB)\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Direct indexing from memory - very fast\n",
    "        return (\n",
    "            torch.from_numpy(self.data_X[idx]),\n",
    "            torch.tensor(self.data_y_class[idx], dtype=torch.long),\n",
    "            torch.tensor(self.data_y_duration[idx], dtype=torch.float32)\n",
    "        )\n",
    "    \n",
    "    def switch_half(self):\n",
    "        \"\"\"Switch to the other half of the data\"\"\"\n",
    "        # Clear current data\n",
    "        del self.data_X\n",
    "        del self.data_y_class\n",
    "        del self.data_y_duration\n",
    "        \n",
    "        # Switch half\n",
    "        self.half = 'second' if self.half == 'first' else 'first'\n",
    "        \n",
    "        # Reload\n",
    "        self.__init__(\n",
    "            self.data_dir.rsplit('/', 2)[0],  # Get base data_dir\n",
    "            self.subset,\n",
    "            int(self.data_dir.split('n')[-1]),  # Extract lookback_window\n",
    "            self.half\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders_half_loaded(data_dir, batch_size, num_workers, lookback_window):\n",
    "    \"\"\"Create data loaders that load half the data at a time\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Base data directory\n",
    "        batch_size: Batch size for training\n",
    "        num_workers: Number of worker processes\n",
    "        lookback_window: Lookback window size\n",
    "    \"\"\"\n",
    "    train_dir = os.path.join(data_dir, 'train', f'n{lookback_window}')\n",
    "    val_dir = os.path.join(data_dir, 'val', f'n{lookback_window}')\n",
    "    \n",
    "    if os.path.exists(train_dir) and os.path.exists(val_dir):\n",
    "        print(\"Creating half-loaded data loaders...\")\n",
    "        \n",
    "        # Start with first half of training data\n",
    "        train_dataset = HalfLoadedMarketRegimeDataset(\n",
    "            data_dir, subset='train', lookback_window=lookback_window, \n",
    "            half='first'\n",
    "        )\n",
    "        \n",
    "        # Validation can load all at once (it's smaller)\n",
    "        val_dataset = HalfLoadedMarketRegimeDataset(\n",
    "            data_dir, subset='val', lookback_window=lookback_window, \n",
    "            half='first'  # Val only has 43 files, load all\n",
    "        )\n",
    "        \n",
    "        # Create data loaders with fewer workers since data is in RAM\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,  # Shuffle within the half\n",
    "            num_workers=2,  # Few workers needed since data is in RAM\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size * 2,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader, train_dataset, val_dataset\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Data directories not found.\")\n",
    "\n",
    "\n",
    "def train_epoch_with_halves(model, train_dataset, batch_size, criterion_class, criterion_duration, \n",
    "                            optimizer, device, class_weight, duration_weight, epoch):\n",
    "    \"\"\"Train for one epoch, switching data halves midway\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Train on both halves\n",
    "    for half in ['first', 'second']:\n",
    "        if half == 'second':\n",
    "            print(f\"  Switching to second half of data...\")\n",
    "            train_dataset.switch_half()\n",
    "            \n",
    "        # Create new dataloader for this half\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2\n",
    "        )\n",
    "        \n",
    "        print(f\"  Training on {half} half ({len(train_dataset)} samples)...\")\n",
    "        \n",
    "        for batch_idx, (data, target_class, target_duration) in enumerate(train_loader):\n",
    "            data = data.to(device, non_blocking=True)\n",
    "            target_class = target_class.to(device, non_blocking=True)\n",
    "            target_duration = target_duration.to(device, non_blocking=True).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            class_logits, duration_pred = model(data)\n",
    "            \n",
    "            loss_class = criterion_class(class_logits, target_class)\n",
    "            loss_duration = criterion_duration(duration_pred, target_duration)\n",
    "            \n",
    "            loss = class_weight * loss_class + duration_weight * loss_duration\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = class_logits.max(1)\n",
    "            total += target_class.size(0)\n",
    "            correct += predicted.eq(target_class).sum().item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'    Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f} '\n",
    "                      f'(Class: {loss_class.item():.4f}, Duration: {loss_duration.item():.4f})')\n",
    "    \n",
    "    # Calculate metrics for full epoch\n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = total_loss / (len(train_dataset) * 2 / batch_size)  # Approximate number of batches\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class_weights(data_dir, lookback_window, device):\n",
    "    \"\"\"Load class weights from training data statistics\"\"\"\n",
    "    try:\n",
    "        # Try to load from manifest file first\n",
    "        manifest_path = os.path.join(data_dir, 'train', f'n{lookback_window}', 'manifest.json')\n",
    "        if os.path.exists(manifest_path):\n",
    "            with open(manifest_path, 'r') as f:\n",
    "                manifest = json.load(f)\n",
    "                if 'class_distribution' in manifest:\n",
    "                    class_dist = manifest['class_distribution']\n",
    "                    total = sum(class_dist.values())\n",
    "                    class_pcts = {\n",
    "                        'bull': class_dist['bull'] / total * 100,\n",
    "                        'flat': class_dist['flat'] / total * 100,\n",
    "                        'bear': class_dist['bear'] / total * 100\n",
    "                    }\n",
    "        else:\n",
    "            # Calculate from data if manifest doesn't exist\n",
    "            print(\"Calculating class weights from training data...\")\n",
    "            train_dir = os.path.join(data_dir, 'train', f'n{lookback_window}')\n",
    "            class_counts = {'bull': 0, 'flat': 0, 'bear': 0}\n",
    "            \n",
    "            for file in os.listdir(train_dir):\n",
    "                if file.endswith('.npz'):\n",
    "                    filepath = os.path.join(train_dir, file)\n",
    "                    with np.load(filepath) as data:\n",
    "                        y_class = data['y_class']\n",
    "                        class_counts['bull'] += np.sum(y_class == 0)\n",
    "                        class_counts['flat'] += np.sum(y_class == 1)\n",
    "                        class_counts['bear'] += np.sum(y_class == 2)\n",
    "            \n",
    "            total = sum(class_counts.values())\n",
    "            class_pcts = {k: v / total * 100 for k, v in class_counts.items()}\n",
    "        \n",
    "        # Calculate weights inversely proportional to class frequency\n",
    "        weights = torch.tensor([\n",
    "            100.0 / class_pcts['bull'],\n",
    "            100.0 / class_pcts['flat'],\n",
    "            100.0 / class_pcts['bear']\n",
    "        ]).to(device)\n",
    "        \n",
    "        # Normalize weights\n",
    "        weights = weights / weights.mean()\n",
    "        \n",
    "        print(f\"Loaded class weights from training data:\")\n",
    "        print(f\"  Bull: {weights[0]:.3f} ({class_pcts['bull']:.1f}% of data)\")\n",
    "        print(f\"  Flat: {weights[1]:.3f} ({class_pcts['flat']:.1f}% of data)\")\n",
    "        print(f\"  Bear: {weights[2]:.3f} ({class_pcts['bear']:.1f}% of data)\")\n",
    "        \n",
    "        return weights\n",
    "    except Exception as e:\n",
    "        # Default weights if calculation fails\n",
    "        print(f\"Error calculating class weights: {e}\")\n",
    "        print(\"Using default class weights\")\n",
    "        return torch.tensor([0.760, 1.294, 0.991]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion_class, criterion_duration, optimizer, device, class_weight, duration_weight):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target_class, target_duration) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target_class = target_class.to(device)\n",
    "        target_duration = target_duration.to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        class_logits, duration_pred = model(data)\n",
    "        \n",
    "        loss_class = criterion_class(class_logits, target_class)\n",
    "        loss_duration = criterion_duration(duration_pred, target_duration)\n",
    "        \n",
    "        loss = class_weight * loss_class + duration_weight * loss_duration\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = class_logits.max(1)\n",
    "        total += target_class.size(0)\n",
    "        correct += predicted.eq(target_class).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'  Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f} '\n",
    "                  f'(Class: {loss_class.item():.4f}, Duration: {loss_duration.item():.4f})')\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion_class, criterion_duration, device, class_weight, duration_weight):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = {0: 0, 1: 0, 2: 0}\n",
    "    class_total = {0: 0, 1: 0, 2: 0}\n",
    "    duration_mse = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target_class, target_duration in val_loader:\n",
    "            data = data.to(device)\n",
    "            target_class = target_class.to(device)\n",
    "            target_duration = target_duration.to(device).unsqueeze(1)\n",
    "            \n",
    "            class_logits, duration_pred = model(data)\n",
    "            \n",
    "            loss_class = criterion_class(class_logits, target_class)\n",
    "            loss_duration = criterion_duration(duration_pred, target_duration)\n",
    "            loss = class_weight * loss_class + duration_weight * loss_duration\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = class_logits.max(1)\n",
    "            total += target_class.size(0)\n",
    "            correct += predicted.eq(target_class).sum().item()\n",
    "            \n",
    "            # Track duration MSE\n",
    "            duration_mse += ((duration_pred - target_duration) ** 2).sum().item()\n",
    "            \n",
    "            for i in range(target_class.size(0)):\n",
    "                label = target_class[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == label:\n",
    "                    class_correct[label] += 1\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    avg_duration_mse = duration_mse / total\n",
    "    \n",
    "    class_accuracies = {\n",
    "        'bull': 100. * class_correct[0] / class_total[0] if class_total[0] > 0 else 0,\n",
    "        'flat': 100. * class_correct[1] / class_total[1] if class_total[1] > 0 else 0,\n",
    "        'bear': 100. * class_correct[2] / class_total[2] if class_total[2] > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return avg_loss, accuracy, class_accuracies, avg_duration_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data with half-loading strategy...\n",
      "Creating half-loaded data loaders...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.7 GB (+1.1 GB)\n",
      "  Loaded 100/255 files, Memory: 2.7 GB (+2.1 GB)\n",
      "  Loaded 150/255 files, Memory: 4.0 GB (+3.4 GB)\n",
      "  Loaded 200/255 files, Memory: 4.8 GB (+4.2 GB)\n",
      "  Loaded 250/255 files, Memory: 5.9 GB (+5.3 GB)\n",
      "Loaded 811636 samples. Memory usage: 7.2 GB (+6.7 GB)\n",
      "Loading first half: 21 files from val set...\n",
      "Loaded 96352 samples. Memory usage: 7.9 GB (+0.6 GB)\n",
      "Ready to train with data swapping between halves\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data with half-loading strategy...\")\n",
    "train_loader, val_loader, train_dataset, val_dataset = create_data_loaders_half_loaded(\n",
    "    DATA_DIR, \n",
    "    BATCH_SIZE,\n",
    "    NUM_WORKERS,\n",
    "    LOOKBACK_WINDOW\n",
    ")\n",
    "print(f\"Ready to train with data swapping between halves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating class weights: cannot access local variable 'class_pcts' where it is not associated with a value\n",
      "Using default class weights\n",
      "Model parameters: 25,011,460\n"
     ]
    }
   ],
   "source": [
    "# Load class weights\n",
    "class_weights = load_class_weights(DATA_DIR, LOOKBACK_WINDOW, DEVICE)\n",
    "\n",
    "# Initialize model\n",
    "model = MarketRegimeTransformer(\n",
    "    lookback_window=LOOKBACK_WINDOW-1,  # -1 because of pct_change\n",
    "    n_features=N_FEATURES,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=N_HEAD,\n",
    "    num_feature_layers=NUM_FEATURE_LAYERS,\n",
    "    num_aggregate_layers=NUM_AGGREGATE_LAYERS,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_class = nn.CrossEntropyLoss(weight=class_weights)\n",
    "criterion_duration = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Learning rate schedulers\n",
    "scheduler1 = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=WARMUP_EPOCHS)\n",
    "scheduler2 = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS-WARMUP_EPOCHS)\n",
    "scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[WARMUP_EPOCHS])\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs('checkpoints', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/80\n",
      "Duration weight: 0.050\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 1.8497 (Class: 1.0886, Duration: 15.2229)\n",
      "    Batch [100/7058] Loss: 1.3861 (Class: 1.1142, Duration: 5.4365)\n",
      "    Batch [200/7058] Loss: 1.4246 (Class: 1.1190, Duration: 6.1104)\n",
      "    Batch [300/7058] Loss: 1.3363 (Class: 1.0764, Duration: 5.1978)\n",
      "    Batch [400/7058] Loss: 1.3822 (Class: 1.0873, Duration: 5.8983)\n",
      "    Batch [500/7058] Loss: 1.2023 (Class: 0.9499, Duration: 5.0475)\n",
      "    Batch [600/7058] Loss: 1.1484 (Class: 0.9501, Duration: 3.9643)\n",
      "    Batch [700/7058] Loss: 1.1520 (Class: 0.9465, Duration: 4.1104)\n",
      "    Batch [800/7058] Loss: 1.1948 (Class: 0.9469, Duration: 4.9571)\n",
      "    Batch [900/7058] Loss: 1.1750 (Class: 0.9247, Duration: 5.0047)\n",
      "    Batch [1000/7058] Loss: 1.1362 (Class: 0.9055, Duration: 4.6146)\n",
      "    Batch [1100/7058] Loss: 1.1958 (Class: 0.9177, Duration: 5.5609)\n",
      "    Batch [1200/7058] Loss: 1.1682 (Class: 0.9130, Duration: 5.1038)\n",
      "    Batch [1300/7058] Loss: 1.0574 (Class: 0.8742, Duration: 3.6654)\n",
      "    Batch [1400/7058] Loss: 1.1947 (Class: 0.9305, Duration: 5.2837)\n",
      "    Batch [1500/7058] Loss: 1.1187 (Class: 0.8521, Duration: 5.3314)\n",
      "    Batch [1600/7058] Loss: 1.1504 (Class: 0.9031, Duration: 4.9461)\n",
      "    Batch [1700/7058] Loss: 1.1848 (Class: 0.8840, Duration: 6.0165)\n",
      "    Batch [1800/7058] Loss: 1.2785 (Class: 0.9868, Duration: 5.8342)\n",
      "    Batch [1900/7058] Loss: 1.1035 (Class: 0.9112, Duration: 3.8457)\n",
      "    Batch [2000/7058] Loss: 1.0388 (Class: 0.8132, Duration: 4.5123)\n",
      "    Batch [2100/7058] Loss: 1.0715 (Class: 0.8491, Duration: 4.4480)\n",
      "    Batch [2200/7058] Loss: 1.1994 (Class: 0.9633, Duration: 4.7204)\n",
      "    Batch [2300/7058] Loss: 1.1197 (Class: 0.9145, Duration: 4.1022)\n",
      "    Batch [2400/7058] Loss: 1.1171 (Class: 0.8667, Duration: 5.0091)\n",
      "    Batch [2500/7058] Loss: 1.0766 (Class: 0.8222, Duration: 5.0884)\n",
      "    Batch [2600/7058] Loss: 1.0675 (Class: 0.8531, Duration: 4.2879)\n",
      "    Batch [2700/7058] Loss: 1.1585 (Class: 0.9175, Duration: 4.8199)\n",
      "    Batch [2800/7058] Loss: 1.0347 (Class: 0.8290, Duration: 4.1144)\n",
      "    Batch [2900/7058] Loss: 1.1815 (Class: 0.9659, Duration: 4.3107)\n",
      "    Batch [3000/7058] Loss: 1.2329 (Class: 0.9222, Duration: 6.2148)\n",
      "    Batch [3100/7058] Loss: 1.1325 (Class: 0.9070, Duration: 4.5096)\n",
      "    Batch [3200/7058] Loss: 1.0396 (Class: 0.8129, Duration: 4.5337)\n",
      "    Batch [3300/7058] Loss: 1.2527 (Class: 1.0153, Duration: 4.7467)\n",
      "    Batch [3400/7058] Loss: 1.1275 (Class: 0.8854, Duration: 4.8411)\n",
      "    Batch [3500/7058] Loss: 1.1322 (Class: 0.9257, Duration: 4.1311)\n",
      "    Batch [3600/7058] Loss: 1.0768 (Class: 0.8418, Duration: 4.7010)\n",
      "    Batch [3700/7058] Loss: 1.0635 (Class: 0.8142, Duration: 4.9867)\n",
      "    Batch [3800/7058] Loss: 1.0995 (Class: 0.8598, Duration: 4.7940)\n",
      "    Batch [3900/7058] Loss: 1.1633 (Class: 0.8997, Duration: 5.2717)\n",
      "    Batch [4000/7058] Loss: 1.0970 (Class: 0.8199, Duration: 5.5432)\n",
      "    Batch [4100/7058] Loss: 1.0755 (Class: 0.8314, Duration: 4.8811)\n",
      "    Batch [4200/7058] Loss: 1.0758 (Class: 0.8631, Duration: 4.2535)\n",
      "    Batch [4300/7058] Loss: 1.1650 (Class: 0.9453, Duration: 4.3945)\n",
      "    Batch [4400/7058] Loss: 1.1142 (Class: 0.8282, Duration: 5.7206)\n",
      "    Batch [4500/7058] Loss: 1.0646 (Class: 0.7855, Duration: 5.5817)\n",
      "    Batch [4600/7058] Loss: 1.0883 (Class: 0.8401, Duration: 4.9628)\n",
      "    Batch [4700/7058] Loss: 1.0574 (Class: 0.8074, Duration: 4.9992)\n",
      "    Batch [4800/7058] Loss: 1.0648 (Class: 0.8391, Duration: 4.5142)\n",
      "    Batch [4900/7058] Loss: 1.0267 (Class: 0.8288, Duration: 3.9586)\n",
      "    Batch [5000/7058] Loss: 1.0642 (Class: 0.8593, Duration: 4.0986)\n",
      "    Batch [5100/7058] Loss: 1.0441 (Class: 0.7962, Duration: 4.9583)\n",
      "    Batch [5200/7058] Loss: 1.0353 (Class: 0.8012, Duration: 4.6815)\n",
      "    Batch [5300/7058] Loss: 1.0528 (Class: 0.8801, Duration: 3.4531)\n",
      "    Batch [5400/7058] Loss: 1.1791 (Class: 0.9302, Duration: 4.9770)\n",
      "    Batch [5500/7058] Loss: 0.9186 (Class: 0.7212, Duration: 3.9486)\n",
      "    Batch [5600/7058] Loss: 1.0848 (Class: 0.8741, Duration: 4.2142)\n",
      "    Batch [5700/7058] Loss: 1.1565 (Class: 0.9245, Duration: 4.6405)\n",
      "    Batch [5800/7058] Loss: 1.0439 (Class: 0.7739, Duration: 5.4009)\n",
      "    Batch [5900/7058] Loss: 1.1173 (Class: 0.9111, Duration: 4.1247)\n",
      "    Batch [6000/7058] Loss: 1.0835 (Class: 0.8101, Duration: 5.4675)\n",
      "    Batch [6100/7058] Loss: 1.0425 (Class: 0.7578, Duration: 5.6922)\n",
      "    Batch [6200/7058] Loss: 1.0713 (Class: 0.8599, Duration: 4.2281)\n",
      "    Batch [6300/7058] Loss: 0.9494 (Class: 0.7205, Duration: 4.5783)\n",
      "    Batch [6400/7058] Loss: 1.0552 (Class: 0.8089, Duration: 4.9261)\n",
      "    Batch [6500/7058] Loss: 1.0355 (Class: 0.8758, Duration: 3.1940)\n",
      "    Batch [6600/7058] Loss: 0.9526 (Class: 0.7625, Duration: 3.8018)\n",
      "    Batch [6700/7058] Loss: 0.9809 (Class: 0.7814, Duration: 3.9901)\n",
      "    Batch [6800/7058] Loss: 1.0110 (Class: 0.7845, Duration: 4.5289)\n",
      "    Batch [6900/7058] Loss: 1.0392 (Class: 0.8264, Duration: 4.2555)\n",
      "    Batch [7000/7058] Loss: 1.0284 (Class: 0.8009, Duration: 4.5494)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 3.3 GB (+0.5 GB)\n",
      "  Loaded 100/255 files, Memory: 4.3 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 5.3 GB (+2.5 GB)\n",
      "  Loaded 200/255 files, Memory: 6.4 GB (+3.6 GB)\n",
      "  Loaded 250/255 files, Memory: 7.4 GB (+4.6 GB)\n",
      "Loaded 800392 samples. Memory usage: 8.1 GB (+5.3 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 0.9736 (Class: 0.7394, Duration: 4.6844)\n",
      "    Batch [100/6960] Loss: 1.0570 (Class: 0.8403, Duration: 4.3342)\n",
      "    Batch [200/6960] Loss: 1.1471 (Class: 0.9233, Duration: 4.4757)\n",
      "    Batch [300/6960] Loss: 1.0648 (Class: 0.8401, Duration: 4.4944)\n",
      "    Batch [400/6960] Loss: 1.1057 (Class: 0.8368, Duration: 5.3777)\n",
      "    Batch [500/6960] Loss: 0.9706 (Class: 0.8123, Duration: 3.1647)\n",
      "    Batch [600/6960] Loss: 1.0569 (Class: 0.8392, Duration: 4.3530)\n",
      "    Batch [700/6960] Loss: 1.1805 (Class: 0.9666, Duration: 4.2778)\n",
      "    Batch [800/6960] Loss: 1.0684 (Class: 0.8139, Duration: 5.0900)\n",
      "    Batch [900/6960] Loss: 0.9846 (Class: 0.8026, Duration: 3.6388)\n",
      "    Batch [1000/6960] Loss: 1.0219 (Class: 0.7722, Duration: 4.9957)\n",
      "    Batch [1100/6960] Loss: 1.1421 (Class: 0.9118, Duration: 4.6074)\n",
      "    Batch [1200/6960] Loss: 1.0505 (Class: 0.8173, Duration: 4.6647)\n",
      "    Batch [1300/6960] Loss: 0.9461 (Class: 0.7593, Duration: 3.7378)\n",
      "    Batch [1400/6960] Loss: 1.0153 (Class: 0.7389, Duration: 5.5280)\n",
      "    Batch [1500/6960] Loss: 1.0620 (Class: 0.8556, Duration: 4.1288)\n",
      "    Batch [1600/6960] Loss: 1.2168 (Class: 0.9592, Duration: 5.1533)\n",
      "    Batch [1700/6960] Loss: 1.1501 (Class: 0.9056, Duration: 4.8900)\n",
      "    Batch [1800/6960] Loss: 0.9963 (Class: 0.7655, Duration: 4.6168)\n",
      "    Batch [1900/6960] Loss: 1.1093 (Class: 0.8184, Duration: 5.8179)\n",
      "    Batch [2000/6960] Loss: 1.0646 (Class: 0.8478, Duration: 4.3369)\n",
      "    Batch [2100/6960] Loss: 0.9551 (Class: 0.7396, Duration: 4.3085)\n",
      "    Batch [2200/6960] Loss: 1.0458 (Class: 0.7934, Duration: 5.0472)\n",
      "    Batch [2300/6960] Loss: 1.0238 (Class: 0.8478, Duration: 3.5201)\n",
      "    Batch [2400/6960] Loss: 1.2514 (Class: 1.0199, Duration: 4.6293)\n",
      "    Batch [2500/6960] Loss: 1.1445 (Class: 0.9397, Duration: 4.0959)\n",
      "    Batch [2600/6960] Loss: 1.0942 (Class: 0.8709, Duration: 4.4659)\n",
      "    Batch [2700/6960] Loss: 0.9165 (Class: 0.7424, Duration: 3.4821)\n",
      "    Batch [2800/6960] Loss: 0.9923 (Class: 0.7561, Duration: 4.7227)\n",
      "    Batch [2900/6960] Loss: 1.0208 (Class: 0.7539, Duration: 5.3375)\n",
      "    Batch [3000/6960] Loss: 1.0935 (Class: 0.8089, Duration: 5.6924)\n",
      "    Batch [3100/6960] Loss: 1.1041 (Class: 0.8659, Duration: 4.7655)\n",
      "    Batch [3200/6960] Loss: 1.0197 (Class: 0.8054, Duration: 4.2856)\n",
      "    Batch [3300/6960] Loss: 0.9923 (Class: 0.7869, Duration: 4.1091)\n",
      "    Batch [3400/6960] Loss: 1.1837 (Class: 0.9564, Duration: 4.5451)\n",
      "    Batch [3500/6960] Loss: 1.0060 (Class: 0.7855, Duration: 4.4105)\n",
      "    Batch [3600/6960] Loss: 1.0371 (Class: 0.7895, Duration: 4.9516)\n",
      "    Batch [3700/6960] Loss: 0.9890 (Class: 0.8229, Duration: 3.3217)\n",
      "    Batch [3800/6960] Loss: 1.0663 (Class: 0.8804, Duration: 3.7172)\n",
      "    Batch [3900/6960] Loss: 0.9783 (Class: 0.7874, Duration: 3.8180)\n",
      "    Batch [4000/6960] Loss: 0.9891 (Class: 0.7568, Duration: 4.6443)\n",
      "    Batch [4100/6960] Loss: 1.0093 (Class: 0.7817, Duration: 4.5521)\n",
      "    Batch [4200/6960] Loss: 1.0572 (Class: 0.8356, Duration: 4.4322)\n",
      "    Batch [4300/6960] Loss: 0.8985 (Class: 0.7211, Duration: 3.5480)\n",
      "    Batch [4400/6960] Loss: 0.9676 (Class: 0.7735, Duration: 3.8820)\n",
      "    Batch [4500/6960] Loss: 1.2066 (Class: 0.8949, Duration: 6.2341)\n",
      "    Batch [4600/6960] Loss: 0.9907 (Class: 0.7904, Duration: 4.0053)\n",
      "    Batch [4700/6960] Loss: 1.1014 (Class: 0.8763, Duration: 4.5009)\n",
      "    Batch [4800/6960] Loss: 0.8766 (Class: 0.6994, Duration: 3.5433)\n",
      "    Batch [4900/6960] Loss: 0.9961 (Class: 0.7546, Duration: 4.8304)\n",
      "    Batch [5000/6960] Loss: 0.9487 (Class: 0.7697, Duration: 3.5813)\n",
      "    Batch [5100/6960] Loss: 0.9162 (Class: 0.7329, Duration: 3.6655)\n",
      "    Batch [5200/6960] Loss: 1.0096 (Class: 0.8070, Duration: 4.0520)\n",
      "    Batch [5300/6960] Loss: 0.9652 (Class: 0.7214, Duration: 4.8772)\n",
      "    Batch [5400/6960] Loss: 1.0534 (Class: 0.7999, Duration: 5.0692)\n",
      "    Batch [5500/6960] Loss: 0.9727 (Class: 0.7407, Duration: 4.6387)\n",
      "    Batch [5600/6960] Loss: 1.0185 (Class: 0.7697, Duration: 4.9770)\n",
      "    Batch [5700/6960] Loss: 1.0830 (Class: 0.8906, Duration: 3.8490)\n",
      "    Batch [5800/6960] Loss: 1.0693 (Class: 0.7986, Duration: 5.4142)\n",
      "    Batch [5900/6960] Loss: 0.9766 (Class: 0.7819, Duration: 3.8929)\n",
      "    Batch [6000/6960] Loss: 0.9974 (Class: 0.7839, Duration: 4.2710)\n",
      "    Batch [6100/6960] Loss: 0.8964 (Class: 0.7126, Duration: 3.6752)\n",
      "    Batch [6200/6960] Loss: 1.1364 (Class: 0.9557, Duration: 3.6139)\n",
      "    Batch [6300/6960] Loss: 1.0129 (Class: 0.8398, Duration: 3.4632)\n",
      "    Batch [6400/6960] Loss: 1.0003 (Class: 0.7937, Duration: 4.1317)\n",
      "    Batch [6500/6960] Loss: 1.1030 (Class: 0.8772, Duration: 4.5176)\n",
      "    Batch [6600/6960] Loss: 1.0652 (Class: 0.8208, Duration: 4.8881)\n",
      "    Batch [6700/6960] Loss: 1.0046 (Class: 0.7347, Duration: 5.3991)\n",
      "    Batch [6800/6960] Loss: 0.9821 (Class: 0.7612, Duration: 4.4187)\n",
      "    Batch [6900/6960] Loss: 1.0636 (Class: 0.8611, Duration: 4.0501)\n",
      "\n",
      "Train Loss: 1.0870, Train Acc: 59.49%\n",
      "Val Loss: 1.0500, Val Acc: 62.97%\n",
      "Val Duration MSE: 4.7912 (RMSE: 2.1889)\n",
      "Class Accuracies - Bull: 61.63%, Flat: 67.19%, Bear: 58.90%\n",
      "Learning Rate: 1.90e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  -> New best model saved! (best balanced accuracy)\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 3.1 GB (+0.5 GB)\n",
      "  Loaded 100/255 files, Memory: 4.2 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 5.4 GB (+2.7 GB)\n",
      "  Loaded 200/255 files, Memory: 6.2 GB (+3.5 GB)\n",
      "  Loaded 250/255 files, Memory: 7.3 GB (+4.6 GB)\n",
      "Loaded 811636 samples. Memory usage: 9.4 GB (+6.7 GB)\n",
      "\n",
      "Epoch 2/80\n",
      "Duration weight: 0.054\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 0.9749 (Class: 0.7256, Duration: 4.5857)\n",
      "    Batch [100/7058] Loss: 1.1185 (Class: 0.8517, Duration: 4.9074)\n",
      "    Batch [200/7058] Loss: 1.0290 (Class: 0.8009, Duration: 4.1953)\n",
      "    Batch [300/7058] Loss: 0.9586 (Class: 0.7829, Duration: 3.2308)\n",
      "    Batch [400/7058] Loss: 1.0655 (Class: 0.7832, Duration: 5.1909)\n",
      "    Batch [500/7058] Loss: 1.0020 (Class: 0.7253, Duration: 5.0882)\n",
      "    Batch [600/7058] Loss: 1.1423 (Class: 0.8729, Duration: 4.9551)\n",
      "    Batch [700/7058] Loss: 1.0967 (Class: 0.8551, Duration: 4.4425)\n",
      "    Batch [800/7058] Loss: 0.9958 (Class: 0.7687, Duration: 4.1768)\n",
      "    Batch [900/7058] Loss: 1.0988 (Class: 0.7831, Duration: 5.8063)\n",
      "    Batch [1000/7058] Loss: 1.1498 (Class: 0.8872, Duration: 4.8295)\n",
      "    Batch [1100/7058] Loss: 1.0841 (Class: 0.7926, Duration: 5.3609)\n",
      "    Batch [1200/7058] Loss: 1.0164 (Class: 0.7512, Duration: 4.8768)\n",
      "    Batch [1300/7058] Loss: 1.2393 (Class: 0.9565, Duration: 5.2010)\n",
      "    Batch [1400/7058] Loss: 1.1133 (Class: 0.8924, Duration: 4.0623)\n",
      "    Batch [1500/7058] Loss: 1.0918 (Class: 0.8051, Duration: 5.2734)\n",
      "    Batch [1600/7058] Loss: 1.0866 (Class: 0.8632, Duration: 4.1082)\n",
      "    Batch [1700/7058] Loss: 1.0681 (Class: 0.8463, Duration: 4.0778)\n",
      "    Batch [1800/7058] Loss: 1.0715 (Class: 0.8205, Duration: 4.6171)\n",
      "    Batch [1900/7058] Loss: 1.0919 (Class: 0.8951, Duration: 3.6188)\n",
      "    Batch [2000/7058] Loss: 1.0984 (Class: 0.8551, Duration: 4.4743)\n",
      "    Batch [2100/7058] Loss: 1.0264 (Class: 0.7942, Duration: 4.2699)\n",
      "    Batch [2200/7058] Loss: 0.9981 (Class: 0.8114, Duration: 3.4321)\n",
      "    Batch [2300/7058] Loss: 1.0820 (Class: 0.8276, Duration: 4.6793)\n",
      "    Batch [2400/7058] Loss: 1.0270 (Class: 0.8160, Duration: 3.8797)\n",
      "    Batch [2500/7058] Loss: 1.0150 (Class: 0.7343, Duration: 5.1631)\n",
      "    Batch [2600/7058] Loss: 1.0464 (Class: 0.7901, Duration: 4.7119)\n",
      "    Batch [2700/7058] Loss: 0.9887 (Class: 0.7750, Duration: 3.9287)\n",
      "    Batch [2800/7058] Loss: 1.0684 (Class: 0.8405, Duration: 4.1907)\n",
      "    Batch [2900/7058] Loss: 1.0447 (Class: 0.8103, Duration: 4.3103)\n",
      "    Batch [3000/7058] Loss: 1.2046 (Class: 0.8709, Duration: 6.1379)\n",
      "    Batch [3100/7058] Loss: 1.2192 (Class: 0.9675, Duration: 4.6295)\n",
      "    Batch [3200/7058] Loss: 1.1537 (Class: 0.8781, Duration: 5.0692)\n",
      "    Batch [3300/7058] Loss: 1.0367 (Class: 0.8200, Duration: 3.9858)\n",
      "    Batch [3400/7058] Loss: 1.0072 (Class: 0.7933, Duration: 3.9337)\n",
      "    Batch [3500/7058] Loss: 1.1227 (Class: 0.8566, Duration: 4.8929)\n",
      "    Batch [3600/7058] Loss: 0.9493 (Class: 0.7139, Duration: 4.3299)\n",
      "    Batch [3700/7058] Loss: 0.9231 (Class: 0.7406, Duration: 3.3547)\n",
      "    Batch [3800/7058] Loss: 1.0898 (Class: 0.8144, Duration: 5.0649)\n",
      "    Batch [3900/7058] Loss: 1.0996 (Class: 0.8475, Duration: 4.6366)\n",
      "    Batch [4000/7058] Loss: 1.0965 (Class: 0.8898, Duration: 3.8010)\n",
      "    Batch [4100/7058] Loss: 1.1181 (Class: 0.8315, Duration: 5.2699)\n",
      "    Batch [4200/7058] Loss: 1.0405 (Class: 0.8010, Duration: 4.4049)\n",
      "    Batch [4300/7058] Loss: 1.0025 (Class: 0.8100, Duration: 3.5395)\n",
      "    Batch [4400/7058] Loss: 1.0793 (Class: 0.8387, Duration: 4.4252)\n",
      "    Batch [4500/7058] Loss: 1.0229 (Class: 0.8014, Duration: 4.0731)\n",
      "    Batch [4600/7058] Loss: 1.0295 (Class: 0.8176, Duration: 3.8972)\n",
      "    Batch [4700/7058] Loss: 1.0599 (Class: 0.7540, Duration: 5.6260)\n",
      "    Batch [4800/7058] Loss: 0.9721 (Class: 0.7201, Duration: 4.6344)\n",
      "    Batch [4900/7058] Loss: 1.1168 (Class: 0.8945, Duration: 4.0898)\n",
      "    Batch [5000/7058] Loss: 1.1285 (Class: 0.8337, Duration: 5.4216)\n",
      "    Batch [5100/7058] Loss: 1.1369 (Class: 0.8709, Duration: 4.8917)\n",
      "    Batch [5200/7058] Loss: 0.9326 (Class: 0.7580, Duration: 3.2105)\n",
      "    Batch [5300/7058] Loss: 1.0915 (Class: 0.8488, Duration: 4.4631)\n",
      "    Batch [5400/7058] Loss: 1.0370 (Class: 0.8096, Duration: 4.1814)\n",
      "    Batch [5500/7058] Loss: 0.9163 (Class: 0.6848, Duration: 4.2582)\n",
      "    Batch [5600/7058] Loss: 1.1101 (Class: 0.8860, Duration: 4.1220)\n",
      "    Batch [5700/7058] Loss: 0.9446 (Class: 0.7776, Duration: 3.0712)\n",
      "    Batch [5800/7058] Loss: 1.0629 (Class: 0.7703, Duration: 5.3822)\n",
      "    Batch [5900/7058] Loss: 0.9083 (Class: 0.7126, Duration: 3.6000)\n",
      "    Batch [6000/7058] Loss: 0.9602 (Class: 0.7641, Duration: 3.6058)\n",
      "    Batch [6100/7058] Loss: 1.0471 (Class: 0.8056, Duration: 4.4407)\n",
      "    Batch [6200/7058] Loss: 1.0439 (Class: 0.7966, Duration: 4.5495)\n",
      "    Batch [6300/7058] Loss: 1.0562 (Class: 0.8062, Duration: 4.5980)\n",
      "    Batch [6400/7058] Loss: 1.0841 (Class: 0.8173, Duration: 4.9060)\n",
      "    Batch [6500/7058] Loss: 0.9267 (Class: 0.7850, Duration: 2.6054)\n",
      "    Batch [6600/7058] Loss: 1.0211 (Class: 0.7794, Duration: 4.4458)\n",
      "    Batch [6700/7058] Loss: 1.1032 (Class: 0.9299, Duration: 3.1869)\n",
      "    Batch [6800/7058] Loss: 1.0220 (Class: 0.7738, Duration: 4.5659)\n",
      "    Batch [6900/7058] Loss: 1.0493 (Class: 0.8603, Duration: 3.4748)\n",
      "    Batch [7000/7058] Loss: 0.9975 (Class: 0.7641, Duration: 4.2931)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 4.0 GB (+0.0 GB)\n",
      "  Loaded 100/255 files, Memory: 4.3 GB (+0.3 GB)\n",
      "  Loaded 150/255 files, Memory: 5.3 GB (+1.2 GB)\n",
      "  Loaded 200/255 files, Memory: 6.4 GB (+2.4 GB)\n",
      "  Loaded 250/255 files, Memory: 7.4 GB (+3.4 GB)\n",
      "Loaded 800392 samples. Memory usage: 9.9 GB (+5.9 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 1.0344 (Class: 0.7886, Duration: 4.5215)\n",
      "    Batch [100/6960] Loss: 0.9968 (Class: 0.7779, Duration: 4.0267)\n",
      "    Batch [200/6960] Loss: 0.9713 (Class: 0.7262, Duration: 4.5069)\n",
      "    Batch [300/6960] Loss: 1.0624 (Class: 0.8063, Duration: 4.7101)\n",
      "    Batch [400/6960] Loss: 1.0562 (Class: 0.7855, Duration: 4.9788)\n",
      "    Batch [500/6960] Loss: 1.0623 (Class: 0.8308, Duration: 4.2575)\n",
      "    Batch [600/6960] Loss: 1.1510 (Class: 0.8980, Duration: 4.6541)\n",
      "    Batch [700/6960] Loss: 1.0486 (Class: 0.8092, Duration: 4.4024)\n",
      "    Batch [800/6960] Loss: 1.1455 (Class: 0.8546, Duration: 5.3503)\n",
      "    Batch [900/6960] Loss: 1.0986 (Class: 0.7730, Duration: 5.9869)\n",
      "    Batch [1000/6960] Loss: 0.9378 (Class: 0.6960, Duration: 4.4463)\n",
      "    Batch [1100/6960] Loss: 0.9399 (Class: 0.7378, Duration: 3.7159)\n",
      "    Batch [1200/6960] Loss: 1.0639 (Class: 0.8582, Duration: 3.7819)\n",
      "    Batch [1300/6960] Loss: 1.1001 (Class: 0.8074, Duration: 5.3838)\n",
      "    Batch [1400/6960] Loss: 0.9606 (Class: 0.7967, Duration: 3.0142)\n",
      "    Batch [1500/6960] Loss: 1.0993 (Class: 0.8498, Duration: 4.5876)\n",
      "    Batch [1600/6960] Loss: 0.8948 (Class: 0.7319, Duration: 2.9960)\n",
      "    Batch [1700/6960] Loss: 1.0095 (Class: 0.8103, Duration: 3.6631)\n",
      "    Batch [1800/6960] Loss: 0.9090 (Class: 0.7126, Duration: 3.6116)\n",
      "    Batch [1900/6960] Loss: 1.0573 (Class: 0.8323, Duration: 4.1368)\n",
      "    Batch [2000/6960] Loss: 1.0476 (Class: 0.7177, Duration: 6.0665)\n",
      "    Batch [2100/6960] Loss: 0.9288 (Class: 0.7375, Duration: 3.5174)\n",
      "    Batch [2200/6960] Loss: 1.0577 (Class: 0.8149, Duration: 4.4645)\n",
      "    Batch [2300/6960] Loss: 0.9869 (Class: 0.7525, Duration: 4.3111)\n",
      "    Batch [2400/6960] Loss: 1.0960 (Class: 0.8838, Duration: 3.9028)\n",
      "    Batch [2500/6960] Loss: 0.9569 (Class: 0.7149, Duration: 4.4501)\n",
      "    Batch [2600/6960] Loss: 1.0013 (Class: 0.7575, Duration: 4.4828)\n",
      "    Batch [2700/6960] Loss: 0.9761 (Class: 0.7457, Duration: 4.2377)\n",
      "    Batch [2800/6960] Loss: 1.0520 (Class: 0.8039, Duration: 4.5620)\n",
      "    Batch [2900/6960] Loss: 1.0712 (Class: 0.8416, Duration: 4.2230)\n",
      "    Batch [3000/6960] Loss: 0.9861 (Class: 0.7608, Duration: 4.1423)\n",
      "    Batch [3100/6960] Loss: 0.9603 (Class: 0.6978, Duration: 4.8281)\n",
      "    Batch [3200/6960] Loss: 1.0989 (Class: 0.8647, Duration: 4.3079)\n",
      "    Batch [3300/6960] Loss: 1.0639 (Class: 0.7626, Duration: 5.5409)\n",
      "    Batch [3400/6960] Loss: 1.0841 (Class: 0.8266, Duration: 4.7355)\n",
      "    Batch [3500/6960] Loss: 0.9604 (Class: 0.7328, Duration: 4.1862)\n",
      "    Batch [3600/6960] Loss: 0.9676 (Class: 0.6909, Duration: 5.0879)\n",
      "    Batch [3700/6960] Loss: 1.0730 (Class: 0.7937, Duration: 5.1369)\n",
      "    Batch [3800/6960] Loss: 1.1042 (Class: 0.8295, Duration: 5.0515)\n",
      "    Batch [3900/6960] Loss: 1.0153 (Class: 0.7761, Duration: 4.3991)\n",
      "    Batch [4000/6960] Loss: 1.0025 (Class: 0.8141, Duration: 3.4647)\n",
      "    Batch [4100/6960] Loss: 0.9316 (Class: 0.7202, Duration: 3.8887)\n",
      "    Batch [4200/6960] Loss: 1.1068 (Class: 0.8674, Duration: 4.4026)\n",
      "    Batch [4300/6960] Loss: 1.0809 (Class: 0.8266, Duration: 4.6764)\n",
      "    Batch [4400/6960] Loss: 0.9956 (Class: 0.7734, Duration: 4.0851)\n",
      "    Batch [4500/6960] Loss: 1.0647 (Class: 0.7995, Duration: 4.8786)\n",
      "    Batch [4600/6960] Loss: 0.9878 (Class: 0.6941, Duration: 5.4000)\n",
      "    Batch [4700/6960] Loss: 1.0092 (Class: 0.7484, Duration: 4.7970)\n",
      "    Batch [4800/6960] Loss: 1.0779 (Class: 0.7947, Duration: 5.2091)\n",
      "    Batch [4900/6960] Loss: 1.0052 (Class: 0.7566, Duration: 4.5718)\n",
      "    Batch [5000/6960] Loss: 0.9441 (Class: 0.7550, Duration: 3.4773)\n",
      "    Batch [5100/6960] Loss: 1.2191 (Class: 0.9906, Duration: 4.2021)\n",
      "    Batch [5200/6960] Loss: 0.9334 (Class: 0.7578, Duration: 3.2289)\n",
      "    Batch [5300/6960] Loss: 0.9522 (Class: 0.7730, Duration: 3.2952)\n",
      "    Batch [5400/6960] Loss: 1.0188 (Class: 0.7462, Duration: 5.0130)\n",
      "    Batch [5500/6960] Loss: 0.9758 (Class: 0.7227, Duration: 4.6548)\n",
      "    Batch [5600/6960] Loss: 1.0668 (Class: 0.8474, Duration: 4.0352)\n",
      "    Batch [5700/6960] Loss: 0.7632 (Class: 0.6136, Duration: 2.7517)\n",
      "    Batch [5800/6960] Loss: 1.0079 (Class: 0.7747, Duration: 4.2881)\n",
      "    Batch [5900/6960] Loss: 1.0376 (Class: 0.7850, Duration: 4.6452)\n",
      "    Batch [6000/6960] Loss: 1.0793 (Class: 0.8309, Duration: 4.5683)\n",
      "    Batch [6100/6960] Loss: 0.9391 (Class: 0.7289, Duration: 3.8658)\n",
      "    Batch [6200/6960] Loss: 0.9957 (Class: 0.8021, Duration: 3.5613)\n",
      "    Batch [6300/6960] Loss: 0.9567 (Class: 0.7442, Duration: 3.9077)\n",
      "    Batch [6400/6960] Loss: 1.0937 (Class: 0.8408, Duration: 4.6506)\n",
      "    Batch [6500/6960] Loss: 0.9613 (Class: 0.7619, Duration: 3.6676)\n",
      "    Batch [6600/6960] Loss: 1.0710 (Class: 0.7876, Duration: 5.2126)\n",
      "    Batch [6700/6960] Loss: 0.9716 (Class: 0.7368, Duration: 4.3191)\n",
      "    Batch [6800/6960] Loss: 1.0125 (Class: 0.7273, Duration: 5.2456)\n",
      "    Batch [6900/6960] Loss: 0.9777 (Class: 0.7265, Duration: 4.6196)\n",
      "\n",
      "Train Loss: 1.0485, Train Acc: 62.56%\n",
      "Val Loss: 1.0427, Val Acc: 64.14%\n",
      "Val Duration MSE: 4.7475 (RMSE: 2.1789)\n",
      "Class Accuracies - Bull: 61.97%, Flat: 68.73%, Bear: 60.87%\n",
      "Learning Rate: 2.80e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  -> New best model saved! (best balanced accuracy)\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 3.2 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 4.2 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 5.4 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 6.2 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 7.3 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 8.2 GB (+5.4 GB)\n",
      "\n",
      "Epoch 3/80\n",
      "Duration weight: 0.059\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 1.0523 (Class: 0.7821, Duration: 4.5994)\n",
      "    Batch [100/7058] Loss: 1.0711 (Class: 0.7619, Duration: 5.2627)\n",
      "    Batch [200/7058] Loss: 1.0564 (Class: 0.7960, Duration: 4.4330)\n",
      "    Batch [300/7058] Loss: 1.0462 (Class: 0.7425, Duration: 5.1696)\n",
      "    Batch [400/7058] Loss: 1.0194 (Class: 0.7431, Duration: 4.7028)\n",
      "    Batch [500/7058] Loss: 1.1014 (Class: 0.8449, Duration: 4.3670)\n",
      "    Batch [600/7058] Loss: 1.0971 (Class: 0.7981, Duration: 5.0895)\n",
      "    Batch [700/7058] Loss: 1.1217 (Class: 0.7982, Duration: 5.5065)\n",
      "    Batch [800/7058] Loss: 1.0877 (Class: 0.8051, Duration: 4.8111)\n",
      "    Batch [900/7058] Loss: 1.1100 (Class: 0.8581, Duration: 4.2882)\n",
      "    Batch [1000/7058] Loss: 1.2881 (Class: 1.0213, Duration: 4.5409)\n",
      "    Batch [1100/7058] Loss: 0.9897 (Class: 0.7570, Duration: 3.9596)\n",
      "    Batch [1200/7058] Loss: 1.1231 (Class: 0.8311, Duration: 4.9701)\n",
      "    Batch [1300/7058] Loss: 1.1640 (Class: 0.8036, Duration: 6.1357)\n",
      "    Batch [1400/7058] Loss: 1.0905 (Class: 0.7664, Duration: 5.5165)\n",
      "    Batch [1500/7058] Loss: 1.1518 (Class: 0.8564, Duration: 5.0284)\n",
      "    Batch [1600/7058] Loss: 0.9855 (Class: 0.7085, Duration: 4.7161)\n",
      "    Batch [1700/7058] Loss: 1.1157 (Class: 0.8624, Duration: 4.3109)\n",
      "    Batch [1800/7058] Loss: 1.0328 (Class: 0.7908, Duration: 4.1195)\n",
      "    Batch [1900/7058] Loss: 1.0151 (Class: 0.7674, Duration: 4.2150)\n",
      "    Batch [2000/7058] Loss: 1.0294 (Class: 0.7939, Duration: 4.0086)\n",
      "    Batch [2100/7058] Loss: 1.0059 (Class: 0.7633, Duration: 4.1299)\n",
      "    Batch [2200/7058] Loss: 1.1023 (Class: 0.8336, Duration: 4.5735)\n",
      "    Batch [2300/7058] Loss: 1.0919 (Class: 0.8441, Duration: 4.2183)\n",
      "    Batch [2400/7058] Loss: 1.0837 (Class: 0.8967, Duration: 3.1829)\n",
      "    Batch [2500/7058] Loss: 0.9891 (Class: 0.7590, Duration: 3.9167)\n",
      "    Batch [2600/7058] Loss: 0.9884 (Class: 0.7913, Duration: 3.3543)\n",
      "    Batch [2700/7058] Loss: 0.9981 (Class: 0.7562, Duration: 4.1176)\n",
      "    Batch [2800/7058] Loss: 0.9162 (Class: 0.7027, Duration: 3.6346)\n",
      "    Batch [2900/7058] Loss: 1.1160 (Class: 0.8043, Duration: 5.3048)\n",
      "    Batch [3000/7058] Loss: 1.0006 (Class: 0.7848, Duration: 3.6727)\n",
      "    Batch [3100/7058] Loss: 1.0429 (Class: 0.8358, Duration: 3.5239)\n",
      "    Batch [3200/7058] Loss: 0.9901 (Class: 0.7521, Duration: 4.0509)\n",
      "    Batch [3300/7058] Loss: 1.0580 (Class: 0.8219, Duration: 4.0182)\n",
      "    Batch [3400/7058] Loss: 0.9298 (Class: 0.6981, Duration: 3.9440)\n",
      "    Batch [3500/7058] Loss: 0.9787 (Class: 0.7220, Duration: 4.3698)\n",
      "    Batch [3600/7058] Loss: 1.0379 (Class: 0.8215, Duration: 3.6834)\n",
      "    Batch [3700/7058] Loss: 1.0494 (Class: 0.7593, Duration: 4.9378)\n",
      "    Batch [3800/7058] Loss: 1.0963 (Class: 0.8055, Duration: 4.9510)\n",
      "    Batch [3900/7058] Loss: 1.1437 (Class: 0.7843, Duration: 6.1177)\n",
      "    Batch [4000/7058] Loss: 0.9173 (Class: 0.7118, Duration: 3.4994)\n",
      "    Batch [4100/7058] Loss: 1.2176 (Class: 0.9085, Duration: 5.2618)\n",
      "    Batch [4200/7058] Loss: 0.9502 (Class: 0.6869, Duration: 4.4817)\n",
      "    Batch [4300/7058] Loss: 1.0134 (Class: 0.7772, Duration: 4.0209)\n",
      "    Batch [4400/7058] Loss: 0.9909 (Class: 0.7581, Duration: 3.9633)\n",
      "    Batch [4500/7058] Loss: 1.0309 (Class: 0.8315, Duration: 3.3933)\n",
      "    Batch [4600/7058] Loss: 0.9506 (Class: 0.7239, Duration: 3.8580)\n",
      "    Batch [4700/7058] Loss: 1.0733 (Class: 0.8008, Duration: 4.6374)\n",
      "    Batch [4800/7058] Loss: 1.0200 (Class: 0.7588, Duration: 4.4462)\n",
      "    Batch [4900/7058] Loss: 1.0688 (Class: 0.7966, Duration: 4.6332)\n",
      "    Batch [5000/7058] Loss: 1.0282 (Class: 0.7813, Duration: 4.2030)\n",
      "    Batch [5100/7058] Loss: 0.9655 (Class: 0.6486, Duration: 5.3933)\n",
      "    Batch [5200/7058] Loss: 1.1096 (Class: 0.8174, Duration: 4.9729)\n",
      "    Batch [5300/7058] Loss: 1.0548 (Class: 0.8562, Duration: 3.3808)\n",
      "    Batch [5400/7058] Loss: 0.9566 (Class: 0.7329, Duration: 3.8078)\n",
      "    Batch [5500/7058] Loss: 1.0707 (Class: 0.8376, Duration: 3.9674)\n",
      "    Batch [5600/7058] Loss: 1.0808 (Class: 0.8362, Duration: 4.1628)\n",
      "    Batch [5700/7058] Loss: 1.0077 (Class: 0.7903, Duration: 3.7002)\n",
      "    Batch [5800/7058] Loss: 1.0044 (Class: 0.7457, Duration: 4.4031)\n",
      "    Batch [5900/7058] Loss: 1.0236 (Class: 0.8119, Duration: 3.6038)\n",
      "    Batch [6000/7058] Loss: 0.9215 (Class: 0.6752, Duration: 4.1921)\n",
      "    Batch [6100/7058] Loss: 1.0199 (Class: 0.7166, Duration: 5.1626)\n",
      "    Batch [6200/7058] Loss: 1.1282 (Class: 0.8630, Duration: 4.5155)\n",
      "    Batch [6300/7058] Loss: 1.0480 (Class: 0.8424, Duration: 3.4984)\n",
      "    Batch [6400/7058] Loss: 1.0914 (Class: 0.7990, Duration: 4.9763)\n",
      "    Batch [6500/7058] Loss: 1.0578 (Class: 0.7294, Duration: 5.5896)\n",
      "    Batch [6600/7058] Loss: 1.0785 (Class: 0.8028, Duration: 4.6928)\n",
      "    Batch [6700/7058] Loss: 0.8876 (Class: 0.6337, Duration: 4.3213)\n",
      "    Batch [6800/7058] Loss: 1.0836 (Class: 0.7744, Duration: 5.2624)\n",
      "    Batch [6900/7058] Loss: 1.0336 (Class: 0.7972, Duration: 4.0247)\n",
      "    Batch [7000/7058] Loss: 1.0894 (Class: 0.8170, Duration: 4.6370)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 3.2 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 4.3 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 5.2 GB (+2.4 GB)\n",
      "  Loaded 200/255 files, Memory: 6.4 GB (+3.6 GB)\n",
      "  Loaded 250/255 files, Memory: 7.4 GB (+4.6 GB)\n",
      "Loaded 800392 samples. Memory usage: 8.1 GB (+5.3 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 0.9560 (Class: 0.7597, Duration: 3.3402)\n",
      "    Batch [100/6960] Loss: 1.0179 (Class: 0.8113, Duration: 3.5180)\n",
      "    Batch [200/6960] Loss: 0.9758 (Class: 0.7376, Duration: 4.0542)\n",
      "    Batch [300/6960] Loss: 0.9874 (Class: 0.7350, Duration: 4.2959)\n",
      "    Batch [400/6960] Loss: 0.9651 (Class: 0.7078, Duration: 4.3800)\n",
      "    Batch [500/6960] Loss: 1.0155 (Class: 0.7203, Duration: 5.0253)\n",
      "    Batch [600/6960] Loss: 1.1439 (Class: 0.8316, Duration: 5.3164)\n",
      "    Batch [700/6960] Loss: 1.0030 (Class: 0.7420, Duration: 4.4421)\n",
      "    Batch [800/6960] Loss: 0.9483 (Class: 0.7310, Duration: 3.7002)\n",
      "    Batch [900/6960] Loss: 1.0457 (Class: 0.8443, Duration: 3.4274)\n",
      "    Batch [1000/6960] Loss: 1.0590 (Class: 0.8607, Duration: 3.3744)\n",
      "    Batch [1100/6960] Loss: 1.0841 (Class: 0.7601, Duration: 5.5149)\n",
      "    Batch [1200/6960] Loss: 1.0874 (Class: 0.7717, Duration: 5.3735)\n",
      "    Batch [1300/6960] Loss: 0.9323 (Class: 0.7659, Duration: 2.8336)\n",
      "    Batch [1400/6960] Loss: 1.0030 (Class: 0.7910, Duration: 3.6073)\n",
      "    Batch [1500/6960] Loss: 0.9965 (Class: 0.7041, Duration: 4.9774)\n",
      "    Batch [1600/6960] Loss: 0.9419 (Class: 0.7215, Duration: 3.7503)\n",
      "    Batch [1700/6960] Loss: 1.0567 (Class: 0.7968, Duration: 4.4246)\n",
      "    Batch [1800/6960] Loss: 1.0390 (Class: 0.7535, Duration: 4.8601)\n",
      "    Batch [1900/6960] Loss: 1.0992 (Class: 0.8349, Duration: 4.4985)\n",
      "    Batch [2000/6960] Loss: 0.9837 (Class: 0.7486, Duration: 4.0015)\n",
      "    Batch [2100/6960] Loss: 1.0750 (Class: 0.8120, Duration: 4.4771)\n",
      "    Batch [2200/6960] Loss: 1.0753 (Class: 0.8161, Duration: 4.4131)\n",
      "    Batch [2300/6960] Loss: 1.0181 (Class: 0.7592, Duration: 4.4070)\n",
      "    Batch [2400/6960] Loss: 0.9587 (Class: 0.7574, Duration: 3.4253)\n",
      "    Batch [2500/6960] Loss: 1.0898 (Class: 0.8436, Duration: 4.1914)\n",
      "    Batch [2600/6960] Loss: 0.9317 (Class: 0.7236, Duration: 3.5429)\n",
      "    Batch [2700/6960] Loss: 0.9904 (Class: 0.7364, Duration: 4.3250)\n",
      "    Batch [2800/6960] Loss: 1.1602 (Class: 0.8443, Duration: 5.3773)\n",
      "    Batch [2900/6960] Loss: 1.0848 (Class: 0.8096, Duration: 4.6850)\n",
      "    Batch [3000/6960] Loss: 0.9890 (Class: 0.7646, Duration: 3.8201)\n",
      "    Batch [3100/6960] Loss: 1.0263 (Class: 0.8206, Duration: 3.5025)\n",
      "    Batch [3200/6960] Loss: 1.0623 (Class: 0.8314, Duration: 3.9296)\n",
      "    Batch [3300/6960] Loss: 1.1761 (Class: 0.8857, Duration: 4.9432)\n",
      "    Batch [3400/6960] Loss: 1.1529 (Class: 0.8413, Duration: 5.3029)\n",
      "    Batch [3500/6960] Loss: 0.9494 (Class: 0.6804, Duration: 4.5800)\n",
      "    Batch [3600/6960] Loss: 1.0616 (Class: 0.8196, Duration: 4.1186)\n",
      "    Batch [3700/6960] Loss: 1.1360 (Class: 0.8467, Duration: 4.9233)\n",
      "    Batch [3800/6960] Loss: 0.9276 (Class: 0.6888, Duration: 4.0640)\n",
      "    Batch [3900/6960] Loss: 0.9625 (Class: 0.7517, Duration: 3.5873)\n",
      "    Batch [4000/6960] Loss: 1.0437 (Class: 0.7813, Duration: 4.4675)\n",
      "    Batch [4100/6960] Loss: 1.0392 (Class: 0.8249, Duration: 3.6475)\n",
      "    Batch [4200/6960] Loss: 1.0315 (Class: 0.7670, Duration: 4.5019)\n",
      "    Batch [4300/6960] Loss: 0.9313 (Class: 0.7416, Duration: 3.2281)\n",
      "    Batch [4400/6960] Loss: 1.0394 (Class: 0.6870, Duration: 5.9993)\n",
      "    Batch [4500/6960] Loss: 0.9757 (Class: 0.7499, Duration: 3.8421)\n",
      "    Batch [4600/6960] Loss: 0.9693 (Class: 0.7195, Duration: 4.2521)\n",
      "    Batch [4700/6960] Loss: 0.9038 (Class: 0.6783, Duration: 3.8390)\n",
      "    Batch [4800/6960] Loss: 0.9059 (Class: 0.6838, Duration: 3.7807)\n",
      "    Batch [4900/6960] Loss: 1.0057 (Class: 0.7612, Duration: 4.1625)\n",
      "    Batch [5000/6960] Loss: 0.9937 (Class: 0.7245, Duration: 4.5813)\n",
      "    Batch [5100/6960] Loss: 1.0869 (Class: 0.7892, Duration: 5.0661)\n",
      "    Batch [5200/6960] Loss: 1.0693 (Class: 0.7352, Duration: 5.6863)\n",
      "    Batch [5300/6960] Loss: 0.9613 (Class: 0.6865, Duration: 4.6779)\n",
      "    Batch [5400/6960] Loss: 0.9251 (Class: 0.6214, Duration: 5.1695)\n",
      "    Batch [5500/6960] Loss: 1.0840 (Class: 0.8531, Duration: 3.9310)\n",
      "    Batch [5600/6960] Loss: 0.9215 (Class: 0.6912, Duration: 3.9215)\n",
      "    Batch [5700/6960] Loss: 0.9123 (Class: 0.6943, Duration: 3.7101)\n",
      "    Batch [5800/6960] Loss: 1.0946 (Class: 0.8683, Duration: 3.8516)\n",
      "    Batch [5900/6960] Loss: 0.7785 (Class: 0.5790, Duration: 3.3951)\n",
      "    Batch [6000/6960] Loss: 0.8907 (Class: 0.6848, Duration: 3.5042)\n",
      "    Batch [6100/6960] Loss: 1.0412 (Class: 0.7790, Duration: 4.4626)\n",
      "    Batch [6200/6960] Loss: 0.9646 (Class: 0.7308, Duration: 3.9790)\n",
      "    Batch [6300/6960] Loss: 1.0743 (Class: 0.8503, Duration: 3.8143)\n",
      "    Batch [6400/6960] Loss: 0.8871 (Class: 0.6708, Duration: 3.6826)\n",
      "    Batch [6500/6960] Loss: 1.0062 (Class: 0.7994, Duration: 3.5193)\n",
      "    Batch [6600/6960] Loss: 1.0679 (Class: 0.8146, Duration: 4.3120)\n",
      "    Batch [6700/6960] Loss: 0.9778 (Class: 0.7603, Duration: 3.7016)\n",
      "    Batch [6800/6960] Loss: 1.0195 (Class: 0.7478, Duration: 4.6248)\n",
      "    Batch [6900/6960] Loss: 0.9984 (Class: 0.7489, Duration: 4.2459)\n",
      "\n",
      "Train Loss: 1.0370, Train Acc: 63.99%\n",
      "Val Loss: 1.0235, Val Acc: 64.64%\n",
      "Val Duration MSE: 4.5929 (RMSE: 2.1431)\n",
      "Class Accuracies - Bull: 56.10%, Flat: 75.17%, Bear: 62.86%\n",
      "Learning Rate: 3.70e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  -> New best model saved! (best balanced accuracy)\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 3.2 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 4.2 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 5.4 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 6.2 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 7.3 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 9.4 GB (+6.6 GB)\n",
      "\n",
      "Epoch 4/80\n",
      "Duration weight: 0.063\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 1.0555 (Class: 0.7826, Duration: 4.3221)\n",
      "    Batch [100/7058] Loss: 0.9784 (Class: 0.7333, Duration: 3.8831)\n",
      "    Batch [200/7058] Loss: 1.0998 (Class: 0.8414, Duration: 4.0948)\n",
      "    Batch [300/7058] Loss: 1.0751 (Class: 0.7440, Duration: 5.2442)\n",
      "    Batch [400/7058] Loss: 0.9366 (Class: 0.7207, Duration: 3.4199)\n",
      "    Batch [500/7058] Loss: 1.0348 (Class: 0.7190, Duration: 5.0032)\n",
      "    Batch [600/7058] Loss: 0.9951 (Class: 0.6926, Duration: 4.7919)\n",
      "    Batch [700/7058] Loss: 1.0336 (Class: 0.7611, Duration: 4.3158)\n",
      "    Batch [800/7058] Loss: 1.0981 (Class: 0.7975, Duration: 4.7621)\n",
      "    Batch [900/7058] Loss: 0.9670 (Class: 0.7588, Duration: 3.2993)\n",
      "    Batch [1000/7058] Loss: 0.9129 (Class: 0.6347, Duration: 4.4072)\n",
      "    Batch [1100/7058] Loss: 1.0347 (Class: 0.7965, Duration: 3.7722)\n",
      "    Batch [1200/7058] Loss: 1.1607 (Class: 0.8700, Duration: 4.6064)\n",
      "    Batch [1300/7058] Loss: 1.0120 (Class: 0.7956, Duration: 3.4292)\n",
      "    Batch [1400/7058] Loss: 0.8639 (Class: 0.6725, Duration: 3.0325)\n",
      "    Batch [1500/7058] Loss: 0.9748 (Class: 0.8046, Duration: 2.6959)\n",
      "    Batch [1600/7058] Loss: 1.2241 (Class: 0.8607, Duration: 5.7568)\n",
      "    Batch [1700/7058] Loss: 0.9558 (Class: 0.6715, Duration: 4.5043)\n",
      "    Batch [1800/7058] Loss: 0.9596 (Class: 0.7032, Duration: 4.0621)\n",
      "    Batch [1900/7058] Loss: 0.9338 (Class: 0.7097, Duration: 3.5501)\n",
      "    Batch [2000/7058] Loss: 1.0518 (Class: 0.7498, Duration: 4.7852)\n",
      "    Batch [2100/7058] Loss: 1.0887 (Class: 0.7611, Duration: 5.1899)\n",
      "    Batch [2200/7058] Loss: 1.0605 (Class: 0.8306, Duration: 3.6429)\n",
      "    Batch [2300/7058] Loss: 1.0902 (Class: 0.6976, Duration: 6.2183)\n",
      "    Batch [2400/7058] Loss: 1.0495 (Class: 0.7854, Duration: 4.1841)\n",
      "    Batch [2500/7058] Loss: 1.0116 (Class: 0.8154, Duration: 3.1076)\n",
      "    Batch [2600/7058] Loss: 1.0476 (Class: 0.8163, Duration: 3.6637)\n",
      "    Batch [2700/7058] Loss: 1.0077 (Class: 0.7418, Duration: 4.2120)\n",
      "    Batch [2800/7058] Loss: 1.0938 (Class: 0.8026, Duration: 4.6129)\n",
      "    Batch [2900/7058] Loss: 1.0846 (Class: 0.7781, Duration: 4.8560)\n",
      "    Batch [3000/7058] Loss: 1.0148 (Class: 0.7666, Duration: 3.9330)\n",
      "    Batch [3100/7058] Loss: 1.0526 (Class: 0.7763, Duration: 4.3768)\n",
      "    Batch [3200/7058] Loss: 0.9693 (Class: 0.7442, Duration: 3.5660)\n",
      "    Batch [3300/7058] Loss: 1.0827 (Class: 0.8460, Duration: 3.7499)\n",
      "    Batch [3400/7058] Loss: 1.0281 (Class: 0.7735, Duration: 4.0324)\n",
      "    Batch [3500/7058] Loss: 1.0754 (Class: 0.8262, Duration: 3.9487)\n",
      "    Batch [3600/7058] Loss: 0.9670 (Class: 0.6986, Duration: 4.2511)\n",
      "    Batch [3700/7058] Loss: 1.0461 (Class: 0.7774, Duration: 4.2567)\n",
      "    Batch [3800/7058] Loss: 1.0209 (Class: 0.7333, Duration: 4.5569)\n",
      "    Batch [3900/7058] Loss: 1.0435 (Class: 0.8365, Duration: 3.2791)\n",
      "    Batch [4000/7058] Loss: 0.9797 (Class: 0.7213, Duration: 4.0933)\n",
      "    Batch [4100/7058] Loss: 1.1257 (Class: 0.8392, Duration: 4.5388)\n",
      "    Batch [4200/7058] Loss: 1.0312 (Class: 0.7639, Duration: 4.2349)\n",
      "    Batch [4300/7058] Loss: 1.0337 (Class: 0.8060, Duration: 3.6081)\n",
      "    Batch [4400/7058] Loss: 0.9592 (Class: 0.7172, Duration: 3.8327)\n",
      "    Batch [4500/7058] Loss: 1.1287 (Class: 0.8725, Duration: 4.0577)\n",
      "    Batch [4600/7058] Loss: 0.9477 (Class: 0.6823, Duration: 4.2049)\n",
      "    Batch [4700/7058] Loss: 1.0482 (Class: 0.7291, Duration: 5.0548)\n",
      "    Batch [4800/7058] Loss: 1.1095 (Class: 0.8675, Duration: 3.8334)\n",
      "    Batch [4900/7058] Loss: 1.0455 (Class: 0.7964, Duration: 3.9458)\n",
      "    Batch [5000/7058] Loss: 0.8335 (Class: 0.6744, Duration: 2.5201)\n",
      "    Batch [5100/7058] Loss: 1.0800 (Class: 0.7907, Duration: 4.5825)\n",
      "    Batch [5200/7058] Loss: 1.1294 (Class: 0.7510, Duration: 5.9931)\n",
      "    Batch [5300/7058] Loss: 1.0909 (Class: 0.8352, Duration: 4.0498)\n",
      "    Batch [5400/7058] Loss: 1.1114 (Class: 0.7766, Duration: 5.3033)\n",
      "    Batch [5500/7058] Loss: 1.0114 (Class: 0.7024, Duration: 4.8958)\n",
      "    Batch [5600/7058] Loss: 0.8712 (Class: 0.6332, Duration: 3.7704)\n",
      "    Batch [5700/7058] Loss: 1.0834 (Class: 0.8310, Duration: 3.9978)\n",
      "    Batch [5800/7058] Loss: 1.0748 (Class: 0.8024, Duration: 4.3151)\n",
      "    Batch [5900/7058] Loss: 1.0828 (Class: 0.7823, Duration: 4.7601)\n",
      "    Batch [6000/7058] Loss: 1.0272 (Class: 0.7883, Duration: 3.7839)\n",
      "    Batch [6100/7058] Loss: 0.9784 (Class: 0.6938, Duration: 4.5088)\n",
      "    Batch [6200/7058] Loss: 0.8776 (Class: 0.6538, Duration: 3.5457)\n",
      "    Batch [6300/7058] Loss: 0.9455 (Class: 0.7146, Duration: 3.6591)\n",
      "    Batch [6400/7058] Loss: 1.1190 (Class: 0.8586, Duration: 4.1266)\n",
      "    Batch [6500/7058] Loss: 0.9918 (Class: 0.7299, Duration: 4.1486)\n",
      "    Batch [6600/7058] Loss: 0.9387 (Class: 0.7588, Duration: 2.8500)\n",
      "    Batch [6700/7058] Loss: 1.1331 (Class: 0.8217, Duration: 4.9331)\n",
      "    Batch [6800/7058] Loss: 1.1459 (Class: 0.8053, Duration: 5.3966)\n",
      "    Batch [6900/7058] Loss: 0.9809 (Class: 0.7334, Duration: 3.9214)\n",
      "    Batch [7000/7058] Loss: 1.0536 (Class: 0.7317, Duration: 5.0987)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 4.0 GB (+0.0 GB)\n",
      "  Loaded 100/255 files, Memory: 4.3 GB (+0.3 GB)\n",
      "  Loaded 150/255 files, Memory: 4.8 GB (+0.8 GB)\n",
      "  Loaded 200/255 files, Memory: 5.4 GB (+1.4 GB)\n",
      "  Loaded 250/255 files, Memory: 6.3 GB (+2.3 GB)\n",
      "Loaded 800392 samples. Memory usage: 8.1 GB (+4.1 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 0.9326 (Class: 0.7084, Duration: 3.5526)\n",
      "    Batch [100/6960] Loss: 1.0424 (Class: 0.7645, Duration: 4.4031)\n",
      "    Batch [200/6960] Loss: 1.0144 (Class: 0.7901, Duration: 3.5534)\n",
      "    Batch [300/6960] Loss: 0.9660 (Class: 0.7400, Duration: 3.5806)\n",
      "    Batch [400/6960] Loss: 1.0571 (Class: 0.7785, Duration: 4.4131)\n",
      "    Batch [500/6960] Loss: 1.0426 (Class: 0.8064, Duration: 3.7432)\n",
      "    Batch [600/6960] Loss: 0.9581 (Class: 0.7322, Duration: 3.5781)\n",
      "    Batch [700/6960] Loss: 1.0087 (Class: 0.7624, Duration: 3.9020)\n",
      "    Batch [800/6960] Loss: 0.9671 (Class: 0.7455, Duration: 3.5106)\n",
      "    Batch [900/6960] Loss: 1.1229 (Class: 0.8548, Duration: 4.2473)\n",
      "    Batch [1000/6960] Loss: 1.0147 (Class: 0.7360, Duration: 4.4159)\n",
      "    Batch [1100/6960] Loss: 0.9875 (Class: 0.6938, Duration: 4.6523)\n",
      "    Batch [1200/6960] Loss: 1.0201 (Class: 0.7628, Duration: 4.0767)\n",
      "    Batch [1300/6960] Loss: 0.9751 (Class: 0.7747, Duration: 3.1747)\n",
      "    Batch [1400/6960] Loss: 0.9505 (Class: 0.7235, Duration: 3.5957)\n",
      "    Batch [1500/6960] Loss: 1.0316 (Class: 0.7359, Duration: 4.6849)\n",
      "    Batch [1600/6960] Loss: 1.1417 (Class: 0.8185, Duration: 5.1200)\n",
      "    Batch [1700/6960] Loss: 0.9824 (Class: 0.6591, Duration: 5.1210)\n",
      "    Batch [1800/6960] Loss: 0.7793 (Class: 0.5587, Duration: 3.4953)\n",
      "    Batch [1900/6960] Loss: 1.1791 (Class: 0.8463, Duration: 5.2726)\n",
      "    Batch [2000/6960] Loss: 1.1188 (Class: 0.7855, Duration: 5.2794)\n",
      "    Batch [2100/6960] Loss: 1.1124 (Class: 0.8698, Duration: 3.8426)\n",
      "    Batch [2200/6960] Loss: 0.9529 (Class: 0.7451, Duration: 3.2914)\n",
      "    Batch [2300/6960] Loss: 1.0343 (Class: 0.7807, Duration: 4.0170)\n",
      "    Batch [2400/6960] Loss: 0.9242 (Class: 0.6710, Duration: 4.0106)\n",
      "    Batch [2500/6960] Loss: 0.9607 (Class: 0.6704, Duration: 4.5987)\n",
      "    Batch [2600/6960] Loss: 0.9054 (Class: 0.7081, Duration: 3.1252)\n",
      "    Batch [2700/6960] Loss: 1.0570 (Class: 0.7597, Duration: 4.7088)\n",
      "    Batch [2800/6960] Loss: 0.9822 (Class: 0.7434, Duration: 3.7823)\n",
      "    Batch [2900/6960] Loss: 1.0389 (Class: 0.7774, Duration: 4.1429)\n",
      "    Batch [3000/6960] Loss: 1.1356 (Class: 0.8428, Duration: 4.6390)\n",
      "    Batch [3100/6960] Loss: 0.9466 (Class: 0.7105, Duration: 3.7414)\n",
      "    Batch [3200/6960] Loss: 1.1109 (Class: 0.8108, Duration: 4.7540)\n",
      "    Batch [3300/6960] Loss: 1.0718 (Class: 0.8292, Duration: 3.8431)\n",
      "    Batch [3400/6960] Loss: 0.9306 (Class: 0.6687, Duration: 4.1491)\n",
      "    Batch [3500/6960] Loss: 0.9560 (Class: 0.7129, Duration: 3.8516)\n",
      "    Batch [3600/6960] Loss: 0.9710 (Class: 0.7507, Duration: 3.4907)\n",
      "    Batch [3700/6960] Loss: 0.9606 (Class: 0.6581, Duration: 4.7930)\n",
      "    Batch [3800/6960] Loss: 1.1194 (Class: 0.7255, Duration: 6.2405)\n",
      "    Batch [3900/6960] Loss: 0.8957 (Class: 0.6497, Duration: 3.8969)\n",
      "    Batch [4000/6960] Loss: 0.8947 (Class: 0.6348, Duration: 4.1173)\n",
      "    Batch [4100/6960] Loss: 0.9595 (Class: 0.5880, Duration: 5.8862)\n",
      "    Batch [4200/6960] Loss: 1.0408 (Class: 0.8074, Duration: 3.6967)\n",
      "    Batch [4300/6960] Loss: 0.8090 (Class: 0.5507, Duration: 4.0920)\n",
      "    Batch [4400/6960] Loss: 0.9220 (Class: 0.7280, Duration: 3.0731)\n",
      "    Batch [4500/6960] Loss: 0.9520 (Class: 0.7167, Duration: 3.7276)\n",
      "    Batch [4600/6960] Loss: 1.0186 (Class: 0.7721, Duration: 3.9038)\n",
      "    Batch [4700/6960] Loss: 0.8307 (Class: 0.6454, Duration: 2.9362)\n",
      "    Batch [4800/6960] Loss: 0.9757 (Class: 0.7303, Duration: 3.8882)\n",
      "    Batch [4900/6960] Loss: 0.9503 (Class: 0.6726, Duration: 4.3993)\n",
      "    Batch [5000/6960] Loss: 1.0351 (Class: 0.7365, Duration: 4.7297)\n",
      "    Batch [5100/6960] Loss: 0.9220 (Class: 0.6218, Duration: 4.7548)\n",
      "    Batch [5200/6960] Loss: 0.9458 (Class: 0.7192, Duration: 3.5904)\n",
      "    Batch [5300/6960] Loss: 1.0114 (Class: 0.7521, Duration: 4.1079)\n",
      "    Batch [5400/6960] Loss: 0.9487 (Class: 0.6772, Duration: 4.3011)\n",
      "    Batch [5500/6960] Loss: 0.9909 (Class: 0.7619, Duration: 3.6282)\n",
      "    Batch [5600/6960] Loss: 0.8766 (Class: 0.6872, Duration: 2.9999)\n",
      "    Batch [5700/6960] Loss: 0.9795 (Class: 0.7121, Duration: 4.2360)\n",
      "    Batch [5800/6960] Loss: 0.8720 (Class: 0.6605, Duration: 3.3508)\n",
      "    Batch [5900/6960] Loss: 0.9489 (Class: 0.7117, Duration: 3.7567)\n",
      "    Batch [6000/6960] Loss: 0.8059 (Class: 0.6190, Duration: 2.9614)\n",
      "    Batch [6100/6960] Loss: 1.0052 (Class: 0.7070, Duration: 4.7231)\n",
      "    Batch [6200/6960] Loss: 0.8969 (Class: 0.6967, Duration: 3.1710)\n",
      "    Batch [6300/6960] Loss: 1.0673 (Class: 0.7708, Duration: 4.6966)\n",
      "    Batch [6400/6960] Loss: 0.9571 (Class: 0.7326, Duration: 3.5558)\n",
      "    Batch [6500/6960] Loss: 0.9513 (Class: 0.7237, Duration: 3.6050)\n",
      "    Batch [6600/6960] Loss: 0.9506 (Class: 0.7126, Duration: 3.7709)\n",
      "    Batch [6700/6960] Loss: 0.9154 (Class: 0.6659, Duration: 3.9524)\n",
      "    Batch [6800/6960] Loss: 0.9389 (Class: 0.6651, Duration: 4.3369)\n",
      "    Batch [6900/6960] Loss: 0.8471 (Class: 0.6210, Duration: 3.5804)\n",
      "\n",
      "Train Loss: 1.0191, Train Acc: 65.62%\n",
      "Val Loss: 0.9979, Val Acc: 67.47%\n",
      "Val Duration MSE: 4.3035 (RMSE: 2.0745)\n",
      "Class Accuracies - Bull: 64.41%, Flat: 66.60%, Bear: 73.69%\n",
      "Learning Rate: 4.60e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  -> New best model saved! (best duration MSE)\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.9 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.9 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 4.2 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 5.0 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 6.1 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 7.2 GB (+5.7 GB)\n",
      "\n",
      "Epoch 5/80\n",
      "Duration weight: 0.068\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 1.0214 (Class: 0.7252, Duration: 4.3880)\n",
      "    Batch [100/7058] Loss: 1.0894 (Class: 0.7845, Duration: 4.5167)\n",
      "    Batch [200/7058] Loss: 0.8999 (Class: 0.6909, Duration: 3.0962)\n",
      "    Batch [300/7058] Loss: 1.0361 (Class: 0.8113, Duration: 3.3309)\n",
      "    Batch [400/7058] Loss: 1.0117 (Class: 0.7635, Duration: 3.6770)\n",
      "    Batch [500/7058] Loss: 1.0158 (Class: 0.7846, Duration: 3.4260)\n",
      "    Batch [600/7058] Loss: 0.9664 (Class: 0.7339, Duration: 3.4437)\n",
      "    Batch [700/7058] Loss: 1.1593 (Class: 0.8214, Duration: 5.0066)\n",
      "    Batch [800/7058] Loss: 0.9979 (Class: 0.7040, Duration: 4.3529)\n",
      "    Batch [900/7058] Loss: 1.0664 (Class: 0.7155, Duration: 5.1991)\n",
      "    Batch [1000/7058] Loss: 1.0057 (Class: 0.7186, Duration: 4.2538)\n",
      "    Batch [1100/7058] Loss: 1.1482 (Class: 0.8205, Duration: 4.8559)\n",
      "    Batch [1200/7058] Loss: 1.0605 (Class: 0.7102, Duration: 5.1898)\n",
      "    Batch [1300/7058] Loss: 0.8939 (Class: 0.6451, Duration: 3.6856)\n",
      "    Batch [1400/7058] Loss: 1.1152 (Class: 0.7755, Duration: 5.0328)\n",
      "    Batch [1500/7058] Loss: 1.0784 (Class: 0.7984, Duration: 4.1482)\n",
      "    Batch [1600/7058] Loss: 0.9077 (Class: 0.6741, Duration: 3.4604)\n",
      "    Batch [1700/7058] Loss: 1.0649 (Class: 0.7827, Duration: 4.1812)\n",
      "    Batch [1800/7058] Loss: 0.9690 (Class: 0.7157, Duration: 3.7524)\n",
      "    Batch [1900/7058] Loss: 1.1528 (Class: 0.8705, Duration: 4.1820)\n",
      "    Batch [2000/7058] Loss: 0.9931 (Class: 0.7009, Duration: 4.3288)\n",
      "    Batch [2100/7058] Loss: 1.0542 (Class: 0.7491, Duration: 4.5201)\n",
      "    Batch [2200/7058] Loss: 1.0053 (Class: 0.6815, Duration: 4.7975)\n",
      "    Batch [2300/7058] Loss: 1.0363 (Class: 0.7563, Duration: 4.1473)\n",
      "    Batch [2400/7058] Loss: 1.0282 (Class: 0.7515, Duration: 4.0990)\n",
      "    Batch [2500/7058] Loss: 1.0139 (Class: 0.7367, Duration: 4.1060)\n",
      "    Batch [2600/7058] Loss: 1.2632 (Class: 0.8673, Duration: 5.8664)\n",
      "    Batch [2700/7058] Loss: 0.9738 (Class: 0.6966, Duration: 4.1063)\n",
      "    Batch [2800/7058] Loss: 1.0176 (Class: 0.7255, Duration: 4.3276)\n",
      "    Batch [2900/7058] Loss: 0.9661 (Class: 0.6487, Duration: 4.7014)\n",
      "    Batch [3000/7058] Loss: 1.1140 (Class: 0.8200, Duration: 4.3544)\n",
      "    Batch [3100/7058] Loss: 0.9604 (Class: 0.7145, Duration: 3.6436)\n",
      "    Batch [3200/7058] Loss: 1.0524 (Class: 0.7590, Duration: 4.3459)\n",
      "    Batch [3300/7058] Loss: 0.9259 (Class: 0.6027, Duration: 4.7884)\n",
      "    Batch [3400/7058] Loss: 0.9906 (Class: 0.6809, Duration: 4.5882)\n",
      "    Batch [3500/7058] Loss: 1.0093 (Class: 0.6881, Duration: 4.7584)\n",
      "    Batch [3600/7058] Loss: 1.0172 (Class: 0.8367, Duration: 2.6737)\n",
      "    Batch [3700/7058] Loss: 1.0420 (Class: 0.8180, Duration: 3.3178)\n",
      "    Batch [3800/7058] Loss: 1.0737 (Class: 0.7735, Duration: 4.4473)\n",
      "    Batch [3900/7058] Loss: 0.9970 (Class: 0.7085, Duration: 4.2744)\n",
      "    Batch [4000/7058] Loss: 1.0534 (Class: 0.7412, Duration: 4.6241)\n",
      "    Batch [4100/7058] Loss: 1.0702 (Class: 0.7892, Duration: 4.1633)\n",
      "    Batch [4200/7058] Loss: 1.0082 (Class: 0.7881, Duration: 3.2606)\n",
      "    Batch [4300/7058] Loss: 0.9918 (Class: 0.7595, Duration: 3.4423)\n",
      "    Batch [4400/7058] Loss: 1.1205 (Class: 0.7893, Duration: 4.9073)\n",
      "    Batch [4500/7058] Loss: 0.8804 (Class: 0.6300, Duration: 3.7090)\n",
      "    Batch [4600/7058] Loss: 0.9332 (Class: 0.7168, Duration: 3.2061)\n",
      "    Batch [4700/7058] Loss: 0.9472 (Class: 0.7066, Duration: 3.5642)\n",
      "    Batch [4800/7058] Loss: 0.9442 (Class: 0.7229, Duration: 3.2782)\n",
      "    Batch [4900/7058] Loss: 1.1130 (Class: 0.8847, Duration: 3.3834)\n",
      "    Batch [5000/7058] Loss: 0.9579 (Class: 0.7646, Duration: 2.8642)\n",
      "    Batch [5100/7058] Loss: 1.0158 (Class: 0.7286, Duration: 4.2548)\n",
      "    Batch [5200/7058] Loss: 1.0141 (Class: 0.7313, Duration: 4.1895)\n",
      "    Batch [5300/7058] Loss: 1.0086 (Class: 0.7674, Duration: 3.5747)\n",
      "    Batch [5400/7058] Loss: 1.0594 (Class: 0.7446, Duration: 4.6649)\n",
      "    Batch [5500/7058] Loss: 0.9301 (Class: 0.6635, Duration: 3.9496)\n",
      "    Batch [5600/7058] Loss: 0.9941 (Class: 0.7277, Duration: 3.9460)\n",
      "    Batch [5700/7058] Loss: 0.9934 (Class: 0.7205, Duration: 4.0434)\n",
      "    Batch [5800/7058] Loss: 1.0917 (Class: 0.7580, Duration: 4.9440)\n",
      "    Batch [5900/7058] Loss: 1.0471 (Class: 0.7196, Duration: 4.8531)\n",
      "    Batch [6000/7058] Loss: 1.0192 (Class: 0.7150, Duration: 4.5071)\n",
      "    Batch [6100/7058] Loss: 0.9931 (Class: 0.7586, Duration: 3.4747)\n",
      "    Batch [6200/7058] Loss: 1.0682 (Class: 0.7860, Duration: 4.1812)\n",
      "    Batch [6300/7058] Loss: 0.9710 (Class: 0.7671, Duration: 3.0204)\n",
      "    Batch [6400/7058] Loss: 0.9143 (Class: 0.6269, Duration: 4.2580)\n",
      "    Batch [6500/7058] Loss: 0.9997 (Class: 0.7364, Duration: 3.8999)\n",
      "    Batch [6600/7058] Loss: 1.0026 (Class: 0.7618, Duration: 3.5671)\n",
      "    Batch [6700/7058] Loss: 1.0465 (Class: 0.8075, Duration: 3.5407)\n",
      "    Batch [6800/7058] Loss: 1.0594 (Class: 0.7804, Duration: 4.1342)\n",
      "    Batch [6900/7058] Loss: 0.9375 (Class: 0.6309, Duration: 4.5424)\n",
      "    Batch [7000/7058] Loss: 1.0795 (Class: 0.7507, Duration: 4.8710)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 2.0 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 3.0 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.9 GB (+2.4 GB)\n",
      "  Loaded 200/255 files, Memory: 5.1 GB (+3.5 GB)\n",
      "  Loaded 250/255 files, Memory: 6.1 GB (+4.5 GB)\n",
      "Loaded 800392 samples. Memory usage: 6.8 GB (+5.2 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 1.2581 (Class: 0.7979, Duration: 6.8183)\n",
      "    Batch [100/6960] Loss: 1.0299 (Class: 0.6358, Duration: 5.8388)\n",
      "    Batch [200/6960] Loss: 0.9975 (Class: 0.7113, Duration: 4.2397)\n",
      "    Batch [300/6960] Loss: 0.9892 (Class: 0.7051, Duration: 4.2091)\n",
      "    Batch [400/6960] Loss: 1.0668 (Class: 0.7747, Duration: 4.3268)\n",
      "    Batch [500/6960] Loss: 0.9182 (Class: 0.6496, Duration: 3.9786)\n",
      "    Batch [600/6960] Loss: 1.0882 (Class: 0.8685, Duration: 3.2554)\n",
      "    Batch [700/6960] Loss: 0.9762 (Class: 0.7380, Duration: 3.5290)\n",
      "    Batch [800/6960] Loss: 0.9488 (Class: 0.7077, Duration: 3.5719)\n",
      "    Batch [900/6960] Loss: 0.9400 (Class: 0.7051, Duration: 3.4810)\n",
      "    Batch [1000/6960] Loss: 1.0557 (Class: 0.7969, Duration: 3.8334)\n",
      "    Batch [1100/6960] Loss: 1.0380 (Class: 0.7350, Duration: 4.4889)\n",
      "    Batch [1200/6960] Loss: 0.9454 (Class: 0.6666, Duration: 4.1305)\n",
      "    Batch [1300/6960] Loss: 0.8667 (Class: 0.6051, Duration: 3.8747)\n",
      "    Batch [1400/6960] Loss: 0.8356 (Class: 0.5956, Duration: 3.5561)\n",
      "    Batch [1500/6960] Loss: 0.9592 (Class: 0.6528, Duration: 4.5386)\n",
      "    Batch [1600/6960] Loss: 0.9415 (Class: 0.6384, Duration: 4.4913)\n",
      "    Batch [1700/6960] Loss: 1.1082 (Class: 0.7144, Duration: 5.8335)\n",
      "    Batch [1800/6960] Loss: 0.9748 (Class: 0.7367, Duration: 3.5278)\n",
      "    Batch [1900/6960] Loss: 0.8479 (Class: 0.5477, Duration: 4.4465)\n",
      "    Batch [2000/6960] Loss: 0.9170 (Class: 0.6632, Duration: 3.7601)\n",
      "    Batch [2100/6960] Loss: 0.9682 (Class: 0.7113, Duration: 3.8057)\n",
      "    Batch [2200/6960] Loss: 0.9926 (Class: 0.7263, Duration: 3.9444)\n",
      "    Batch [2300/6960] Loss: 1.0666 (Class: 0.7743, Duration: 4.3306)\n",
      "    Batch [2400/6960] Loss: 1.0718 (Class: 0.7777, Duration: 4.3582)\n",
      "    Batch [2500/6960] Loss: 0.8901 (Class: 0.6623, Duration: 3.3755)\n",
      "    Batch [2600/6960] Loss: 0.9568 (Class: 0.7139, Duration: 3.5985)\n",
      "    Batch [2700/6960] Loss: 0.9261 (Class: 0.7085, Duration: 3.2239)\n",
      "    Batch [2800/6960] Loss: 0.8953 (Class: 0.6534, Duration: 3.5834)\n",
      "    Batch [2900/6960] Loss: 1.0105 (Class: 0.8043, Duration: 3.0550)\n",
      "    Batch [3000/6960] Loss: 0.9505 (Class: 0.6741, Duration: 4.0949)\n",
      "    Batch [3100/6960] Loss: 1.0396 (Class: 0.7342, Duration: 4.5242)\n",
      "    Batch [3200/6960] Loss: 0.9749 (Class: 0.7345, Duration: 3.5622)\n",
      "    Batch [3300/6960] Loss: 1.0379 (Class: 0.7810, Duration: 3.8069)\n",
      "    Batch [3400/6960] Loss: 0.9799 (Class: 0.7628, Duration: 3.2154)\n",
      "    Batch [3500/6960] Loss: 0.9089 (Class: 0.6211, Duration: 4.2632)\n",
      "    Batch [3600/6960] Loss: 0.9982 (Class: 0.7788, Duration: 3.2499)\n",
      "    Batch [3700/6960] Loss: 0.9011 (Class: 0.6756, Duration: 3.3411)\n",
      "    Batch [3800/6960] Loss: 0.9693 (Class: 0.7662, Duration: 3.0085)\n",
      "    Batch [3900/6960] Loss: 0.9483 (Class: 0.7343, Duration: 3.1694)\n",
      "    Batch [4000/6960] Loss: 1.1125 (Class: 0.8353, Duration: 4.1073)\n",
      "    Batch [4100/6960] Loss: 0.8585 (Class: 0.6407, Duration: 3.2268)\n",
      "    Batch [4200/6960] Loss: 1.0302 (Class: 0.7636, Duration: 3.9497)\n",
      "    Batch [4300/6960] Loss: 0.9857 (Class: 0.6467, Duration: 5.0224)\n",
      "    Batch [4400/6960] Loss: 1.0324 (Class: 0.7566, Duration: 4.0858)\n",
      "    Batch [4500/6960] Loss: 0.9621 (Class: 0.7308, Duration: 3.4270)\n",
      "    Batch [4600/6960] Loss: 1.0552 (Class: 0.6925, Duration: 5.3725)\n",
      "    Batch [4700/6960] Loss: 1.0022 (Class: 0.7345, Duration: 3.9655)\n",
      "    Batch [4800/6960] Loss: 1.0211 (Class: 0.7084, Duration: 4.6324)\n",
      "    Batch [4900/6960] Loss: 0.9076 (Class: 0.6495, Duration: 3.8247)\n",
      "    Batch [5000/6960] Loss: 0.9089 (Class: 0.7036, Duration: 3.0426)\n",
      "    Batch [5100/6960] Loss: 1.0204 (Class: 0.7314, Duration: 4.2814)\n",
      "    Batch [5200/6960] Loss: 1.0741 (Class: 0.7609, Duration: 4.6397)\n",
      "    Batch [5300/6960] Loss: 1.0074 (Class: 0.7547, Duration: 3.7429)\n",
      "    Batch [5400/6960] Loss: 1.0116 (Class: 0.7136, Duration: 4.4147)\n",
      "    Batch [5500/6960] Loss: 0.9770 (Class: 0.7603, Duration: 3.2107)\n",
      "    Batch [5600/6960] Loss: 0.9515 (Class: 0.6878, Duration: 3.9061)\n",
      "    Batch [5700/6960] Loss: 0.8220 (Class: 0.6093, Duration: 3.1524)\n",
      "    Batch [5800/6960] Loss: 0.9398 (Class: 0.6506, Duration: 4.2852)\n",
      "    Batch [5900/6960] Loss: 0.9820 (Class: 0.7150, Duration: 3.9549)\n",
      "    Batch [6000/6960] Loss: 0.9575 (Class: 0.6914, Duration: 3.9420)\n",
      "    Batch [6100/6960] Loss: 0.9396 (Class: 0.7044, Duration: 3.4843)\n",
      "    Batch [6200/6960] Loss: 0.8177 (Class: 0.5844, Duration: 3.4563)\n",
      "    Batch [6300/6960] Loss: 0.8960 (Class: 0.7154, Duration: 2.6755)\n",
      "    Batch [6400/6960] Loss: 0.8869 (Class: 0.6533, Duration: 3.4604)\n",
      "    Batch [6500/6960] Loss: 1.0172 (Class: 0.7327, Duration: 4.2160)\n",
      "    Batch [6600/6960] Loss: 0.9161 (Class: 0.6229, Duration: 4.3431)\n",
      "    Batch [6700/6960] Loss: 0.9198 (Class: 0.6677, Duration: 3.7351)\n",
      "    Batch [6800/6960] Loss: 0.8648 (Class: 0.6474, Duration: 3.2208)\n",
      "    Batch [6900/6960] Loss: 0.8959 (Class: 0.6272, Duration: 3.9804)\n",
      "\n",
      "Train Loss: 0.9989, Train Acc: 67.21%\n",
      "Val Loss: 0.9783, Val Acc: 69.47%\n",
      "Val Duration MSE: 4.1971 (RMSE: 2.0487)\n",
      "Class Accuracies - Bull: 68.53%, Flat: 70.40%, Bear: 69.62%\n",
      "Learning Rate: 5.50e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  -> New best model saved! (best duration MSE)\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.8 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.8 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 4.1 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 4.9 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 6.0 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 6.9 GB (+5.4 GB)\n",
      "\n",
      "Epoch 6/80\n",
      "Duration weight: 0.072\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 1.0494 (Class: 0.7475, Duration: 4.2004)\n",
      "    Batch [100/7058] Loss: 1.0031 (Class: 0.6964, Duration: 4.2663)\n",
      "    Batch [200/7058] Loss: 0.9195 (Class: 0.6774, Duration: 3.3681)\n",
      "    Batch [300/7058] Loss: 0.9508 (Class: 0.6794, Duration: 3.7764)\n",
      "    Batch [400/7058] Loss: 1.0108 (Class: 0.7041, Duration: 4.2683)\n",
      "    Batch [500/7058] Loss: 1.0377 (Class: 0.7417, Duration: 4.1189)\n",
      "    Batch [600/7058] Loss: 1.0055 (Class: 0.7317, Duration: 3.8097)\n",
      "    Batch [700/7058] Loss: 1.0599 (Class: 0.7852, Duration: 3.8218)\n",
      "    Batch [800/7058] Loss: 0.9283 (Class: 0.7061, Duration: 3.0912)\n",
      "    Batch [900/7058] Loss: 0.9940 (Class: 0.7179, Duration: 3.8412)\n",
      "    Batch [1000/7058] Loss: 1.0689 (Class: 0.7260, Duration: 4.7705)\n",
      "    Batch [1100/7058] Loss: 1.0790 (Class: 0.7820, Duration: 4.1326)\n",
      "    Batch [1200/7058] Loss: 1.1596 (Class: 0.8281, Duration: 4.6125)\n",
      "    Batch [1300/7058] Loss: 1.2065 (Class: 0.8894, Duration: 4.4118)\n",
      "    Batch [1400/7058] Loss: 1.0307 (Class: 0.7609, Duration: 3.7548)\n",
      "    Batch [1500/7058] Loss: 1.0096 (Class: 0.7622, Duration: 3.4427)\n",
      "    Batch [1600/7058] Loss: 0.9836 (Class: 0.6591, Duration: 4.5138)\n",
      "    Batch [1700/7058] Loss: 0.8621 (Class: 0.6377, Duration: 3.1223)\n",
      "    Batch [1800/7058] Loss: 0.9274 (Class: 0.6371, Duration: 4.0389)\n",
      "    Batch [1900/7058] Loss: 1.0460 (Class: 0.7191, Duration: 4.5480)\n",
      "    Batch [2000/7058] Loss: 0.9859 (Class: 0.7210, Duration: 3.6860)\n",
      "    Batch [2100/7058] Loss: 1.0403 (Class: 0.7521, Duration: 4.0110)\n",
      "    Batch [2200/7058] Loss: 0.9355 (Class: 0.6012, Duration: 4.6511)\n",
      "    Batch [2300/7058] Loss: 1.0380 (Class: 0.7821, Duration: 3.5604)\n",
      "    Batch [2400/7058] Loss: 0.9591 (Class: 0.6856, Duration: 3.8050)\n",
      "    Batch [2500/7058] Loss: 0.9215 (Class: 0.6533, Duration: 3.7315)\n",
      "    Batch [2600/7058] Loss: 0.8093 (Class: 0.5554, Duration: 3.5319)\n",
      "    Batch [2700/7058] Loss: 0.9928 (Class: 0.7506, Duration: 3.3701)\n",
      "    Batch [2800/7058] Loss: 0.8847 (Class: 0.6039, Duration: 3.9069)\n",
      "    Batch [2900/7058] Loss: 0.9337 (Class: 0.6406, Duration: 4.0774)\n",
      "    Batch [3000/7058] Loss: 0.9874 (Class: 0.6926, Duration: 4.1006)\n",
      "    Batch [3100/7058] Loss: 1.0684 (Class: 0.7683, Duration: 4.1754)\n",
      "    Batch [3200/7058] Loss: 1.0839 (Class: 0.7343, Duration: 4.8646)\n",
      "    Batch [3300/7058] Loss: 1.0756 (Class: 0.7956, Duration: 3.8961)\n",
      "    Batch [3400/7058] Loss: 0.9862 (Class: 0.7409, Duration: 3.4131)\n",
      "    Batch [3500/7058] Loss: 0.8691 (Class: 0.5950, Duration: 3.8129)\n",
      "    Batch [3600/7058] Loss: 1.0173 (Class: 0.7793, Duration: 3.3118)\n",
      "    Batch [3700/7058] Loss: 0.9049 (Class: 0.6453, Duration: 3.6113)\n",
      "    Batch [3800/7058] Loss: 0.9621 (Class: 0.7224, Duration: 3.3350)\n",
      "    Batch [3900/7058] Loss: 1.0731 (Class: 0.7982, Duration: 3.8255)\n",
      "    Batch [4000/7058] Loss: 0.9303 (Class: 0.6844, Duration: 3.4211)\n",
      "    Batch [4100/7058] Loss: 1.0183 (Class: 0.7416, Duration: 3.8494)\n",
      "    Batch [4200/7058] Loss: 1.0412 (Class: 0.6633, Duration: 5.2581)\n",
      "    Batch [4300/7058] Loss: 0.9444 (Class: 0.6418, Duration: 4.2106)\n",
      "    Batch [4400/7058] Loss: 1.0280 (Class: 0.6447, Duration: 5.3325)\n",
      "    Batch [4500/7058] Loss: 1.0428 (Class: 0.7528, Duration: 4.0353)\n",
      "    Batch [4600/7058] Loss: 1.0070 (Class: 0.7675, Duration: 3.3322)\n",
      "    Batch [4700/7058] Loss: 0.9942 (Class: 0.6974, Duration: 4.1292)\n",
      "    Batch [4800/7058] Loss: 0.9801 (Class: 0.6865, Duration: 4.0847)\n",
      "    Batch [4900/7058] Loss: 0.9301 (Class: 0.7387, Duration: 2.6640)\n",
      "    Batch [5000/7058] Loss: 0.9352 (Class: 0.6724, Duration: 3.6569)\n",
      "    Batch [5100/7058] Loss: 1.0155 (Class: 0.7177, Duration: 4.1431)\n",
      "    Batch [5200/7058] Loss: 1.0898 (Class: 0.7962, Duration: 4.0854)\n",
      "    Batch [5300/7058] Loss: 1.0392 (Class: 0.7770, Duration: 3.6480)\n",
      "    Batch [5400/7058] Loss: 0.9526 (Class: 0.6499, Duration: 4.2114)\n",
      "    Batch [5500/7058] Loss: 0.9792 (Class: 0.6697, Duration: 4.3060)\n",
      "    Batch [5600/7058] Loss: 0.9450 (Class: 0.6496, Duration: 4.1099)\n",
      "    Batch [5700/7058] Loss: 0.8567 (Class: 0.5964, Duration: 3.6219)\n",
      "    Batch [5800/7058] Loss: 1.0483 (Class: 0.7444, Duration: 4.2291)\n",
      "    Batch [5900/7058] Loss: 1.0253 (Class: 0.8086, Duration: 3.0146)\n",
      "    Batch [6000/7058] Loss: 0.9572 (Class: 0.6746, Duration: 3.9323)\n",
      "    Batch [6100/7058] Loss: 0.9692 (Class: 0.7684, Duration: 2.7934)\n",
      "    Batch [6200/7058] Loss: 0.9264 (Class: 0.6395, Duration: 3.9926)\n",
      "    Batch [6300/7058] Loss: 0.9866 (Class: 0.7112, Duration: 3.8314)\n",
      "    Batch [6400/7058] Loss: 0.9342 (Class: 0.6453, Duration: 4.0202)\n",
      "    Batch [6500/7058] Loss: 0.9469 (Class: 0.6877, Duration: 3.6065)\n",
      "    Batch [6600/7058] Loss: 0.8526 (Class: 0.6096, Duration: 3.3817)\n",
      "    Batch [6700/7058] Loss: 1.0896 (Class: 0.7723, Duration: 4.4147)\n",
      "    Batch [6800/7058] Loss: 0.9824 (Class: 0.6503, Duration: 4.6202)\n",
      "    Batch [6900/7058] Loss: 1.0884 (Class: 0.7845, Duration: 4.2270)\n",
      "    Batch [7000/7058] Loss: 1.0945 (Class: 0.8007, Duration: 4.0879)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.9 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 2.9 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.7 GB (+2.2 GB)\n",
      "  Loaded 200/255 files, Memory: 4.9 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 5.9 GB (+4.4 GB)\n",
      "Loaded 800392 samples. Memory usage: 9.1 GB (+7.6 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 0.9682 (Class: 0.6657, Duration: 4.2086)\n",
      "    Batch [100/6960] Loss: 1.0759 (Class: 0.7859, Duration: 4.0339)\n",
      "    Batch [200/6960] Loss: 0.9180 (Class: 0.7021, Duration: 3.0039)\n",
      "    Batch [300/6960] Loss: 0.9594 (Class: 0.6738, Duration: 3.9741)\n",
      "    Batch [400/6960] Loss: 1.0456 (Class: 0.7286, Duration: 4.4103)\n",
      "    Batch [500/6960] Loss: 1.0904 (Class: 0.8502, Duration: 3.3417)\n",
      "    Batch [600/6960] Loss: 0.9989 (Class: 0.6690, Duration: 4.5900)\n",
      "    Batch [700/6960] Loss: 0.8765 (Class: 0.6747, Duration: 2.8065)\n",
      "    Batch [800/6960] Loss: 1.0557 (Class: 0.8148, Duration: 3.3507)\n",
      "    Batch [900/6960] Loss: 1.0197 (Class: 0.6863, Duration: 4.6384)\n",
      "    Batch [1000/6960] Loss: 0.9447 (Class: 0.6715, Duration: 3.8018)\n",
      "    Batch [1100/6960] Loss: 0.9347 (Class: 0.6605, Duration: 3.8145)\n",
      "    Batch [1200/6960] Loss: 0.8859 (Class: 0.6381, Duration: 3.4473)\n",
      "    Batch [1300/6960] Loss: 0.9599 (Class: 0.7596, Duration: 2.7873)\n",
      "    Batch [1400/6960] Loss: 0.9859 (Class: 0.7097, Duration: 3.8432)\n",
      "    Batch [1500/6960] Loss: 0.9190 (Class: 0.6624, Duration: 3.5702)\n",
      "    Batch [1600/6960] Loss: 0.9605 (Class: 0.7344, Duration: 3.1450)\n",
      "    Batch [1700/6960] Loss: 1.0865 (Class: 0.8211, Duration: 3.6924)\n",
      "    Batch [1800/6960] Loss: 1.0258 (Class: 0.7009, Duration: 4.5213)\n",
      "    Batch [1900/6960] Loss: 0.9678 (Class: 0.7322, Duration: 3.2778)\n",
      "    Batch [2000/6960] Loss: 0.9662 (Class: 0.6218, Duration: 4.7910)\n",
      "    Batch [2100/6960] Loss: 1.1498 (Class: 0.8013, Duration: 4.8478)\n",
      "    Batch [2200/6960] Loss: 1.1100 (Class: 0.8295, Duration: 3.9036)\n",
      "    Batch [2300/6960] Loss: 0.8294 (Class: 0.5913, Duration: 3.3126)\n",
      "    Batch [2400/6960] Loss: 0.9626 (Class: 0.6741, Duration: 4.0131)\n",
      "    Batch [2500/6960] Loss: 1.0770 (Class: 0.7483, Duration: 4.5724)\n",
      "    Batch [2600/6960] Loss: 0.8548 (Class: 0.5935, Duration: 3.6351)\n",
      "    Batch [2700/6960] Loss: 0.8391 (Class: 0.5549, Duration: 3.9537)\n",
      "    Batch [2800/6960] Loss: 0.9564 (Class: 0.7494, Duration: 2.8793)\n",
      "    Batch [2900/6960] Loss: 0.9554 (Class: 0.7036, Duration: 3.5030)\n",
      "    Batch [3000/6960] Loss: 0.8634 (Class: 0.6548, Duration: 2.9027)\n",
      "    Batch [3100/6960] Loss: 0.9391 (Class: 0.6396, Duration: 4.1667)\n",
      "    Batch [3200/6960] Loss: 0.9341 (Class: 0.6365, Duration: 4.1398)\n",
      "    Batch [3300/6960] Loss: 0.9792 (Class: 0.7622, Duration: 3.0187)\n",
      "    Batch [3400/6960] Loss: 0.8673 (Class: 0.6504, Duration: 3.0173)\n",
      "    Batch [3500/6960] Loss: 0.9415 (Class: 0.6295, Duration: 4.3415)\n",
      "    Batch [3600/6960] Loss: 1.1644 (Class: 0.8660, Duration: 4.1516)\n",
      "    Batch [3700/6960] Loss: 0.9384 (Class: 0.6660, Duration: 3.7896)\n",
      "    Batch [3800/6960] Loss: 1.0285 (Class: 0.7850, Duration: 3.3872)\n",
      "    Batch [3900/6960] Loss: 1.0093 (Class: 0.7145, Duration: 4.1012)\n",
      "    Batch [4000/6960] Loss: 0.8989 (Class: 0.5953, Duration: 4.2238)\n",
      "    Batch [4100/6960] Loss: 0.8791 (Class: 0.6480, Duration: 3.2160)\n",
      "    Batch [4200/6960] Loss: 0.9474 (Class: 0.6138, Duration: 4.6420)\n",
      "    Batch [4300/6960] Loss: 0.8543 (Class: 0.6295, Duration: 3.1277)\n",
      "    Batch [4400/6960] Loss: 0.9096 (Class: 0.6032, Duration: 4.2619)\n",
      "    Batch [4500/6960] Loss: 0.9449 (Class: 0.7299, Duration: 2.9911)\n",
      "    Batch [4600/6960] Loss: 1.0009 (Class: 0.6826, Duration: 4.4283)\n",
      "    Batch [4700/6960] Loss: 0.9662 (Class: 0.7071, Duration: 3.6059)\n",
      "    Batch [4800/6960] Loss: 0.9492 (Class: 0.6598, Duration: 4.0256)\n",
      "    Batch [4900/6960] Loss: 1.0329 (Class: 0.7210, Duration: 4.3398)\n",
      "    Batch [5000/6960] Loss: 0.8866 (Class: 0.6214, Duration: 3.6900)\n",
      "    Batch [5100/6960] Loss: 0.9267 (Class: 0.6530, Duration: 3.8077)\n",
      "    Batch [5200/6960] Loss: 0.9629 (Class: 0.6515, Duration: 4.3322)\n",
      "    Batch [5300/6960] Loss: 0.9323 (Class: 0.6233, Duration: 4.2997)\n",
      "    Batch [5400/6960] Loss: 0.8891 (Class: 0.5939, Duration: 4.1070)\n",
      "    Batch [5500/6960] Loss: 0.9879 (Class: 0.7160, Duration: 3.7840)\n",
      "    Batch [5600/6960] Loss: 0.8593 (Class: 0.6236, Duration: 3.2803)\n",
      "    Batch [5700/6960] Loss: 1.0061 (Class: 0.6945, Duration: 4.3355)\n",
      "    Batch [5800/6960] Loss: 0.8726 (Class: 0.6474, Duration: 3.1334)\n",
      "    Batch [5900/6960] Loss: 0.8508 (Class: 0.6330, Duration: 3.0313)\n",
      "    Batch [6000/6960] Loss: 1.0388 (Class: 0.7605, Duration: 3.8720)\n",
      "    Batch [6100/6960] Loss: 0.9341 (Class: 0.6803, Duration: 3.5313)\n",
      "    Batch [6200/6960] Loss: 0.9440 (Class: 0.6312, Duration: 4.3516)\n",
      "    Batch [6300/6960] Loss: 0.8820 (Class: 0.6300, Duration: 3.5056)\n",
      "    Batch [6400/6960] Loss: 0.9365 (Class: 0.6250, Duration: 4.3332)\n",
      "    Batch [6500/6960] Loss: 1.0572 (Class: 0.8091, Duration: 3.4512)\n",
      "    Batch [6600/6960] Loss: 1.1039 (Class: 0.7889, Duration: 4.3829)\n",
      "    Batch [6700/6960] Loss: 0.9054 (Class: 0.6576, Duration: 3.4473)\n",
      "    Batch [6800/6960] Loss: 1.0036 (Class: 0.6920, Duration: 4.3353)\n",
      "    Batch [6900/6960] Loss: 0.8738 (Class: 0.6332, Duration: 3.3465)\n",
      "\n",
      "Train Loss: 0.9766, Train Acc: 68.75%\n",
      "Val Loss: 1.0561, Val Acc: 69.70%\n",
      "Val Duration MSE: 4.0975 (RMSE: 2.0242)\n",
      "Class Accuracies - Bull: 78.91%, Flat: 52.34%, Bear: 80.46%\n",
      "Learning Rate: 6.40e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  -> New best model saved! (best duration MSE)\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.6 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.6 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.9 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 4.7 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 6.8 GB (+5.5 GB)\n",
      "\n",
      "Epoch 7/80\n",
      "Duration weight: 0.076\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 0.9787 (Class: 0.6963, Duration: 3.7041)\n",
      "    Batch [100/7058] Loss: 0.9620 (Class: 0.6347, Duration: 4.2933)\n",
      "    Batch [200/7058] Loss: 0.9757 (Class: 0.7654, Duration: 2.7579)\n",
      "    Batch [300/7058] Loss: 1.0480 (Class: 0.7276, Duration: 4.2019)\n",
      "    Batch [400/7058] Loss: 0.9733 (Class: 0.6783, Duration: 3.8693)\n",
      "    Batch [500/7058] Loss: 0.9412 (Class: 0.7072, Duration: 3.0678)\n",
      "    Batch [600/7058] Loss: 0.9519 (Class: 0.6927, Duration: 3.3993)\n",
      "    Batch [700/7058] Loss: 0.9903 (Class: 0.7053, Duration: 3.7378)\n",
      "    Batch [800/7058] Loss: 0.8454 (Class: 0.6033, Duration: 3.1744)\n",
      "    Batch [900/7058] Loss: 0.9750 (Class: 0.6887, Duration: 3.7556)\n",
      "    Batch [1000/7058] Loss: 1.0865 (Class: 0.6423, Duration: 5.8255)\n",
      "    Batch [1100/7058] Loss: 0.9701 (Class: 0.6871, Duration: 3.7109)\n",
      "    Batch [1200/7058] Loss: 0.9153 (Class: 0.6758, Duration: 3.1406)\n",
      "    Batch [1300/7058] Loss: 1.0884 (Class: 0.7430, Duration: 4.5293)\n",
      "    Batch [1400/7058] Loss: 1.0589 (Class: 0.7559, Duration: 3.9737)\n",
      "    Batch [1500/7058] Loss: 1.0285 (Class: 0.7573, Duration: 3.5568)\n",
      "    Batch [1600/7058] Loss: 0.9340 (Class: 0.6535, Duration: 3.6780)\n",
      "    Batch [1700/7058] Loss: 0.8936 (Class: 0.6512, Duration: 3.1791)\n",
      "    Batch [1800/7058] Loss: 0.8987 (Class: 0.6585, Duration: 3.1500)\n",
      "    Batch [1900/7058] Loss: 1.0726 (Class: 0.8008, Duration: 3.5637)\n",
      "    Batch [2000/7058] Loss: 0.8910 (Class: 0.5612, Duration: 4.3252)\n",
      "    Batch [2100/7058] Loss: 1.1163 (Class: 0.7943, Duration: 4.2225)\n",
      "    Batch [2200/7058] Loss: 0.8564 (Class: 0.5806, Duration: 3.6182)\n",
      "    Batch [2300/7058] Loss: 0.9081 (Class: 0.6164, Duration: 3.8258)\n",
      "    Batch [2400/7058] Loss: 1.0480 (Class: 0.7048, Duration: 4.5005)\n",
      "    Batch [2500/7058] Loss: 0.9580 (Class: 0.6924, Duration: 3.4821)\n",
      "    Batch [2600/7058] Loss: 1.0443 (Class: 0.7240, Duration: 4.2006)\n",
      "    Batch [2700/7058] Loss: 0.9491 (Class: 0.7261, Duration: 2.9234)\n",
      "    Batch [2800/7058] Loss: 1.0833 (Class: 0.7623, Duration: 4.2110)\n",
      "    Batch [2900/7058] Loss: 1.0102 (Class: 0.7046, Duration: 4.0071)\n",
      "    Batch [3000/7058] Loss: 1.1039 (Class: 0.7732, Duration: 4.3374)\n",
      "    Batch [3100/7058] Loss: 0.8672 (Class: 0.5965, Duration: 3.5511)\n",
      "    Batch [3200/7058] Loss: 1.0115 (Class: 0.6570, Duration: 4.6502)\n",
      "    Batch [3300/7058] Loss: 1.0286 (Class: 0.7285, Duration: 3.9360)\n",
      "    Batch [3400/7058] Loss: 1.1160 (Class: 0.8015, Duration: 4.1246)\n",
      "    Batch [3500/7058] Loss: 0.9962 (Class: 0.7407, Duration: 3.3512)\n",
      "    Batch [3600/7058] Loss: 0.8965 (Class: 0.6488, Duration: 3.2488)\n",
      "    Batch [3700/7058] Loss: 0.8909 (Class: 0.6461, Duration: 3.2102)\n",
      "    Batch [3800/7058] Loss: 1.0372 (Class: 0.7203, Duration: 4.1559)\n",
      "    Batch [3900/7058] Loss: 0.9072 (Class: 0.6242, Duration: 3.7122)\n",
      "    Batch [4000/7058] Loss: 0.8444 (Class: 0.6224, Duration: 2.9115)\n",
      "    Batch [4100/7058] Loss: 1.0478 (Class: 0.6378, Duration: 5.3777)\n",
      "    Batch [4200/7058] Loss: 0.9411 (Class: 0.6477, Duration: 3.8477)\n",
      "    Batch [4300/7058] Loss: 0.8872 (Class: 0.5984, Duration: 3.7878)\n",
      "    Batch [4400/7058] Loss: 0.8845 (Class: 0.6472, Duration: 3.1116)\n",
      "    Batch [4500/7058] Loss: 1.0006 (Class: 0.7269, Duration: 3.5890)\n",
      "    Batch [4600/7058] Loss: 0.9509 (Class: 0.6625, Duration: 3.7825)\n",
      "    Batch [4700/7058] Loss: 0.9194 (Class: 0.6766, Duration: 3.1837)\n",
      "    Batch [4800/7058] Loss: 0.9718 (Class: 0.6684, Duration: 3.9788)\n",
      "    Batch [4900/7058] Loss: 0.9912 (Class: 0.6831, Duration: 4.0416)\n",
      "    Batch [5000/7058] Loss: 0.9013 (Class: 0.6205, Duration: 3.6828)\n",
      "    Batch [5100/7058] Loss: 0.9061 (Class: 0.6797, Duration: 2.9687)\n",
      "    Batch [5200/7058] Loss: 0.8886 (Class: 0.6880, Duration: 2.6315)\n",
      "    Batch [5300/7058] Loss: 0.9899 (Class: 0.6706, Duration: 4.1866)\n",
      "    Batch [5400/7058] Loss: 0.9631 (Class: 0.6738, Duration: 3.7944)\n",
      "    Batch [5500/7058] Loss: 1.0254 (Class: 0.6944, Duration: 4.3407)\n",
      "    Batch [5600/7058] Loss: 0.8762 (Class: 0.6389, Duration: 3.1118)\n",
      "    Batch [5700/7058] Loss: 0.8100 (Class: 0.5846, Duration: 2.9560)\n",
      "    Batch [5800/7058] Loss: 1.0599 (Class: 0.7357, Duration: 4.2524)\n",
      "    Batch [5900/7058] Loss: 0.9127 (Class: 0.6611, Duration: 3.2999)\n",
      "    Batch [6000/7058] Loss: 0.8755 (Class: 0.5602, Duration: 4.1356)\n",
      "    Batch [6100/7058] Loss: 0.9729 (Class: 0.6391, Duration: 4.3779)\n",
      "    Batch [6200/7058] Loss: 0.9951 (Class: 0.7443, Duration: 3.2894)\n",
      "    Batch [6300/7058] Loss: 0.9323 (Class: 0.6373, Duration: 3.8688)\n",
      "    Batch [6400/7058] Loss: 1.0325 (Class: 0.7164, Duration: 4.1454)\n",
      "    Batch [6500/7058] Loss: 1.0105 (Class: 0.5969, Duration: 5.4234)\n",
      "    Batch [6600/7058] Loss: 0.9946 (Class: 0.6785, Duration: 4.1457)\n",
      "    Batch [6700/7058] Loss: 0.9907 (Class: 0.6638, Duration: 4.2871)\n",
      "    Batch [6800/7058] Loss: 0.7779 (Class: 0.5088, Duration: 3.5290)\n",
      "    Batch [6900/7058] Loss: 0.9923 (Class: 0.7364, Duration: 3.3564)\n",
      "    Batch [7000/7058] Loss: 0.8889 (Class: 0.6229, Duration: 3.4888)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.7 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 2.7 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 3.7 GB (+2.4 GB)\n",
      "  Loaded 200/255 files, Memory: 4.9 GB (+3.6 GB)\n",
      "  Loaded 250/255 files, Memory: 5.9 GB (+4.6 GB)\n",
      "Loaded 800392 samples. Memory usage: 7.7 GB (+6.5 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 1.0992 (Class: 0.7614, Duration: 4.4301)\n",
      "    Batch [100/6960] Loss: 0.9513 (Class: 0.6938, Duration: 3.3776)\n",
      "    Batch [200/6960] Loss: 0.9253 (Class: 0.7016, Duration: 2.9343)\n",
      "    Batch [300/6960] Loss: 0.9262 (Class: 0.6579, Duration: 3.5175)\n",
      "    Batch [400/6960] Loss: 0.7980 (Class: 0.5556, Duration: 3.1797)\n",
      "    Batch [500/6960] Loss: 0.9746 (Class: 0.6944, Duration: 3.6754)\n",
      "    Batch [600/6960] Loss: 0.8745 (Class: 0.6340, Duration: 3.1532)\n",
      "    Batch [700/6960] Loss: 1.0373 (Class: 0.7582, Duration: 3.6591)\n",
      "    Batch [800/6960] Loss: 1.1637 (Class: 0.7928, Duration: 4.8640)\n",
      "    Batch [900/6960] Loss: 0.8601 (Class: 0.6141, Duration: 3.2270)\n",
      "    Batch [1000/6960] Loss: 0.8415 (Class: 0.6515, Duration: 2.4910)\n",
      "    Batch [1100/6960] Loss: 0.9756 (Class: 0.6307, Duration: 4.5238)\n",
      "    Batch [1200/6960] Loss: 0.9286 (Class: 0.6877, Duration: 3.1584)\n",
      "    Batch [1300/6960] Loss: 0.8135 (Class: 0.5869, Duration: 2.9718)\n",
      "    Batch [1400/6960] Loss: 0.9915 (Class: 0.6566, Duration: 4.3920)\n",
      "    Batch [1500/6960] Loss: 0.8879 (Class: 0.6017, Duration: 3.7544)\n",
      "    Batch [1600/6960] Loss: 0.8367 (Class: 0.6078, Duration: 3.0018)\n",
      "    Batch [1700/6960] Loss: 0.8856 (Class: 0.6236, Duration: 3.4362)\n",
      "    Batch [1800/6960] Loss: 0.7994 (Class: 0.5955, Duration: 2.6737)\n",
      "    Batch [1900/6960] Loss: 1.0321 (Class: 0.7332, Duration: 3.9201)\n",
      "    Batch [2000/6960] Loss: 0.9810 (Class: 0.7126, Duration: 3.5189)\n",
      "    Batch [2100/6960] Loss: 0.8860 (Class: 0.6405, Duration: 3.2199)\n",
      "    Batch [2200/6960] Loss: 0.9121 (Class: 0.6462, Duration: 3.4872)\n",
      "    Batch [2300/6960] Loss: 0.9631 (Class: 0.7053, Duration: 3.3814)\n",
      "    Batch [2400/6960] Loss: 0.9511 (Class: 0.6659, Duration: 3.7406)\n",
      "    Batch [2500/6960] Loss: 1.0168 (Class: 0.7544, Duration: 3.4421)\n",
      "    Batch [2600/6960] Loss: 0.8978 (Class: 0.5590, Duration: 4.4438)\n",
      "    Batch [2700/6960] Loss: 0.9013 (Class: 0.6221, Duration: 3.6623)\n",
      "    Batch [2800/6960] Loss: 0.8422 (Class: 0.5398, Duration: 3.9663)\n",
      "    Batch [2900/6960] Loss: 0.8886 (Class: 0.6184, Duration: 3.5439)\n",
      "    Batch [3000/6960] Loss: 0.8309 (Class: 0.5806, Duration: 3.2818)\n",
      "    Batch [3100/6960] Loss: 0.9367 (Class: 0.6112, Duration: 4.2685)\n",
      "    Batch [3200/6960] Loss: 0.9530 (Class: 0.6483, Duration: 3.9957)\n",
      "    Batch [3300/6960] Loss: 0.9621 (Class: 0.6520, Duration: 4.0665)\n",
      "    Batch [3400/6960] Loss: 0.9453 (Class: 0.6640, Duration: 3.6897)\n",
      "    Batch [3500/6960] Loss: 1.0464 (Class: 0.6975, Duration: 4.5757)\n",
      "    Batch [3600/6960] Loss: 1.1090 (Class: 0.6825, Duration: 5.5931)\n",
      "    Batch [3700/6960] Loss: 0.9267 (Class: 0.6020, Duration: 4.2585)\n",
      "    Batch [3800/6960] Loss: 1.0378 (Class: 0.7202, Duration: 4.1662)\n",
      "    Batch [3900/6960] Loss: 0.9476 (Class: 0.6945, Duration: 3.3194)\n",
      "    Batch [4000/6960] Loss: 0.8037 (Class: 0.5916, Duration: 2.7817)\n",
      "    Batch [4100/6960] Loss: 0.8595 (Class: 0.5720, Duration: 3.7706)\n",
      "    Batch [4200/6960] Loss: 0.9453 (Class: 0.6578, Duration: 3.7713)\n",
      "    Batch [4300/6960] Loss: 0.9431 (Class: 0.6755, Duration: 3.5095)\n",
      "    Batch [4400/6960] Loss: 0.8125 (Class: 0.5861, Duration: 2.9695)\n",
      "    Batch [4500/6960] Loss: 0.9132 (Class: 0.6879, Duration: 2.9555)\n",
      "    Batch [4600/6960] Loss: 1.0527 (Class: 0.7284, Duration: 4.2536)\n",
      "    Batch [4700/6960] Loss: 0.8547 (Class: 0.5698, Duration: 3.7362)\n",
      "    Batch [4800/6960] Loss: 0.7356 (Class: 0.5287, Duration: 2.7142)\n",
      "    Batch [4900/6960] Loss: 0.9417 (Class: 0.6700, Duration: 3.5629)\n",
      "    Batch [5000/6960] Loss: 0.9156 (Class: 0.6201, Duration: 3.8749)\n",
      "    Batch [5100/6960] Loss: 0.8292 (Class: 0.5778, Duration: 3.2972)\n",
      "    Batch [5200/6960] Loss: 0.9473 (Class: 0.6529, Duration: 3.8601)\n",
      "    Batch [5300/6960] Loss: 0.9428 (Class: 0.5950, Duration: 4.5616)\n",
      "    Batch [5400/6960] Loss: 0.9667 (Class: 0.7052, Duration: 3.4303)\n",
      "    Batch [5500/6960] Loss: 0.7610 (Class: 0.5189, Duration: 3.1750)\n",
      "    Batch [5600/6960] Loss: 0.8839 (Class: 0.6058, Duration: 3.6475)\n",
      "    Batch [5700/6960] Loss: 0.8409 (Class: 0.6461, Duration: 2.5542)\n",
      "    Batch [5800/6960] Loss: 0.8336 (Class: 0.6530, Duration: 2.3681)\n",
      "    Batch [5900/6960] Loss: 1.0246 (Class: 0.7044, Duration: 4.1992)\n",
      "    Batch [6000/6960] Loss: 0.8548 (Class: 0.5724, Duration: 3.7037)\n",
      "    Batch [6100/6960] Loss: 0.9025 (Class: 0.6559, Duration: 3.2346)\n",
      "    Batch [6200/6960] Loss: 0.8357 (Class: 0.6054, Duration: 3.0211)\n",
      "    Batch [6300/6960] Loss: 0.9112 (Class: 0.6174, Duration: 3.8540)\n",
      "    Batch [6400/6960] Loss: 0.8484 (Class: 0.6027, Duration: 3.2219)\n",
      "    Batch [6500/6960] Loss: 0.8532 (Class: 0.6425, Duration: 2.7628)\n",
      "    Batch [6600/6960] Loss: 0.8522 (Class: 0.5429, Duration: 4.0569)\n",
      "    Batch [6700/6960] Loss: 0.7928 (Class: 0.5448, Duration: 3.2516)\n",
      "    Batch [6800/6960] Loss: 0.8963 (Class: 0.6316, Duration: 3.4704)\n",
      "    Batch [6900/6960] Loss: 0.8738 (Class: 0.6217, Duration: 3.3060)\n",
      "\n",
      "Train Loss: 0.9517, Train Acc: 70.33%\n",
      "Val Loss: 1.0530, Val Acc: 71.36%\n",
      "Val Duration MSE: 4.1552 (RMSE: 2.0384)\n",
      "Class Accuracies - Bull: 80.78%, Flat: 60.73%, Bear: 71.89%\n",
      "Learning Rate: 7.30e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  -> New best model saved! (best balanced accuracy)\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.6 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.6 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.9 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 4.7 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 6.7 GB (+5.4 GB)\n",
      "\n",
      "Epoch 8/80\n",
      "Duration weight: 0.081\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 0.9663 (Class: 0.6779, Duration: 3.5774)\n",
      "    Batch [100/7058] Loss: 0.8660 (Class: 0.5866, Duration: 3.4656)\n",
      "    Batch [200/7058] Loss: 1.0059 (Class: 0.7465, Duration: 3.2173)\n",
      "    Batch [300/7058] Loss: 1.1301 (Class: 0.7348, Duration: 4.9029)\n",
      "    Batch [400/7058] Loss: 1.0473 (Class: 0.7809, Duration: 3.3035)\n",
      "    Batch [500/7058] Loss: 0.8685 (Class: 0.5871, Duration: 3.4899)\n",
      "    Batch [600/7058] Loss: 1.0179 (Class: 0.7233, Duration: 3.6533)\n",
      "    Batch [700/7058] Loss: 0.9260 (Class: 0.6341, Duration: 3.6210)\n",
      "    Batch [800/7058] Loss: 0.7575 (Class: 0.5421, Duration: 2.6714)\n",
      "    Batch [900/7058] Loss: 0.9316 (Class: 0.5852, Duration: 4.2966)\n",
      "    Batch [1000/7058] Loss: 1.0251 (Class: 0.7098, Duration: 3.9109)\n",
      "    Batch [1100/7058] Loss: 0.8844 (Class: 0.6217, Duration: 3.2576)\n",
      "    Batch [1200/7058] Loss: 0.8694 (Class: 0.5954, Duration: 3.3993)\n",
      "    Batch [1300/7058] Loss: 0.9902 (Class: 0.6787, Duration: 3.8646)\n",
      "    Batch [1400/7058] Loss: 1.1377 (Class: 0.7544, Duration: 4.7541)\n",
      "    Batch [1500/7058] Loss: 1.0183 (Class: 0.6839, Duration: 4.1471)\n",
      "    Batch [1600/7058] Loss: 1.0143 (Class: 0.6787, Duration: 4.1625)\n",
      "    Batch [1700/7058] Loss: 0.8746 (Class: 0.6472, Duration: 2.8207)\n",
      "    Batch [1800/7058] Loss: 0.8545 (Class: 0.6102, Duration: 3.0298)\n",
      "    Batch [1900/7058] Loss: 0.8809 (Class: 0.5253, Duration: 4.4114)\n",
      "    Batch [2000/7058] Loss: 0.8857 (Class: 0.5785, Duration: 3.8110)\n",
      "    Batch [2100/7058] Loss: 1.0047 (Class: 0.6756, Duration: 4.0812)\n",
      "    Batch [2200/7058] Loss: 0.9575 (Class: 0.6058, Duration: 4.3626)\n",
      "    Batch [2300/7058] Loss: 1.1240 (Class: 0.6587, Duration: 5.7708)\n",
      "    Batch [2400/7058] Loss: 1.1118 (Class: 0.8111, Duration: 3.7297)\n",
      "    Batch [2500/7058] Loss: 0.8601 (Class: 0.5987, Duration: 3.2424)\n",
      "    Batch [2600/7058] Loss: 0.9003 (Class: 0.5356, Duration: 4.5233)\n",
      "    Batch [2700/7058] Loss: 0.9681 (Class: 0.7246, Duration: 3.0201)\n",
      "    Batch [2800/7058] Loss: 0.8422 (Class: 0.5467, Duration: 3.6644)\n",
      "    Batch [2900/7058] Loss: 0.9396 (Class: 0.7149, Duration: 2.7867)\n",
      "    Batch [3000/7058] Loss: 1.0658 (Class: 0.8455, Duration: 2.7323)\n",
      "    Batch [3100/7058] Loss: 0.8859 (Class: 0.6386, Duration: 3.0680)\n",
      "    Batch [3200/7058] Loss: 0.9944 (Class: 0.6756, Duration: 3.9535)\n",
      "    Batch [3300/7058] Loss: 0.9661 (Class: 0.6529, Duration: 3.8856)\n",
      "    Batch [3400/7058] Loss: 0.9251 (Class: 0.6490, Duration: 3.4247)\n",
      "    Batch [3500/7058] Loss: 0.9340 (Class: 0.6603, Duration: 3.3949)\n",
      "    Batch [3600/7058] Loss: 0.9461 (Class: 0.6684, Duration: 3.4444)\n",
      "    Batch [3700/7058] Loss: 0.9726 (Class: 0.7048, Duration: 3.3215)\n",
      "    Batch [3800/7058] Loss: 0.9099 (Class: 0.6097, Duration: 3.7229)\n",
      "    Batch [3900/7058] Loss: 0.8577 (Class: 0.6242, Duration: 2.8969)\n",
      "    Batch [4000/7058] Loss: 0.8975 (Class: 0.6329, Duration: 3.2818)\n",
      "    Batch [4100/7058] Loss: 1.1003 (Class: 0.7484, Duration: 4.3643)\n",
      "    Batch [4200/7058] Loss: 0.7587 (Class: 0.5403, Duration: 2.7084)\n",
      "    Batch [4300/7058] Loss: 1.0079 (Class: 0.6574, Duration: 4.3472)\n",
      "    Batch [4400/7058] Loss: 0.9972 (Class: 0.6592, Duration: 4.1923)\n",
      "    Batch [4500/7058] Loss: 0.8875 (Class: 0.6445, Duration: 3.0141)\n",
      "    Batch [4600/7058] Loss: 0.9213 (Class: 0.6531, Duration: 3.3266)\n",
      "    Batch [4700/7058] Loss: 0.9456 (Class: 0.6737, Duration: 3.3731)\n",
      "    Batch [4800/7058] Loss: 0.8797 (Class: 0.6183, Duration: 3.2420)\n",
      "    Batch [4900/7058] Loss: 1.0430 (Class: 0.7185, Duration: 4.0244)\n",
      "    Batch [5000/7058] Loss: 1.0263 (Class: 0.6578, Duration: 4.5701)\n",
      "    Batch [5100/7058] Loss: 0.8954 (Class: 0.6168, Duration: 3.4555)\n",
      "    Batch [5200/7058] Loss: 0.9352 (Class: 0.6430, Duration: 3.6243)\n",
      "    Batch [5300/7058] Loss: 0.9190 (Class: 0.6339, Duration: 3.5362)\n",
      "    Batch [5400/7058] Loss: 0.9700 (Class: 0.7001, Duration: 3.3480)\n",
      "    Batch [5500/7058] Loss: 0.8830 (Class: 0.5373, Duration: 4.2872)\n",
      "    Batch [5600/7058] Loss: 0.9910 (Class: 0.6972, Duration: 3.6440)\n",
      "    Batch [5700/7058] Loss: 0.8400 (Class: 0.6118, Duration: 2.8308)\n",
      "    Batch [5800/7058] Loss: 1.1031 (Class: 0.6976, Duration: 5.0297)\n",
      "    Batch [5900/7058] Loss: 0.8280 (Class: 0.5729, Duration: 3.1637)\n",
      "    Batch [6000/7058] Loss: 1.0762 (Class: 0.7674, Duration: 3.8303)\n",
      "    Batch [6100/7058] Loss: 0.9725 (Class: 0.7150, Duration: 3.1935)\n",
      "    Batch [6200/7058] Loss: 0.8280 (Class: 0.5685, Duration: 3.2182)\n",
      "    Batch [6300/7058] Loss: 0.8690 (Class: 0.6237, Duration: 3.0416)\n",
      "    Batch [6400/7058] Loss: 0.7359 (Class: 0.5122, Duration: 2.7741)\n",
      "    Batch [6500/7058] Loss: 0.8706 (Class: 0.6336, Duration: 2.9401)\n",
      "    Batch [6600/7058] Loss: 0.9033 (Class: 0.6153, Duration: 3.5724)\n",
      "    Batch [6700/7058] Loss: 1.0595 (Class: 0.7552, Duration: 3.7744)\n",
      "    Batch [6800/7058] Loss: 0.8732 (Class: 0.5603, Duration: 3.8807)\n",
      "    Batch [6900/7058] Loss: 1.0021 (Class: 0.7913, Duration: 2.6142)\n",
      "    Batch [7000/7058] Loss: 1.0559 (Class: 0.7704, Duration: 3.5412)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.7 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 2.7 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 3.7 GB (+2.4 GB)\n",
      "  Loaded 200/255 files, Memory: 4.9 GB (+3.6 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.6 GB)\n",
      "Loaded 800392 samples. Memory usage: 6.6 GB (+5.3 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 1.0990 (Class: 0.7906, Duration: 3.8257)\n",
      "    Batch [100/6960] Loss: 0.8801 (Class: 0.6353, Duration: 3.0367)\n",
      "    Batch [200/6960] Loss: 0.7773 (Class: 0.5644, Duration: 2.6415)\n",
      "    Batch [300/6960] Loss: 0.8472 (Class: 0.5510, Duration: 3.6746)\n",
      "    Batch [400/6960] Loss: 0.9431 (Class: 0.6191, Duration: 4.0190)\n",
      "    Batch [500/6960] Loss: 0.9143 (Class: 0.7094, Duration: 2.5415)\n",
      "    Batch [600/6960] Loss: 0.9418 (Class: 0.6752, Duration: 3.3066)\n",
      "    Batch [700/6960] Loss: 1.1437 (Class: 0.7684, Duration: 4.6547)\n",
      "    Batch [800/6960] Loss: 0.9211 (Class: 0.6714, Duration: 3.0973)\n",
      "    Batch [900/6960] Loss: 0.9243 (Class: 0.6663, Duration: 3.1996)\n",
      "    Batch [1000/6960] Loss: 0.9846 (Class: 0.6965, Duration: 3.5735)\n",
      "    Batch [1100/6960] Loss: 0.9194 (Class: 0.6666, Duration: 3.1361)\n",
      "    Batch [1200/6960] Loss: 0.9156 (Class: 0.6573, Duration: 3.2048)\n",
      "    Batch [1300/6960] Loss: 1.0246 (Class: 0.6801, Duration: 4.2724)\n",
      "    Batch [1400/6960] Loss: 0.9482 (Class: 0.6443, Duration: 3.7693)\n",
      "    Batch [1500/6960] Loss: 0.9225 (Class: 0.6181, Duration: 3.7765)\n",
      "    Batch [1600/6960] Loss: 1.0548 (Class: 0.7412, Duration: 3.8891)\n",
      "    Batch [1700/6960] Loss: 1.0739 (Class: 0.7382, Duration: 4.1639)\n",
      "    Batch [1800/6960] Loss: 0.8732 (Class: 0.5586, Duration: 3.9016)\n",
      "    Batch [1900/6960] Loss: 1.0442 (Class: 0.7022, Duration: 4.2422)\n",
      "    Batch [2000/6960] Loss: 0.9363 (Class: 0.6456, Duration: 3.6051)\n",
      "    Batch [2100/6960] Loss: 0.8242 (Class: 0.5656, Duration: 3.2076)\n",
      "    Batch [2200/6960] Loss: 0.8946 (Class: 0.6617, Duration: 2.8875)\n",
      "    Batch [2300/6960] Loss: 0.7041 (Class: 0.4782, Duration: 2.8011)\n",
      "    Batch [2400/6960] Loss: 0.9682 (Class: 0.7076, Duration: 3.2320)\n",
      "    Batch [2500/6960] Loss: 0.9075 (Class: 0.6032, Duration: 3.7737)\n",
      "    Batch [2600/6960] Loss: 0.8708 (Class: 0.6252, Duration: 3.0461)\n",
      "    Batch [2700/6960] Loss: 0.7858 (Class: 0.5402, Duration: 3.0458)\n",
      "    Batch [2800/6960] Loss: 0.8721 (Class: 0.5983, Duration: 3.3969)\n",
      "    Batch [2900/6960] Loss: 0.8017 (Class: 0.5218, Duration: 3.4724)\n",
      "    Batch [3000/6960] Loss: 0.9081 (Class: 0.6066, Duration: 3.7387)\n",
      "    Batch [3100/6960] Loss: 0.9503 (Class: 0.6907, Duration: 3.2206)\n",
      "    Batch [3200/6960] Loss: 0.8646 (Class: 0.6472, Duration: 2.6966)\n",
      "    Batch [3300/6960] Loss: 0.9409 (Class: 0.6199, Duration: 3.9815)\n",
      "    Batch [3400/6960] Loss: 0.8530 (Class: 0.5870, Duration: 3.2989)\n",
      "    Batch [3500/6960] Loss: 0.8816 (Class: 0.5863, Duration: 3.6628)\n",
      "    Batch [3600/6960] Loss: 0.8755 (Class: 0.5530, Duration: 3.9999)\n",
      "    Batch [3700/6960] Loss: 0.9478 (Class: 0.5638, Duration: 4.7627)\n",
      "    Batch [3800/6960] Loss: 0.7579 (Class: 0.4418, Duration: 3.9204)\n",
      "    Batch [3900/6960] Loss: 0.7228 (Class: 0.5183, Duration: 2.5369)\n",
      "    Batch [4000/6960] Loss: 0.9976 (Class: 0.6998, Duration: 3.6939)\n",
      "    Batch [4100/6960] Loss: 0.8177 (Class: 0.5723, Duration: 3.0442)\n",
      "    Batch [4200/6960] Loss: 0.8471 (Class: 0.5201, Duration: 4.0564)\n",
      "    Batch [4300/6960] Loss: 0.7761 (Class: 0.5700, Duration: 2.5566)\n",
      "    Batch [4400/6960] Loss: 0.8591 (Class: 0.5577, Duration: 3.7382)\n",
      "    Batch [4500/6960] Loss: 1.0076 (Class: 0.7241, Duration: 3.5172)\n",
      "    Batch [4600/6960] Loss: 0.8299 (Class: 0.5878, Duration: 3.0017)\n",
      "    Batch [4700/6960] Loss: 0.8430 (Class: 0.6020, Duration: 2.9893)\n",
      "    Batch [4800/6960] Loss: 0.9011 (Class: 0.6170, Duration: 3.5227)\n",
      "    Batch [4900/6960] Loss: 0.7321 (Class: 0.4852, Duration: 3.0619)\n",
      "    Batch [5000/6960] Loss: 0.9588 (Class: 0.6773, Duration: 3.4917)\n",
      "    Batch [5100/6960] Loss: 0.9307 (Class: 0.6404, Duration: 3.5998)\n",
      "    Batch [5200/6960] Loss: 1.1126 (Class: 0.8015, Duration: 3.8586)\n",
      "    Batch [5300/6960] Loss: 0.7397 (Class: 0.5626, Duration: 2.1962)\n",
      "    Batch [5400/6960] Loss: 0.9451 (Class: 0.6390, Duration: 3.7966)\n",
      "    Batch [5500/6960] Loss: 0.8085 (Class: 0.5800, Duration: 2.8350)\n",
      "    Batch [5600/6960] Loss: 0.9637 (Class: 0.7278, Duration: 2.9262)\n",
      "    Batch [5700/6960] Loss: 1.1602 (Class: 0.7670, Duration: 4.8775)\n",
      "    Batch [5800/6960] Loss: 0.8737 (Class: 0.5793, Duration: 3.6514)\n",
      "    Batch [5900/6960] Loss: 0.7725 (Class: 0.5419, Duration: 2.8599)\n",
      "    Batch [6000/6960] Loss: 0.9092 (Class: 0.6257, Duration: 3.5161)\n",
      "    Batch [6100/6960] Loss: 0.9257 (Class: 0.6274, Duration: 3.7003)\n",
      "    Batch [6200/6960] Loss: 0.7436 (Class: 0.4798, Duration: 3.2713)\n",
      "    Batch [6300/6960] Loss: 0.8266 (Class: 0.5659, Duration: 3.2342)\n",
      "    Batch [6400/6960] Loss: 1.0086 (Class: 0.7463, Duration: 3.2533)\n",
      "    Batch [6500/6960] Loss: 0.7653 (Class: 0.5523, Duration: 2.6429)\n",
      "    Batch [6600/6960] Loss: 0.7006 (Class: 0.4591, Duration: 2.9949)\n",
      "    Batch [6700/6960] Loss: 0.8158 (Class: 0.5642, Duration: 3.1200)\n",
      "    Batch [6800/6960] Loss: 0.7377 (Class: 0.4983, Duration: 2.9699)\n",
      "    Batch [6900/6960] Loss: 0.9466 (Class: 0.6683, Duration: 3.4522)\n",
      "\n",
      "Train Loss: 0.9256, Train Acc: 71.90%\n",
      "Val Loss: 1.0584, Val Acc: 69.95%\n",
      "Val Duration MSE: 4.1222 (RMSE: 2.0303)\n",
      "Class Accuracies - Bull: 73.50%, Flat: 60.74%, Bear: 77.83%\n",
      "Learning Rate: 8.20e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.6 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.6 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.9 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 4.7 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 6.8 GB (+5.5 GB)\n",
      "\n",
      "Epoch 9/80\n",
      "Duration weight: 0.085\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 0.9651 (Class: 0.6382, Duration: 3.8454)\n",
      "    Batch [100/7058] Loss: 1.0233 (Class: 0.6671, Duration: 4.1910)\n",
      "    Batch [200/7058] Loss: 0.9513 (Class: 0.6149, Duration: 3.9585)\n",
      "    Batch [300/7058] Loss: 0.9305 (Class: 0.6735, Duration: 3.0237)\n",
      "    Batch [400/7058] Loss: 1.0200 (Class: 0.6336, Duration: 4.5449)\n",
      "    Batch [500/7058] Loss: 0.8642 (Class: 0.6209, Duration: 2.8633)\n",
      "    Batch [600/7058] Loss: 1.0708 (Class: 0.6971, Duration: 4.3964)\n",
      "    Batch [700/7058] Loss: 1.0247 (Class: 0.6852, Duration: 3.9938)\n",
      "    Batch [800/7058] Loss: 1.0819 (Class: 0.7727, Duration: 3.6369)\n",
      "    Batch [900/7058] Loss: 1.0719 (Class: 0.7332, Duration: 3.9839)\n",
      "    Batch [1000/7058] Loss: 0.8499 (Class: 0.6006, Duration: 2.9331)\n",
      "    Batch [1100/7058] Loss: 0.8969 (Class: 0.5635, Duration: 3.9218)\n",
      "    Batch [1200/7058] Loss: 0.9244 (Class: 0.6580, Duration: 3.1342)\n",
      "    Batch [1300/7058] Loss: 1.0225 (Class: 0.7053, Duration: 3.7316)\n",
      "    Batch [1400/7058] Loss: 0.8495 (Class: 0.6027, Duration: 2.9044)\n",
      "    Batch [1500/7058] Loss: 0.8624 (Class: 0.6066, Duration: 3.0093)\n",
      "    Batch [1600/7058] Loss: 0.8788 (Class: 0.6465, Duration: 2.7329)\n",
      "    Batch [1700/7058] Loss: 1.0840 (Class: 0.7740, Duration: 3.6467)\n",
      "    Batch [1800/7058] Loss: 0.9176 (Class: 0.6403, Duration: 3.2621)\n",
      "    Batch [1900/7058] Loss: 0.8686 (Class: 0.5818, Duration: 3.3745)\n",
      "    Batch [2000/7058] Loss: 1.1466 (Class: 0.8179, Duration: 3.8669)\n",
      "    Batch [2100/7058] Loss: 1.0371 (Class: 0.7054, Duration: 3.9023)\n",
      "    Batch [2200/7058] Loss: 1.0426 (Class: 0.6902, Duration: 4.1461)\n",
      "    Batch [2300/7058] Loss: 0.8801 (Class: 0.6218, Duration: 3.0386)\n",
      "    Batch [2400/7058] Loss: 0.9212 (Class: 0.6150, Duration: 3.6026)\n",
      "    Batch [2500/7058] Loss: 0.9198 (Class: 0.5666, Duration: 4.1547)\n",
      "    Batch [2600/7058] Loss: 1.0071 (Class: 0.7268, Duration: 3.2983)\n",
      "    Batch [2700/7058] Loss: 0.7885 (Class: 0.5355, Duration: 2.9766)\n",
      "    Batch [2800/7058] Loss: 0.9165 (Class: 0.5820, Duration: 3.9354)\n",
      "    Batch [2900/7058] Loss: 1.1501 (Class: 0.7838, Duration: 4.3090)\n",
      "    Batch [3000/7058] Loss: 0.8922 (Class: 0.6255, Duration: 3.1382)\n",
      "    Batch [3100/7058] Loss: 0.9003 (Class: 0.5518, Duration: 4.1000)\n",
      "    Batch [3200/7058] Loss: 0.8886 (Class: 0.5735, Duration: 3.7075)\n",
      "    Batch [3300/7058] Loss: 0.8738 (Class: 0.5325, Duration: 4.0153)\n",
      "    Batch [3400/7058] Loss: 0.9682 (Class: 0.6311, Duration: 3.9653)\n",
      "    Batch [3500/7058] Loss: 0.8558 (Class: 0.5481, Duration: 3.6200)\n",
      "    Batch [3600/7058] Loss: 0.8047 (Class: 0.5781, Duration: 2.6661)\n",
      "    Batch [3700/7058] Loss: 1.0100 (Class: 0.6635, Duration: 4.0767)\n",
      "    Batch [3800/7058] Loss: 0.8733 (Class: 0.6243, Duration: 2.9296)\n",
      "    Batch [3900/7058] Loss: 0.8672 (Class: 0.6283, Duration: 2.8110)\n",
      "    Batch [4000/7058] Loss: 0.9936 (Class: 0.7501, Duration: 2.8643)\n",
      "    Batch [4100/7058] Loss: 0.8495 (Class: 0.5823, Duration: 3.1437)\n",
      "    Batch [4200/7058] Loss: 0.9255 (Class: 0.6166, Duration: 3.6351)\n",
      "    Batch [4300/7058] Loss: 0.9077 (Class: 0.6527, Duration: 3.0004)\n",
      "    Batch [4400/7058] Loss: 0.8043 (Class: 0.5174, Duration: 3.3756)\n",
      "    Batch [4500/7058] Loss: 0.8859 (Class: 0.5875, Duration: 3.5106)\n",
      "    Batch [4600/7058] Loss: 0.8817 (Class: 0.6020, Duration: 3.2898)\n",
      "    Batch [4700/7058] Loss: 0.8870 (Class: 0.6152, Duration: 3.1975)\n",
      "    Batch [4800/7058] Loss: 0.8782 (Class: 0.6241, Duration: 2.9894)\n",
      "    Batch [4900/7058] Loss: 1.0237 (Class: 0.7253, Duration: 3.5110)\n",
      "    Batch [5000/7058] Loss: 0.9067 (Class: 0.6495, Duration: 3.0250)\n",
      "    Batch [5100/7058] Loss: 0.8345 (Class: 0.5312, Duration: 3.5682)\n",
      "    Batch [5200/7058] Loss: 0.8620 (Class: 0.6195, Duration: 2.8527)\n",
      "    Batch [5300/7058] Loss: 0.8818 (Class: 0.5527, Duration: 3.8710)\n",
      "    Batch [5400/7058] Loss: 0.9364 (Class: 0.6090, Duration: 3.8515)\n",
      "    Batch [5500/7058] Loss: 0.9380 (Class: 0.6141, Duration: 3.8111)\n",
      "    Batch [5600/7058] Loss: 0.8715 (Class: 0.6288, Duration: 2.8559)\n",
      "    Batch [5700/7058] Loss: 0.9892 (Class: 0.6374, Duration: 4.1385)\n",
      "    Batch [5800/7058] Loss: 0.8453 (Class: 0.5486, Duration: 3.4913)\n",
      "    Batch [5900/7058] Loss: 0.9510 (Class: 0.6608, Duration: 3.4136)\n",
      "    Batch [6000/7058] Loss: 0.9157 (Class: 0.5881, Duration: 3.8539)\n",
      "    Batch [6100/7058] Loss: 0.8545 (Class: 0.5329, Duration: 3.7837)\n",
      "    Batch [6200/7058] Loss: 0.8671 (Class: 0.6182, Duration: 2.9288)\n",
      "    Batch [6300/7058] Loss: 0.9638 (Class: 0.6661, Duration: 3.5032)\n",
      "    Batch [6400/7058] Loss: 1.0346 (Class: 0.7041, Duration: 3.8884)\n",
      "    Batch [6500/7058] Loss: 0.8606 (Class: 0.5707, Duration: 3.4106)\n",
      "    Batch [6600/7058] Loss: 0.8506 (Class: 0.5485, Duration: 3.5536)\n",
      "    Batch [6700/7058] Loss: 0.8425 (Class: 0.5773, Duration: 3.1199)\n",
      "    Batch [6800/7058] Loss: 0.8460 (Class: 0.6008, Duration: 2.8841)\n",
      "    Batch [6900/7058] Loss: 0.8393 (Class: 0.5694, Duration: 3.1752)\n",
      "    Batch [7000/7058] Loss: 0.8298 (Class: 0.5893, Duration: 2.8297)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.7 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 2.7 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 3.7 GB (+2.4 GB)\n",
      "  Loaded 200/255 files, Memory: 4.8 GB (+3.6 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.6 GB)\n",
      "Loaded 800392 samples. Memory usage: 7.0 GB (+5.8 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 0.9080 (Class: 0.6325, Duration: 3.2400)\n",
      "    Batch [100/6960] Loss: 0.9557 (Class: 0.5684, Duration: 4.5565)\n",
      "    Batch [200/6960] Loss: 1.0638 (Class: 0.7410, Duration: 3.7969)\n",
      "    Batch [300/6960] Loss: 0.7008 (Class: 0.4988, Duration: 2.3763)\n",
      "    Batch [400/6960] Loss: 0.8579 (Class: 0.5831, Duration: 3.2324)\n",
      "    Batch [500/6960] Loss: 0.9298 (Class: 0.6167, Duration: 3.6831)\n",
      "    Batch [600/6960] Loss: 1.0306 (Class: 0.6697, Duration: 4.2454)\n",
      "    Batch [700/6960] Loss: 0.8249 (Class: 0.6091, Duration: 2.5386)\n",
      "    Batch [800/6960] Loss: 1.0359 (Class: 0.7318, Duration: 3.5771)\n",
      "    Batch [900/6960] Loss: 0.9631 (Class: 0.5901, Duration: 4.3892)\n",
      "    Batch [1000/6960] Loss: 0.8386 (Class: 0.5330, Duration: 3.5955)\n",
      "    Batch [1100/6960] Loss: 0.8798 (Class: 0.5976, Duration: 3.3210)\n",
      "    Batch [1200/6960] Loss: 0.9399 (Class: 0.6198, Duration: 3.7651)\n",
      "    Batch [1300/6960] Loss: 0.8926 (Class: 0.5768, Duration: 3.7151)\n",
      "    Batch [1400/6960] Loss: 0.7664 (Class: 0.5274, Duration: 2.8117)\n",
      "    Batch [1500/6960] Loss: 0.8351 (Class: 0.5251, Duration: 3.6474)\n",
      "    Batch [1600/6960] Loss: 0.9028 (Class: 0.6762, Duration: 2.6654)\n",
      "    Batch [1700/6960] Loss: 1.0676 (Class: 0.6701, Duration: 4.6758)\n",
      "    Batch [1800/6960] Loss: 0.7390 (Class: 0.4583, Duration: 3.3028)\n",
      "    Batch [1900/6960] Loss: 0.8925 (Class: 0.5864, Duration: 3.6012)\n",
      "    Batch [2000/6960] Loss: 0.9611 (Class: 0.6431, Duration: 3.7407)\n",
      "    Batch [2100/6960] Loss: 0.8993 (Class: 0.5902, Duration: 3.6369)\n",
      "    Batch [2200/6960] Loss: 0.9320 (Class: 0.5977, Duration: 3.9339)\n",
      "    Batch [2300/6960] Loss: 1.0369 (Class: 0.7297, Duration: 3.6137)\n",
      "    Batch [2400/6960] Loss: 0.9079 (Class: 0.6055, Duration: 3.5572)\n",
      "    Batch [2500/6960] Loss: 0.9458 (Class: 0.6561, Duration: 3.4085)\n",
      "    Batch [2600/6960] Loss: 0.8334 (Class: 0.5363, Duration: 3.4949)\n",
      "    Batch [2700/6960] Loss: 0.8331 (Class: 0.5645, Duration: 3.1601)\n",
      "    Batch [2800/6960] Loss: 0.9935 (Class: 0.6854, Duration: 3.6251)\n",
      "    Batch [2900/6960] Loss: 0.9902 (Class: 0.7132, Duration: 3.2589)\n",
      "    Batch [3000/6960] Loss: 0.7617 (Class: 0.5076, Duration: 2.9891)\n",
      "    Batch [3100/6960] Loss: 0.8618 (Class: 0.5663, Duration: 3.4770)\n",
      "    Batch [3200/6960] Loss: 0.8683 (Class: 0.5005, Duration: 4.3276)\n",
      "    Batch [3300/6960] Loss: 0.8672 (Class: 0.5795, Duration: 3.3838)\n",
      "    Batch [3400/6960] Loss: 0.7409 (Class: 0.4976, Duration: 2.8623)\n",
      "    Batch [3500/6960] Loss: 0.8782 (Class: 0.5696, Duration: 3.6309)\n",
      "    Batch [3600/6960] Loss: 0.8392 (Class: 0.5508, Duration: 3.3937)\n",
      "    Batch [3700/6960] Loss: 1.0425 (Class: 0.6768, Duration: 4.3022)\n",
      "    Batch [3800/6960] Loss: 0.8885 (Class: 0.5599, Duration: 3.8664)\n",
      "    Batch [3900/6960] Loss: 0.8338 (Class: 0.5249, Duration: 3.6348)\n",
      "    Batch [4000/6960] Loss: 0.9490 (Class: 0.6455, Duration: 3.5701)\n",
      "    Batch [4100/6960] Loss: 0.8219 (Class: 0.5319, Duration: 3.4115)\n",
      "    Batch [4200/6960] Loss: 0.8284 (Class: 0.5102, Duration: 3.7438)\n",
      "    Batch [4300/6960] Loss: 0.8179 (Class: 0.5427, Duration: 3.2376)\n",
      "    Batch [4400/6960] Loss: 0.7850 (Class: 0.5529, Duration: 2.7308)\n",
      "    Batch [4500/6960] Loss: 0.9969 (Class: 0.6727, Duration: 3.8140)\n",
      "    Batch [4600/6960] Loss: 0.8234 (Class: 0.5562, Duration: 3.1439)\n",
      "    Batch [4700/6960] Loss: 0.7953 (Class: 0.4773, Duration: 3.7408)\n",
      "    Batch [4800/6960] Loss: 0.7952 (Class: 0.5326, Duration: 3.0897)\n",
      "    Batch [4900/6960] Loss: 0.9780 (Class: 0.6602, Duration: 3.7378)\n",
      "    Batch [5000/6960] Loss: 0.8605 (Class: 0.6435, Duration: 2.5526)\n",
      "    Batch [5100/6960] Loss: 0.7929 (Class: 0.4633, Duration: 3.8783)\n",
      "    Batch [5200/6960] Loss: 0.9670 (Class: 0.6665, Duration: 3.5359)\n",
      "    Batch [5300/6960] Loss: 0.8683 (Class: 0.5965, Duration: 3.1974)\n",
      "    Batch [5400/6960] Loss: 0.8092 (Class: 0.5640, Duration: 2.8853)\n",
      "    Batch [5500/6960] Loss: 0.7489 (Class: 0.4709, Duration: 3.2696)\n",
      "    Batch [5600/6960] Loss: 0.8683 (Class: 0.5879, Duration: 3.2996)\n",
      "    Batch [5700/6960] Loss: 0.9428 (Class: 0.5965, Duration: 4.0738)\n",
      "    Batch [5800/6960] Loss: 0.6955 (Class: 0.4972, Duration: 2.3323)\n",
      "    Batch [5900/6960] Loss: 1.0225 (Class: 0.6732, Duration: 4.1094)\n",
      "    Batch [6000/6960] Loss: 0.7998 (Class: 0.5915, Duration: 2.4505)\n",
      "    Batch [6100/6960] Loss: 0.9871 (Class: 0.7161, Duration: 3.1881)\n",
      "    Batch [6200/6960] Loss: 0.8050 (Class: 0.5279, Duration: 3.2611)\n",
      "    Batch [6300/6960] Loss: 0.7813 (Class: 0.4985, Duration: 3.3268)\n",
      "    Batch [6400/6960] Loss: 0.8387 (Class: 0.5593, Duration: 3.2870)\n",
      "    Batch [6500/6960] Loss: 0.8997 (Class: 0.6043, Duration: 3.4761)\n",
      "    Batch [6600/6960] Loss: 0.9355 (Class: 0.6254, Duration: 3.6489)\n",
      "    Batch [6700/6960] Loss: 0.7684 (Class: 0.4911, Duration: 3.2619)\n",
      "    Batch [6800/6960] Loss: 0.8202 (Class: 0.5530, Duration: 3.1431)\n",
      "    Batch [6900/6960] Loss: 0.7095 (Class: 0.4682, Duration: 2.8389)\n",
      "\n",
      "Train Loss: 0.8977, Train Acc: 73.47%\n",
      "Val Loss: 1.1041, Val Acc: 71.18%\n",
      "Val Duration MSE: 4.1735 (RMSE: 2.0429)\n",
      "Class Accuracies - Bull: 80.23%, Flat: 58.31%, Bear: 75.56%\n",
      "Learning Rate: 9.10e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  -> New best model saved! (best balanced accuracy)\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.6 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.6 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.9 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 4.7 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 6.9 GB (+5.7 GB)\n",
      "\n",
      "Epoch 10/80\n",
      "Duration weight: 0.089\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 1.0136 (Class: 0.6744, Duration: 3.7951)\n",
      "    Batch [100/7058] Loss: 1.0381 (Class: 0.6530, Duration: 4.3095)\n",
      "    Batch [200/7058] Loss: 0.9376 (Class: 0.6549, Duration: 3.1629)\n",
      "    Batch [300/7058] Loss: 0.8609 (Class: 0.5423, Duration: 3.5650)\n",
      "    Batch [400/7058] Loss: 0.9956 (Class: 0.7041, Duration: 3.2619)\n",
      "    Batch [500/7058] Loss: 1.0731 (Class: 0.7190, Duration: 3.9621)\n",
      "    Batch [600/7058] Loss: 0.9189 (Class: 0.5883, Duration: 3.6989)\n",
      "    Batch [700/7058] Loss: 0.8606 (Class: 0.6459, Duration: 2.4021)\n",
      "    Batch [800/7058] Loss: 0.8222 (Class: 0.5452, Duration: 3.0992)\n",
      "    Batch [900/7058] Loss: 0.9098 (Class: 0.5718, Duration: 3.7819)\n",
      "    Batch [1000/7058] Loss: 0.8104 (Class: 0.5555, Duration: 2.8527)\n",
      "    Batch [1100/7058] Loss: 0.9199 (Class: 0.5945, Duration: 3.6410)\n",
      "    Batch [1200/7058] Loss: 0.9581 (Class: 0.6448, Duration: 3.5049)\n",
      "    Batch [1300/7058] Loss: 0.8562 (Class: 0.5997, Duration: 2.8692)\n",
      "    Batch [1400/7058] Loss: 0.9010 (Class: 0.5983, Duration: 3.3867)\n",
      "    Batch [1500/7058] Loss: 0.9331 (Class: 0.6538, Duration: 3.1244)\n",
      "    Batch [1600/7058] Loss: 1.0184 (Class: 0.7237, Duration: 3.2972)\n",
      "    Batch [1700/7058] Loss: 0.8588 (Class: 0.5196, Duration: 3.7951)\n",
      "    Batch [1800/7058] Loss: 0.9403 (Class: 0.6346, Duration: 3.4203)\n",
      "    Batch [1900/7058] Loss: 0.8716 (Class: 0.6408, Duration: 2.5822)\n",
      "    Batch [2000/7058] Loss: 0.9228 (Class: 0.6705, Duration: 2.8233)\n",
      "    Batch [2100/7058] Loss: 0.8761 (Class: 0.5580, Duration: 3.5596)\n",
      "    Batch [2200/7058] Loss: 0.8937 (Class: 0.6218, Duration: 3.0423)\n",
      "    Batch [2300/7058] Loss: 0.8767 (Class: 0.5416, Duration: 3.7494)\n",
      "    Batch [2400/7058] Loss: 0.8115 (Class: 0.4847, Duration: 3.6564)\n",
      "    Batch [2500/7058] Loss: 0.7667 (Class: 0.5379, Duration: 2.5599)\n",
      "    Batch [2600/7058] Loss: 0.8685 (Class: 0.5711, Duration: 3.3275)\n",
      "    Batch [2700/7058] Loss: 1.0010 (Class: 0.5988, Duration: 4.4995)\n",
      "    Batch [2800/7058] Loss: 0.9854 (Class: 0.6563, Duration: 3.6827)\n",
      "    Batch [2900/7058] Loss: 0.7972 (Class: 0.5551, Duration: 2.7080)\n",
      "    Batch [3000/7058] Loss: 0.9390 (Class: 0.5999, Duration: 3.7936)\n",
      "    Batch [3100/7058] Loss: 0.8176 (Class: 0.5501, Duration: 2.9928)\n",
      "    Batch [3200/7058] Loss: 0.9392 (Class: 0.5920, Duration: 3.8848)\n",
      "    Batch [3300/7058] Loss: 0.9109 (Class: 0.6421, Duration: 3.0081)\n",
      "    Batch [3400/7058] Loss: 0.9930 (Class: 0.6253, Duration: 4.1145)\n",
      "    Batch [3500/7058] Loss: 0.9353 (Class: 0.6380, Duration: 3.3254)\n",
      "    Batch [3600/7058] Loss: 1.0359 (Class: 0.7503, Duration: 3.1955)\n",
      "    Batch [3700/7058] Loss: 0.6878 (Class: 0.4765, Duration: 2.3653)\n",
      "    Batch [3800/7058] Loss: 0.8065 (Class: 0.4661, Duration: 3.8083)\n",
      "    Batch [3900/7058] Loss: 0.9132 (Class: 0.6179, Duration: 3.3034)\n",
      "    Batch [4000/7058] Loss: 0.7968 (Class: 0.4894, Duration: 3.4395)\n",
      "    Batch [4100/7058] Loss: 0.8348 (Class: 0.5326, Duration: 3.3810)\n",
      "    Batch [4200/7058] Loss: 0.8728 (Class: 0.5920, Duration: 3.1413)\n",
      "    Batch [4300/7058] Loss: 0.9382 (Class: 0.6550, Duration: 3.1695)\n",
      "    Batch [4400/7058] Loss: 0.8369 (Class: 0.5604, Duration: 3.0939)\n",
      "    Batch [4500/7058] Loss: 0.9903 (Class: 0.6535, Duration: 3.7683)\n",
      "    Batch [4600/7058] Loss: 0.7965 (Class: 0.5502, Duration: 2.7559)\n",
      "    Batch [4700/7058] Loss: 0.9981 (Class: 0.6779, Duration: 3.5828)\n",
      "    Batch [4800/7058] Loss: 0.8515 (Class: 0.5544, Duration: 3.3242)\n",
      "    Batch [4900/7058] Loss: 0.8218 (Class: 0.5727, Duration: 2.7865)\n",
      "    Batch [5000/7058] Loss: 0.8250 (Class: 0.5803, Duration: 2.7372)\n",
      "    Batch [5100/7058] Loss: 0.7460 (Class: 0.5009, Duration: 2.7422)\n",
      "    Batch [5200/7058] Loss: 0.8808 (Class: 0.5857, Duration: 3.3020)\n",
      "    Batch [5300/7058] Loss: 0.7579 (Class: 0.4989, Duration: 2.8989)\n",
      "    Batch [5400/7058] Loss: 0.9682 (Class: 0.5914, Duration: 4.2158)\n",
      "    Batch [5500/7058] Loss: 1.0190 (Class: 0.7132, Duration: 3.4209)\n",
      "    Batch [5600/7058] Loss: 0.8952 (Class: 0.5602, Duration: 3.7488)\n",
      "    Batch [5700/7058] Loss: 1.0308 (Class: 0.7063, Duration: 3.6315)\n",
      "    Batch [5800/7058] Loss: 0.9151 (Class: 0.5497, Duration: 4.0884)\n",
      "    Batch [5900/7058] Loss: 0.8559 (Class: 0.5312, Duration: 3.6335)\n",
      "    Batch [6000/7058] Loss: 0.7369 (Class: 0.4521, Duration: 3.1868)\n",
      "    Batch [6100/7058] Loss: 0.8772 (Class: 0.5750, Duration: 3.3822)\n",
      "    Batch [6200/7058] Loss: 0.8162 (Class: 0.5149, Duration: 3.3716)\n",
      "    Batch [6300/7058] Loss: 1.0057 (Class: 0.6617, Duration: 3.8483)\n",
      "    Batch [6400/7058] Loss: 0.9522 (Class: 0.6198, Duration: 3.7189)\n",
      "    Batch [6500/7058] Loss: 0.8738 (Class: 0.5586, Duration: 3.5274)\n",
      "    Batch [6600/7058] Loss: 0.8317 (Class: 0.5481, Duration: 3.1731)\n",
      "    Batch [6700/7058] Loss: 0.9015 (Class: 0.6163, Duration: 3.1916)\n",
      "    Batch [6800/7058] Loss: 0.9402 (Class: 0.5869, Duration: 3.9532)\n",
      "    Batch [6900/7058] Loss: 0.7992 (Class: 0.5497, Duration: 2.7920)\n",
      "    Batch [7000/7058] Loss: 0.8770 (Class: 0.5050, Duration: 4.1624)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.7 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 2.7 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 3.7 GB (+2.4 GB)\n",
      "  Loaded 200/255 files, Memory: 4.8 GB (+3.6 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.6 GB)\n",
      "Loaded 800392 samples. Memory usage: 6.6 GB (+5.3 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 1.2140 (Class: 0.8768, Duration: 3.7725)\n",
      "    Batch [100/6960] Loss: 0.8928 (Class: 0.5507, Duration: 3.8278)\n",
      "    Batch [200/6960] Loss: 0.9998 (Class: 0.6069, Duration: 4.3961)\n",
      "    Batch [300/6960] Loss: 0.9550 (Class: 0.5477, Duration: 4.5575)\n",
      "    Batch [400/6960] Loss: 0.9134 (Class: 0.6181, Duration: 3.3035)\n",
      "    Batch [500/6960] Loss: 0.9972 (Class: 0.6095, Duration: 4.3384)\n",
      "    Batch [600/6960] Loss: 0.8774 (Class: 0.5912, Duration: 3.2023)\n",
      "    Batch [700/6960] Loss: 0.9613 (Class: 0.6057, Duration: 3.9788)\n",
      "    Batch [800/6960] Loss: 0.8752 (Class: 0.5895, Duration: 3.1966)\n",
      "    Batch [900/6960] Loss: 1.1034 (Class: 0.7311, Duration: 4.1653)\n",
      "    Batch [1000/6960] Loss: 0.9412 (Class: 0.6453, Duration: 3.3107)\n",
      "    Batch [1100/6960] Loss: 0.9304 (Class: 0.6641, Duration: 2.9795)\n",
      "    Batch [1200/6960] Loss: 0.8748 (Class: 0.5503, Duration: 3.6314)\n",
      "    Batch [1300/6960] Loss: 1.0250 (Class: 0.7568, Duration: 3.0009)\n",
      "    Batch [1400/6960] Loss: 1.0065 (Class: 0.7005, Duration: 3.4241)\n",
      "    Batch [1500/6960] Loss: 0.8433 (Class: 0.5827, Duration: 2.9155)\n",
      "    Batch [1600/6960] Loss: 0.7211 (Class: 0.4346, Duration: 3.2055)\n",
      "    Batch [1700/6960] Loss: 0.8552 (Class: 0.5494, Duration: 3.4214)\n",
      "    Batch [1800/6960] Loss: 0.8428 (Class: 0.5935, Duration: 2.7904)\n",
      "    Batch [1900/6960] Loss: 0.9246 (Class: 0.6154, Duration: 3.4599)\n",
      "    Batch [2000/6960] Loss: 0.8062 (Class: 0.4896, Duration: 3.5421)\n",
      "    Batch [2100/6960] Loss: 0.8716 (Class: 0.5572, Duration: 3.5171)\n",
      "    Batch [2200/6960] Loss: 0.7312 (Class: 0.4638, Duration: 2.9912)\n",
      "    Batch [2300/6960] Loss: 0.8523 (Class: 0.4916, Duration: 4.0361)\n",
      "    Batch [2400/6960] Loss: 0.8374 (Class: 0.5808, Duration: 2.8713)\n",
      "    Batch [2500/6960] Loss: 0.8076 (Class: 0.5950, Duration: 2.3791)\n",
      "    Batch [2600/6960] Loss: 0.7878 (Class: 0.5139, Duration: 3.0643)\n",
      "    Batch [2700/6960] Loss: 0.7975 (Class: 0.5039, Duration: 3.2842)\n",
      "    Batch [2800/6960] Loss: 0.8163 (Class: 0.5281, Duration: 3.2244)\n",
      "    Batch [2900/6960] Loss: 0.8009 (Class: 0.5245, Duration: 3.0927)\n",
      "    Batch [3000/6960] Loss: 0.6992 (Class: 0.3827, Duration: 3.5417)\n",
      "    Batch [3100/6960] Loss: 0.7355 (Class: 0.5016, Duration: 2.6170)\n",
      "    Batch [3200/6960] Loss: 0.8713 (Class: 0.6313, Duration: 2.6853)\n",
      "    Batch [3300/6960] Loss: 0.8422 (Class: 0.6248, Duration: 2.4321)\n",
      "    Batch [3400/6960] Loss: 0.8388 (Class: 0.5635, Duration: 3.0803)\n",
      "    Batch [3500/6960] Loss: 0.7520 (Class: 0.5506, Duration: 2.2531)\n",
      "    Batch [3600/6960] Loss: 0.8166 (Class: 0.4841, Duration: 3.7199)\n",
      "    Batch [3700/6960] Loss: 0.8367 (Class: 0.6296, Duration: 2.3166)\n",
      "    Batch [3800/6960] Loss: 0.8106 (Class: 0.5335, Duration: 3.1010)\n",
      "    Batch [3900/6960] Loss: 0.8434 (Class: 0.6269, Duration: 2.4227)\n",
      "    Batch [4000/6960] Loss: 0.8438 (Class: 0.5424, Duration: 3.3725)\n",
      "    Batch [4100/6960] Loss: 0.9897 (Class: 0.5960, Duration: 4.4045)\n",
      "    Batch [4200/6960] Loss: 0.9283 (Class: 0.6410, Duration: 3.2146)\n",
      "    Batch [4300/6960] Loss: 0.8218 (Class: 0.5072, Duration: 3.5209)\n",
      "    Batch [4400/6960] Loss: 0.9571 (Class: 0.6252, Duration: 3.7139)\n",
      "    Batch [4500/6960] Loss: 0.8364 (Class: 0.5459, Duration: 3.2501)\n",
      "    Batch [4600/6960] Loss: 0.9641 (Class: 0.6101, Duration: 3.9605)\n",
      "    Batch [4700/6960] Loss: 0.8077 (Class: 0.5333, Duration: 3.0704)\n",
      "    Batch [4800/6960] Loss: 0.6996 (Class: 0.4515, Duration: 2.7753)\n",
      "    Batch [4900/6960] Loss: 0.8503 (Class: 0.5877, Duration: 2.9388)\n",
      "    Batch [5000/6960] Loss: 0.8355 (Class: 0.5662, Duration: 3.0132)\n",
      "    Batch [5100/6960] Loss: 0.8220 (Class: 0.4828, Duration: 3.7959)\n",
      "    Batch [5200/6960] Loss: 0.8567 (Class: 0.5635, Duration: 3.2814)\n",
      "    Batch [5300/6960] Loss: 0.8929 (Class: 0.5508, Duration: 3.8276)\n",
      "    Batch [5400/6960] Loss: 0.8435 (Class: 0.5306, Duration: 3.5009)\n",
      "    Batch [5500/6960] Loss: 0.8182 (Class: 0.5579, Duration: 2.9118)\n",
      "    Batch [5600/6960] Loss: 0.7495 (Class: 0.5055, Duration: 2.7300)\n",
      "    Batch [5700/6960] Loss: 0.9732 (Class: 0.5905, Duration: 4.2815)\n",
      "    Batch [5800/6960] Loss: 0.5200 (Class: 0.3567, Duration: 1.8272)\n",
      "    Batch [5900/6960] Loss: 0.9438 (Class: 0.6687, Duration: 3.0778)\n",
      "    Batch [6000/6960] Loss: 0.7473 (Class: 0.4642, Duration: 3.1675)\n",
      "    Batch [6100/6960] Loss: 0.8444 (Class: 0.5721, Duration: 3.0467)\n",
      "    Batch [6200/6960] Loss: 0.8267 (Class: 0.5911, Duration: 2.6357)\n",
      "    Batch [6300/6960] Loss: 0.8529 (Class: 0.5443, Duration: 3.4530)\n",
      "    Batch [6400/6960] Loss: 0.8301 (Class: 0.5561, Duration: 3.0655)\n",
      "    Batch [6500/6960] Loss: 0.9690 (Class: 0.6379, Duration: 3.7042)\n",
      "    Batch [6600/6960] Loss: 0.8634 (Class: 0.5271, Duration: 3.7631)\n",
      "    Batch [6700/6960] Loss: 0.8384 (Class: 0.5296, Duration: 3.4556)\n",
      "    Batch [6800/6960] Loss: 0.7358 (Class: 0.4484, Duration: 3.2157)\n",
      "    Batch [6900/6960] Loss: 0.8209 (Class: 0.5461, Duration: 3.0745)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ed/projects/trends/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 0.8727, Train Acc: 74.85%\n",
      "Val Loss: 1.1346, Val Acc: 71.34%\n",
      "Val Duration MSE: 4.1540 (RMSE: 2.0381)\n",
      "Class Accuracies - Bull: 75.82%, Flat: 65.27%, Bear: 73.07%\n",
      "Learning Rate: 1.00e-04\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  -> New best model saved! (best balanced accuracy)\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.6 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.6 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.9 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 4.7 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 6.7 GB (+5.4 GB)\n",
      "\n",
      "Epoch 11/80\n",
      "Duration weight: 0.094\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 1.0741 (Class: 0.6154, Duration: 4.8924)\n",
      "    Batch [100/7058] Loss: 0.9152 (Class: 0.6066, Duration: 3.2922)\n",
      "    Batch [200/7058] Loss: 0.9423 (Class: 0.6142, Duration: 3.4995)\n",
      "    Batch [300/7058] Loss: 1.1181 (Class: 0.6637, Duration: 4.8475)\n",
      "    Batch [400/7058] Loss: 0.9382 (Class: 0.6498, Duration: 3.0754)\n",
      "    Batch [500/7058] Loss: 0.8113 (Class: 0.4638, Duration: 3.7068)\n",
      "    Batch [600/7058] Loss: 0.8247 (Class: 0.5650, Duration: 2.7706)\n",
      "    Batch [700/7058] Loss: 0.8863 (Class: 0.5725, Duration: 3.3481)\n",
      "    Batch [800/7058] Loss: 0.8898 (Class: 0.6340, Duration: 2.7285)\n",
      "    Batch [900/7058] Loss: 0.9357 (Class: 0.6140, Duration: 3.4313)\n",
      "    Batch [1000/7058] Loss: 0.8938 (Class: 0.5569, Duration: 3.5935)\n",
      "    Batch [1100/7058] Loss: 0.8359 (Class: 0.5494, Duration: 3.0560)\n",
      "    Batch [1200/7058] Loss: 0.8854 (Class: 0.5540, Duration: 3.5351)\n",
      "    Batch [1300/7058] Loss: 0.9283 (Class: 0.6509, Duration: 2.9586)\n",
      "    Batch [1400/7058] Loss: 0.9941 (Class: 0.6649, Duration: 3.5113)\n",
      "    Batch [1500/7058] Loss: 0.9381 (Class: 0.5827, Duration: 3.7911)\n",
      "    Batch [1600/7058] Loss: 1.0080 (Class: 0.6911, Duration: 3.3798)\n",
      "    Batch [1700/7058] Loss: 0.8796 (Class: 0.6619, Duration: 2.3226)\n",
      "    Batch [1800/7058] Loss: 0.8990 (Class: 0.5284, Duration: 3.9534)\n",
      "    Batch [1900/7058] Loss: 0.9103 (Class: 0.5872, Duration: 3.4462)\n",
      "    Batch [2000/7058] Loss: 0.8854 (Class: 0.6236, Duration: 2.7928)\n",
      "    Batch [2100/7058] Loss: 1.1244 (Class: 0.7431, Duration: 4.0672)\n",
      "    Batch [2200/7058] Loss: 0.9191 (Class: 0.6602, Duration: 2.7611)\n",
      "    Batch [2300/7058] Loss: 0.9616 (Class: 0.6328, Duration: 3.5071)\n",
      "    Batch [2400/7058] Loss: 0.7858 (Class: 0.5325, Duration: 2.7012)\n",
      "    Batch [2500/7058] Loss: 0.8328 (Class: 0.5187, Duration: 3.3503)\n",
      "    Batch [2600/7058] Loss: 0.8654 (Class: 0.5970, Duration: 2.8622)\n",
      "    Batch [2700/7058] Loss: 0.8904 (Class: 0.5356, Duration: 3.7839)\n",
      "    Batch [2800/7058] Loss: 0.9234 (Class: 0.6015, Duration: 3.4336)\n",
      "    Batch [2900/7058] Loss: 0.7895 (Class: 0.5220, Duration: 2.8533)\n",
      "    Batch [3000/7058] Loss: 0.7117 (Class: 0.4670, Duration: 2.6104)\n",
      "    Batch [3100/7058] Loss: 0.8686 (Class: 0.5580, Duration: 3.3131)\n",
      "    Batch [3200/7058] Loss: 1.0225 (Class: 0.6210, Duration: 4.2821)\n",
      "    Batch [3300/7058] Loss: 0.9000 (Class: 0.6191, Duration: 2.9961)\n",
      "    Batch [3400/7058] Loss: 0.8241 (Class: 0.5395, Duration: 3.0361)\n",
      "    Batch [3500/7058] Loss: 0.8838 (Class: 0.5802, Duration: 3.2388)\n",
      "    Batch [3600/7058] Loss: 0.9282 (Class: 0.5906, Duration: 3.6012)\n",
      "    Batch [3700/7058] Loss: 0.7154 (Class: 0.5098, Duration: 2.1929)\n",
      "    Batch [3800/7058] Loss: 0.7957 (Class: 0.5760, Duration: 2.3431)\n",
      "    Batch [3900/7058] Loss: 0.8517 (Class: 0.6245, Duration: 2.4240)\n",
      "    Batch [4000/7058] Loss: 0.9074 (Class: 0.5803, Duration: 3.4894)\n",
      "    Batch [4100/7058] Loss: 0.8445 (Class: 0.5347, Duration: 3.3035)\n",
      "    Batch [4200/7058] Loss: 1.0294 (Class: 0.7191, Duration: 3.3098)\n",
      "    Batch [4300/7058] Loss: 0.9160 (Class: 0.5821, Duration: 3.5615)\n",
      "    Batch [4400/7058] Loss: 0.8533 (Class: 0.6246, Duration: 2.4403)\n",
      "    Batch [4500/7058] Loss: 0.7637 (Class: 0.4634, Duration: 3.2040)\n",
      "    Batch [4600/7058] Loss: 0.9927 (Class: 0.6939, Duration: 3.1880)\n",
      "    Batch [4700/7058] Loss: 0.8352 (Class: 0.5186, Duration: 3.3776)\n",
      "    Batch [4800/7058] Loss: 0.8734 (Class: 0.5832, Duration: 3.0963)\n",
      "    Batch [4900/7058] Loss: 0.8046 (Class: 0.5520, Duration: 2.6941)\n",
      "    Batch [5000/7058] Loss: 0.7458 (Class: 0.4770, Duration: 2.8678)\n",
      "    Batch [5100/7058] Loss: 0.8175 (Class: 0.5172, Duration: 3.2039)\n",
      "    Batch [5200/7058] Loss: 0.8499 (Class: 0.5853, Duration: 2.8219)\n",
      "    Batch [5300/7058] Loss: 0.7547 (Class: 0.5041, Duration: 2.6735)\n",
      "    Batch [5400/7058] Loss: 0.8762 (Class: 0.5661, Duration: 3.3080)\n",
      "    Batch [5500/7058] Loss: 0.8372 (Class: 0.5413, Duration: 3.1563)\n",
      "    Batch [5600/7058] Loss: 0.7863 (Class: 0.5698, Duration: 2.3088)\n",
      "    Batch [5700/7058] Loss: 0.8668 (Class: 0.5823, Duration: 3.0352)\n",
      "    Batch [5800/7058] Loss: 0.8341 (Class: 0.5658, Duration: 2.8624)\n",
      "    Batch [5900/7058] Loss: 0.8010 (Class: 0.5607, Duration: 2.5632)\n",
      "    Batch [6000/7058] Loss: 0.8017 (Class: 0.5707, Duration: 2.4643)\n",
      "    Batch [6100/7058] Loss: 1.0247 (Class: 0.6936, Duration: 3.5310)\n",
      "    Batch [6200/7058] Loss: 0.8586 (Class: 0.5813, Duration: 2.9575)\n",
      "    Batch [6300/7058] Loss: 0.6228 (Class: 0.3904, Duration: 2.4788)\n",
      "    Batch [6400/7058] Loss: 0.8021 (Class: 0.5641, Duration: 2.5392)\n",
      "    Batch [6500/7058] Loss: 0.8437 (Class: 0.4892, Duration: 3.7821)\n",
      "    Batch [6600/7058] Loss: 0.7857 (Class: 0.5196, Duration: 2.8376)\n",
      "    Batch [6700/7058] Loss: 0.7307 (Class: 0.5008, Duration: 2.4526)\n",
      "    Batch [6800/7058] Loss: 0.9817 (Class: 0.7049, Duration: 2.9519)\n",
      "    Batch [6900/7058] Loss: 0.8012 (Class: 0.4990, Duration: 3.2233)\n",
      "    Batch [7000/7058] Loss: 0.8992 (Class: 0.5703, Duration: 3.5081)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.7 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 2.7 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 3.7 GB (+2.4 GB)\n",
      "  Loaded 200/255 files, Memory: 4.9 GB (+3.6 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.6 GB)\n",
      "Loaded 800392 samples. Memory usage: 6.6 GB (+5.3 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 1.2142 (Class: 0.8061, Duration: 4.3529)\n",
      "    Batch [100/6960] Loss: 0.9092 (Class: 0.6247, Duration: 3.0352)\n",
      "    Batch [200/6960] Loss: 0.8710 (Class: 0.5930, Duration: 2.9647)\n",
      "    Batch [300/6960] Loss: 0.8745 (Class: 0.5098, Duration: 3.8900)\n",
      "    Batch [400/6960] Loss: 0.9644 (Class: 0.6232, Duration: 3.6395)\n",
      "    Batch [500/6960] Loss: 0.9723 (Class: 0.5872, Duration: 4.1076)\n",
      "    Batch [600/6960] Loss: 0.8455 (Class: 0.5342, Duration: 3.3209)\n",
      "    Batch [700/6960] Loss: 0.8357 (Class: 0.5281, Duration: 3.2807)\n",
      "    Batch [800/6960] Loss: 0.9258 (Class: 0.5874, Duration: 3.6092)\n",
      "    Batch [900/6960] Loss: 1.0686 (Class: 0.6581, Duration: 4.3788)\n",
      "    Batch [1000/6960] Loss: 0.8405 (Class: 0.4994, Duration: 3.6378)\n",
      "    Batch [1100/6960] Loss: 0.7284 (Class: 0.4621, Duration: 2.8404)\n",
      "    Batch [1200/6960] Loss: 0.9765 (Class: 0.5980, Duration: 4.0376)\n",
      "    Batch [1300/6960] Loss: 0.8573 (Class: 0.5972, Duration: 2.7748)\n",
      "    Batch [1400/6960] Loss: 0.8803 (Class: 0.5787, Duration: 3.2175)\n",
      "    Batch [1500/6960] Loss: 0.8569 (Class: 0.5424, Duration: 3.3552)\n",
      "    Batch [1600/6960] Loss: 0.7259 (Class: 0.4712, Duration: 2.7166)\n",
      "    Batch [1700/6960] Loss: 0.9284 (Class: 0.4915, Duration: 4.6601)\n",
      "    Batch [1800/6960] Loss: 0.8787 (Class: 0.5720, Duration: 3.2719)\n",
      "    Batch [1900/6960] Loss: 0.9170 (Class: 0.5579, Duration: 3.8304)\n",
      "    Batch [2000/6960] Loss: 0.8983 (Class: 0.5174, Duration: 4.0629)\n",
      "    Batch [2100/6960] Loss: 0.9167 (Class: 0.5698, Duration: 3.6999)\n",
      "    Batch [2200/6960] Loss: 0.8499 (Class: 0.5389, Duration: 3.3168)\n",
      "    Batch [2300/6960] Loss: 0.8794 (Class: 0.4880, Duration: 4.1743)\n",
      "    Batch [2400/6960] Loss: 0.8288 (Class: 0.5400, Duration: 3.0802)\n",
      "    Batch [2500/6960] Loss: 0.6923 (Class: 0.4562, Duration: 2.5177)\n",
      "    Batch [2600/6960] Loss: 0.7851 (Class: 0.4785, Duration: 3.2702)\n",
      "    Batch [2700/6960] Loss: 0.6112 (Class: 0.3904, Duration: 2.3557)\n",
      "    Batch [2800/6960] Loss: 0.8743 (Class: 0.5263, Duration: 3.7119)\n",
      "    Batch [2900/6960] Loss: 0.8064 (Class: 0.5197, Duration: 3.0584)\n",
      "    Batch [3000/6960] Loss: 0.8507 (Class: 0.5313, Duration: 3.4072)\n",
      "    Batch [3100/6960] Loss: 0.7947 (Class: 0.4471, Duration: 3.7080)\n",
      "    Batch [3200/6960] Loss: 0.8903 (Class: 0.6493, Duration: 2.5715)\n",
      "    Batch [3300/6960] Loss: 0.7000 (Class: 0.4684, Duration: 2.4712)\n",
      "    Batch [3400/6960] Loss: 0.6833 (Class: 0.4413, Duration: 2.5810)\n",
      "    Batch [3500/6960] Loss: 0.7213 (Class: 0.4470, Duration: 2.9263)\n",
      "    Batch [3600/6960] Loss: 0.7943 (Class: 0.5222, Duration: 2.9026)\n",
      "    Batch [3700/6960] Loss: 0.8196 (Class: 0.5393, Duration: 2.9904)\n",
      "    Batch [3800/6960] Loss: 0.7654 (Class: 0.5201, Duration: 2.6169)\n",
      "    Batch [3900/6960] Loss: 0.8345 (Class: 0.5326, Duration: 3.2194)\n",
      "    Batch [4000/6960] Loss: 0.8473 (Class: 0.5151, Duration: 3.5432)\n",
      "    Batch [4100/6960] Loss: 0.8544 (Class: 0.5726, Duration: 3.0051)\n",
      "    Batch [4200/6960] Loss: 0.7954 (Class: 0.5429, Duration: 2.6935)\n",
      "    Batch [4300/6960] Loss: 0.8329 (Class: 0.5353, Duration: 3.1750)\n",
      "    Batch [4400/6960] Loss: 0.8592 (Class: 0.5334, Duration: 3.4749)\n",
      "    Batch [4500/6960] Loss: 0.8157 (Class: 0.4718, Duration: 3.6681)\n",
      "    Batch [4600/6960] Loss: 0.7521 (Class: 0.5180, Duration: 2.4975)\n",
      "    Batch [4700/6960] Loss: 0.8437 (Class: 0.5965, Duration: 2.6364)\n",
      "    Batch [4800/6960] Loss: 0.8565 (Class: 0.5804, Duration: 2.9444)\n",
      "    Batch [4900/6960] Loss: 0.7444 (Class: 0.4667, Duration: 2.9627)\n",
      "    Batch [5000/6960] Loss: 0.8828 (Class: 0.5467, Duration: 3.5853)\n",
      "    Batch [5100/6960] Loss: 0.9066 (Class: 0.6442, Duration: 2.7986)\n",
      "    Batch [5200/6960] Loss: 0.7347 (Class: 0.5020, Duration: 2.4821)\n",
      "    Batch [5300/6960] Loss: 0.7151 (Class: 0.4716, Duration: 2.5968)\n",
      "    Batch [5400/6960] Loss: 0.7525 (Class: 0.4762, Duration: 2.9464)\n",
      "    Batch [5500/6960] Loss: 0.6664 (Class: 0.3896, Duration: 2.9532)\n",
      "    Batch [5600/6960] Loss: 0.7583 (Class: 0.4686, Duration: 3.0894)\n",
      "    Batch [5700/6960] Loss: 0.6983 (Class: 0.4351, Duration: 2.8073)\n",
      "    Batch [5800/6960] Loss: 0.7783 (Class: 0.4996, Duration: 2.9724)\n",
      "    Batch [5900/6960] Loss: 1.0394 (Class: 0.7524, Duration: 3.0619)\n",
      "    Batch [6000/6960] Loss: 0.7555 (Class: 0.4830, Duration: 2.9066)\n",
      "    Batch [6100/6960] Loss: 0.8056 (Class: 0.4905, Duration: 3.3612)\n",
      "    Batch [6200/6960] Loss: 0.7189 (Class: 0.4792, Duration: 2.5570)\n",
      "    Batch [6300/6960] Loss: 0.7121 (Class: 0.4272, Duration: 3.0393)\n",
      "    Batch [6400/6960] Loss: 0.6666 (Class: 0.4052, Duration: 2.7878)\n",
      "    Batch [6500/6960] Loss: 0.8316 (Class: 0.5404, Duration: 3.1063)\n",
      "    Batch [6600/6960] Loss: 0.7885 (Class: 0.5812, Duration: 2.2110)\n",
      "    Batch [6700/6960] Loss: 0.7917 (Class: 0.5096, Duration: 3.0092)\n",
      "    Batch [6800/6960] Loss: 0.7649 (Class: 0.4904, Duration: 2.9276)\n",
      "    Batch [6900/6960] Loss: 0.7675 (Class: 0.5434, Duration: 2.3902)\n",
      "\n",
      "Train Loss: 0.8485, Train Acc: 76.19%\n",
      "Val Loss: 1.1777, Val Acc: 70.48%\n",
      "Val Duration MSE: 4.2898 (RMSE: 2.0712)\n",
      "Class Accuracies - Bull: 73.66%, Flat: 63.37%, Bear: 75.86%\n",
      "Learning Rate: 9.99e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.6 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.6 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.9 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 4.7 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 6.8 GB (+5.5 GB)\n",
      "\n",
      "Epoch 12/80\n",
      "Duration weight: 0.098\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 0.8633 (Class: 0.5491, Duration: 3.2017)\n",
      "    Batch [100/7058] Loss: 0.9644 (Class: 0.6762, Duration: 2.9379)\n",
      "    Batch [200/7058] Loss: 0.9445 (Class: 0.6405, Duration: 3.0988)\n",
      "    Batch [300/7058] Loss: 0.9918 (Class: 0.6645, Duration: 3.3357)\n",
      "    Batch [400/7058] Loss: 0.9047 (Class: 0.5415, Duration: 3.7021)\n",
      "    Batch [500/7058] Loss: 0.8494 (Class: 0.5380, Duration: 3.1727)\n",
      "    Batch [600/7058] Loss: 0.8289 (Class: 0.5464, Duration: 2.8795)\n",
      "    Batch [700/7058] Loss: 1.0079 (Class: 0.5819, Duration: 4.3411)\n",
      "    Batch [800/7058] Loss: 0.9572 (Class: 0.6176, Duration: 3.4612)\n",
      "    Batch [900/7058] Loss: 0.9647 (Class: 0.6253, Duration: 3.4595)\n",
      "    Batch [1000/7058] Loss: 0.8592 (Class: 0.4798, Duration: 3.8668)\n",
      "    Batch [1100/7058] Loss: 1.0327 (Class: 0.7127, Duration: 3.2613)\n",
      "    Batch [1200/7058] Loss: 0.7201 (Class: 0.4736, Duration: 2.5124)\n",
      "    Batch [1300/7058] Loss: 0.7336 (Class: 0.4656, Duration: 2.7313)\n",
      "    Batch [1400/7058] Loss: 0.9613 (Class: 0.5816, Duration: 3.8691)\n",
      "    Batch [1500/7058] Loss: 0.8726 (Class: 0.6612, Duration: 2.1548)\n",
      "    Batch [1600/7058] Loss: 0.9754 (Class: 0.5175, Duration: 4.6662)\n",
      "    Batch [1700/7058] Loss: 0.9202 (Class: 0.5934, Duration: 3.3308)\n",
      "    Batch [1800/7058] Loss: 0.7441 (Class: 0.5009, Duration: 2.4779)\n",
      "    Batch [1900/7058] Loss: 0.7846 (Class: 0.5288, Duration: 2.6064)\n",
      "    Batch [2000/7058] Loss: 0.8043 (Class: 0.5137, Duration: 2.9615)\n",
      "    Batch [2100/7058] Loss: 0.9888 (Class: 0.6486, Duration: 3.4670)\n",
      "    Batch [2200/7058] Loss: 0.7612 (Class: 0.4816, Duration: 2.8488)\n",
      "    Batch [2300/7058] Loss: 0.7864 (Class: 0.5398, Duration: 2.5133)\n",
      "    Batch [2400/7058] Loss: 0.6689 (Class: 0.4222, Duration: 2.5144)\n",
      "    Batch [2500/7058] Loss: 0.8650 (Class: 0.5421, Duration: 3.2899)\n",
      "    Batch [2600/7058] Loss: 0.7511 (Class: 0.4600, Duration: 2.9670)\n",
      "    Batch [2700/7058] Loss: 0.7725 (Class: 0.5341, Duration: 2.4294)\n",
      "    Batch [2800/7058] Loss: 0.7735 (Class: 0.4581, Duration: 3.2141)\n",
      "    Batch [2900/7058] Loss: 0.9173 (Class: 0.5476, Duration: 3.7676)\n",
      "    Batch [3000/7058] Loss: 0.8455 (Class: 0.4936, Duration: 3.5868)\n",
      "    Batch [3100/7058] Loss: 0.8413 (Class: 0.4998, Duration: 3.4801)\n",
      "    Batch [3200/7058] Loss: 0.8762 (Class: 0.5138, Duration: 3.6937)\n",
      "    Batch [3300/7058] Loss: 0.8618 (Class: 0.5427, Duration: 3.2520)\n",
      "    Batch [3400/7058] Loss: 0.7962 (Class: 0.5102, Duration: 2.9144)\n",
      "    Batch [3500/7058] Loss: 0.8831 (Class: 0.5601, Duration: 3.2919)\n",
      "    Batch [3600/7058] Loss: 0.9794 (Class: 0.6215, Duration: 3.6469)\n",
      "    Batch [3700/7058] Loss: 0.9326 (Class: 0.6664, Duration: 2.7129)\n",
      "    Batch [3800/7058] Loss: 0.8107 (Class: 0.4553, Duration: 3.6228)\n",
      "    Batch [3900/7058] Loss: 0.7734 (Class: 0.4740, Duration: 3.0511)\n",
      "    Batch [4000/7058] Loss: 0.9656 (Class: 0.6504, Duration: 3.2120)\n",
      "    Batch [4100/7058] Loss: 0.6265 (Class: 0.4138, Duration: 2.1673)\n",
      "    Batch [4200/7058] Loss: 0.8402 (Class: 0.5089, Duration: 3.3766)\n",
      "    Batch [4300/7058] Loss: 0.7512 (Class: 0.4672, Duration: 2.8943)\n",
      "    Batch [4400/7058] Loss: 0.7256 (Class: 0.4906, Duration: 2.3949)\n",
      "    Batch [4500/7058] Loss: 0.7167 (Class: 0.4726, Duration: 2.4877)\n",
      "    Batch [4600/7058] Loss: 0.7867 (Class: 0.5774, Duration: 2.1329)\n",
      "    Batch [4700/7058] Loss: 0.7324 (Class: 0.4708, Duration: 2.6667)\n",
      "    Batch [4800/7058] Loss: 0.6384 (Class: 0.4281, Duration: 2.1430)\n",
      "    Batch [4900/7058] Loss: 0.8756 (Class: 0.6342, Duration: 2.4602)\n",
      "    Batch [5000/7058] Loss: 0.9549 (Class: 0.5791, Duration: 3.8305)\n",
      "    Batch [5100/7058] Loss: 0.8327 (Class: 0.5063, Duration: 3.3262)\n",
      "    Batch [5200/7058] Loss: 0.6814 (Class: 0.4411, Duration: 2.4482)\n",
      "    Batch [5300/7058] Loss: 0.9935 (Class: 0.5774, Duration: 4.2410)\n",
      "    Batch [5400/7058] Loss: 0.7374 (Class: 0.4265, Duration: 3.1680)\n",
      "    Batch [5500/7058] Loss: 0.7690 (Class: 0.5306, Duration: 2.4294)\n",
      "    Batch [5600/7058] Loss: 0.6857 (Class: 0.4114, Duration: 2.7962)\n",
      "    Batch [5700/7058] Loss: 0.7104 (Class: 0.4684, Duration: 2.4656)\n",
      "    Batch [5800/7058] Loss: 0.9094 (Class: 0.6723, Duration: 2.4156)\n",
      "    Batch [5900/7058] Loss: 0.6767 (Class: 0.4457, Duration: 2.3543)\n",
      "    Batch [6000/7058] Loss: 0.9947 (Class: 0.6343, Duration: 3.6735)\n",
      "    Batch [6100/7058] Loss: 0.6453 (Class: 0.3966, Duration: 2.5338)\n",
      "    Batch [6200/7058] Loss: 0.6527 (Class: 0.3678, Duration: 2.9039)\n",
      "    Batch [6300/7058] Loss: 0.8949 (Class: 0.6100, Duration: 2.9032)\n",
      "    Batch [6400/7058] Loss: 0.7469 (Class: 0.4447, Duration: 3.0799)\n",
      "    Batch [6500/7058] Loss: 0.7126 (Class: 0.4515, Duration: 2.6606)\n",
      "    Batch [6600/7058] Loss: 0.9099 (Class: 0.6193, Duration: 2.9621)\n",
      "    Batch [6700/7058] Loss: 0.7041 (Class: 0.4275, Duration: 2.8187)\n",
      "    Batch [6800/7058] Loss: 0.7185 (Class: 0.4279, Duration: 2.9615)\n",
      "    Batch [6900/7058] Loss: 0.7439 (Class: 0.4589, Duration: 2.9048)\n",
      "    Batch [7000/7058] Loss: 0.8261 (Class: 0.5639, Duration: 2.6717)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.7 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 2.7 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 3.7 GB (+2.4 GB)\n",
      "  Loaded 200/255 files, Memory: 4.9 GB (+3.6 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.6 GB)\n",
      "Loaded 800392 samples. Memory usage: 8.8 GB (+7.5 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 0.9773 (Class: 0.6744, Duration: 3.0861)\n",
      "    Batch [100/6960] Loss: 0.8551 (Class: 0.6149, Duration: 2.4481)\n",
      "    Batch [200/6960] Loss: 0.8283 (Class: 0.5675, Duration: 2.6576)\n",
      "    Batch [300/6960] Loss: 0.9014 (Class: 0.6000, Duration: 3.0714)\n",
      "    Batch [400/6960] Loss: 0.8894 (Class: 0.5682, Duration: 3.2733)\n",
      "    Batch [500/6960] Loss: 0.8288 (Class: 0.5331, Duration: 3.0133)\n",
      "    Batch [600/6960] Loss: 0.7024 (Class: 0.5061, Duration: 2.0001)\n",
      "    Batch [700/6960] Loss: 0.7836 (Class: 0.4902, Duration: 2.9902)\n",
      "    Batch [800/6960] Loss: 0.7029 (Class: 0.4564, Duration: 2.5122)\n",
      "    Batch [900/6960] Loss: 0.8079 (Class: 0.5467, Duration: 2.6619)\n",
      "    Batch [1000/6960] Loss: 0.6970 (Class: 0.4228, Duration: 2.7947)\n",
      "    Batch [1100/6960] Loss: 0.9588 (Class: 0.6356, Duration: 3.2943)\n",
      "    Batch [1200/6960] Loss: 0.7917 (Class: 0.4813, Duration: 3.1632)\n",
      "    Batch [1300/6960] Loss: 0.8098 (Class: 0.5434, Duration: 2.7146)\n",
      "    Batch [1400/6960] Loss: 0.8570 (Class: 0.5768, Duration: 2.8554)\n",
      "    Batch [1500/6960] Loss: 0.8168 (Class: 0.4561, Duration: 3.6764)\n",
      "    Batch [1600/6960] Loss: 0.5235 (Class: 0.3599, Duration: 1.6670)\n",
      "    Batch [1700/6960] Loss: 0.8785 (Class: 0.5822, Duration: 3.0197)\n",
      "    Batch [1800/6960] Loss: 0.5881 (Class: 0.3209, Duration: 2.7225)\n",
      "    Batch [1900/6960] Loss: 0.7415 (Class: 0.4498, Duration: 2.9729)\n",
      "    Batch [2000/6960] Loss: 0.7067 (Class: 0.3962, Duration: 3.1648)\n",
      "    Batch [2100/6960] Loss: 0.7942 (Class: 0.4688, Duration: 3.3167)\n",
      "    Batch [2200/6960] Loss: 0.8713 (Class: 0.5079, Duration: 3.7033)\n",
      "    Batch [2300/6960] Loss: 0.7293 (Class: 0.4618, Duration: 2.7262)\n",
      "    Batch [2400/6960] Loss: 0.7799 (Class: 0.5342, Duration: 2.5039)\n",
      "    Batch [2500/6960] Loss: 0.6189 (Class: 0.4177, Duration: 2.0508)\n",
      "    Batch [2600/6960] Loss: 0.8249 (Class: 0.5526, Duration: 2.7752)\n",
      "    Batch [2700/6960] Loss: 0.7487 (Class: 0.4640, Duration: 2.9011)\n",
      "    Batch [2800/6960] Loss: 0.8016 (Class: 0.5295, Duration: 2.7731)\n",
      "    Batch [2900/6960] Loss: 0.8293 (Class: 0.5548, Duration: 2.7977)\n",
      "    Batch [3000/6960] Loss: 0.8083 (Class: 0.5350, Duration: 2.7851)\n",
      "    Batch [3100/6960] Loss: 0.7490 (Class: 0.5107, Duration: 2.4285)\n",
      "    Batch [3200/6960] Loss: 0.7060 (Class: 0.3913, Duration: 3.2070)\n",
      "    Batch [3300/6960] Loss: 0.7363 (Class: 0.4643, Duration: 2.7712)\n",
      "    Batch [3400/6960] Loss: 0.7413 (Class: 0.5198, Duration: 2.2568)\n",
      "    Batch [3500/6960] Loss: 0.8813 (Class: 0.6085, Duration: 2.7797)\n",
      "    Batch [3600/6960] Loss: 0.8372 (Class: 0.5000, Duration: 3.4362)\n",
      "    Batch [3700/6960] Loss: 0.8930 (Class: 0.5448, Duration: 3.5491)\n",
      "    Batch [3800/6960] Loss: 0.7797 (Class: 0.4683, Duration: 3.1740)\n",
      "    Batch [3900/6960] Loss: 0.7642 (Class: 0.4966, Duration: 2.7273)\n",
      "    Batch [4000/6960] Loss: 0.8402 (Class: 0.6205, Duration: 2.2382)\n",
      "    Batch [4100/6960] Loss: 0.6231 (Class: 0.3818, Duration: 2.4591)\n",
      "    Batch [4200/6960] Loss: 0.6991 (Class: 0.4560, Duration: 2.4767)\n",
      "    Batch [4300/6960] Loss: 0.7628 (Class: 0.4271, Duration: 3.4210)\n",
      "    Batch [4400/6960] Loss: 0.7888 (Class: 0.4871, Duration: 3.0742)\n",
      "    Batch [4500/6960] Loss: 0.8486 (Class: 0.5230, Duration: 3.3185)\n",
      "    Batch [4600/6960] Loss: 0.6558 (Class: 0.3852, Duration: 2.7579)\n",
      "    Batch [4700/6960] Loss: 0.7380 (Class: 0.4356, Duration: 3.0817)\n",
      "    Batch [4800/6960] Loss: 0.8206 (Class: 0.5023, Duration: 3.2431)\n",
      "    Batch [4900/6960] Loss: 1.0455 (Class: 0.6199, Duration: 4.3365)\n",
      "    Batch [5000/6960] Loss: 0.8173 (Class: 0.5441, Duration: 2.7844)\n",
      "    Batch [5100/6960] Loss: 0.6757 (Class: 0.4224, Duration: 2.5815)\n",
      "    Batch [5200/6960] Loss: 0.7160 (Class: 0.4240, Duration: 2.9761)\n",
      "    Batch [5300/6960] Loss: 0.7815 (Class: 0.4460, Duration: 3.4185)\n",
      "    Batch [5400/6960] Loss: 0.7648 (Class: 0.4539, Duration: 3.1685)\n",
      "    Batch [5500/6960] Loss: 0.8294 (Class: 0.4906, Duration: 3.4531)\n",
      "    Batch [5600/6960] Loss: 0.8544 (Class: 0.5489, Duration: 3.1128)\n",
      "    Batch [5700/6960] Loss: 0.7709 (Class: 0.4831, Duration: 2.9332)\n",
      "    Batch [5800/6960] Loss: 0.8420 (Class: 0.5369, Duration: 3.1088)\n",
      "    Batch [5900/6960] Loss: 0.6760 (Class: 0.4329, Duration: 2.4772)\n",
      "    Batch [6000/6960] Loss: 0.7686 (Class: 0.5470, Duration: 2.2586)\n",
      "    Batch [6100/6960] Loss: 0.7434 (Class: 0.4021, Duration: 3.4774)\n",
      "    Batch [6200/6960] Loss: 0.7579 (Class: 0.5211, Duration: 2.4136)\n",
      "    Batch [6300/6960] Loss: 0.7087 (Class: 0.4471, Duration: 2.6658)\n",
      "    Batch [6400/6960] Loss: 0.6934 (Class: 0.4730, Duration: 2.2461)\n",
      "    Batch [6500/6960] Loss: 0.6179 (Class: 0.3590, Duration: 2.6382)\n",
      "    Batch [6600/6960] Loss: 0.6892 (Class: 0.4118, Duration: 2.8274)\n",
      "    Batch [6700/6960] Loss: 0.6941 (Class: 0.4163, Duration: 2.8302)\n",
      "    Batch [6800/6960] Loss: 0.6190 (Class: 0.3694, Duration: 2.5440)\n",
      "    Batch [6900/6960] Loss: 0.7186 (Class: 0.4151, Duration: 3.0933)\n",
      "\n",
      "Train Loss: 0.8088, Train Acc: 78.10%\n",
      "Val Loss: 1.2681, Val Acc: 70.20%\n",
      "Val Duration MSE: 4.2991 (RMSE: 2.0734)\n",
      "Class Accuracies - Bull: 71.46%, Flat: 62.38%, Bear: 79.72%\n",
      "Learning Rate: 9.98e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.6 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.6 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.9 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 4.7 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 7.0 GB (+5.7 GB)\n",
      "\n",
      "Epoch 13/80\n",
      "Duration weight: 0.103\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 0.9182 (Class: 0.6216, Duration: 2.8934)\n",
      "    Batch [100/7058] Loss: 0.8413 (Class: 0.5945, Duration: 2.4077)\n",
      "    Batch [200/7058] Loss: 0.8421 (Class: 0.4927, Duration: 3.4079)\n",
      "    Batch [300/7058] Loss: 0.9175 (Class: 0.5926, Duration: 3.1698)\n",
      "    Batch [400/7058] Loss: 0.8458 (Class: 0.5601, Duration: 2.7882)\n",
      "    Batch [500/7058] Loss: 0.7276 (Class: 0.4973, Duration: 2.2473)\n",
      "    Batch [600/7058] Loss: 0.8845 (Class: 0.5425, Duration: 3.3358)\n",
      "    Batch [700/7058] Loss: 0.7512 (Class: 0.4247, Duration: 3.1854)\n",
      "    Batch [800/7058] Loss: 0.9506 (Class: 0.5571, Duration: 3.8389)\n",
      "    Batch [900/7058] Loss: 0.7216 (Class: 0.4462, Duration: 2.6865)\n",
      "    Batch [1000/7058] Loss: 0.8227 (Class: 0.5033, Duration: 3.1159)\n",
      "    Batch [1100/7058] Loss: 0.8736 (Class: 0.5597, Duration: 3.0631)\n",
      "    Batch [1200/7058] Loss: 0.8386 (Class: 0.5153, Duration: 3.1536)\n",
      "    Batch [1300/7058] Loss: 0.8692 (Class: 0.5407, Duration: 3.2053)\n",
      "    Batch [1400/7058] Loss: 0.9084 (Class: 0.5609, Duration: 3.3910)\n",
      "    Batch [1500/7058] Loss: 0.8716 (Class: 0.5443, Duration: 3.1934)\n",
      "    Batch [1600/7058] Loss: 0.8112 (Class: 0.4643, Duration: 3.3849)\n",
      "    Batch [1700/7058] Loss: 0.7322 (Class: 0.4395, Duration: 2.8557)\n",
      "    Batch [1800/7058] Loss: 0.8362 (Class: 0.5297, Duration: 2.9898)\n",
      "    Batch [1900/7058] Loss: 0.9081 (Class: 0.6011, Duration: 2.9953)\n",
      "    Batch [2000/7058] Loss: 0.7085 (Class: 0.4677, Duration: 2.3493)\n",
      "    Batch [2100/7058] Loss: 0.6391 (Class: 0.3725, Duration: 2.6007)\n",
      "    Batch [2200/7058] Loss: 0.6384 (Class: 0.4255, Duration: 2.0768)\n",
      "    Batch [2300/7058] Loss: 0.9138 (Class: 0.5053, Duration: 3.9852)\n",
      "    Batch [2400/7058] Loss: 0.7796 (Class: 0.5462, Duration: 2.2774)\n",
      "    Batch [2500/7058] Loss: 0.7741 (Class: 0.4297, Duration: 3.3599)\n",
      "    Batch [2600/7058] Loss: 0.9558 (Class: 0.5489, Duration: 3.9700)\n",
      "    Batch [2700/7058] Loss: 0.9909 (Class: 0.5792, Duration: 4.0165)\n",
      "    Batch [2800/7058] Loss: 0.6909 (Class: 0.4378, Duration: 2.4695)\n",
      "    Batch [2900/7058] Loss: 0.8366 (Class: 0.5642, Duration: 2.6579)\n",
      "    Batch [3000/7058] Loss: 0.7213 (Class: 0.4478, Duration: 2.6680)\n",
      "    Batch [3100/7058] Loss: 0.9803 (Class: 0.6397, Duration: 3.3228)\n",
      "    Batch [3200/7058] Loss: 0.6854 (Class: 0.3993, Duration: 2.7909)\n",
      "    Batch [3300/7058] Loss: 0.9716 (Class: 0.6153, Duration: 3.4756)\n",
      "    Batch [3400/7058] Loss: 0.6038 (Class: 0.3817, Duration: 2.1663)\n",
      "    Batch [3500/7058] Loss: 0.7678 (Class: 0.5005, Duration: 2.6073)\n",
      "    Batch [3600/7058] Loss: 0.7867 (Class: 0.4718, Duration: 3.0724)\n",
      "    Batch [3700/7058] Loss: 0.7509 (Class: 0.4742, Duration: 2.6988)\n",
      "    Batch [3800/7058] Loss: 0.6749 (Class: 0.4451, Duration: 2.2418)\n",
      "    Batch [3900/7058] Loss: 0.6721 (Class: 0.4373, Duration: 2.2908)\n",
      "    Batch [4000/7058] Loss: 0.8420 (Class: 0.5124, Duration: 3.2159)\n",
      "    Batch [4100/7058] Loss: 0.8630 (Class: 0.4709, Duration: 3.8250)\n",
      "    Batch [4200/7058] Loss: 0.8241 (Class: 0.5036, Duration: 3.1268)\n",
      "    Batch [4300/7058] Loss: 0.7326 (Class: 0.4523, Duration: 2.7348)\n",
      "    Batch [4400/7058] Loss: 0.8103 (Class: 0.5082, Duration: 2.9471)\n",
      "    Batch [4500/7058] Loss: 0.8019 (Class: 0.4352, Duration: 3.5775)\n",
      "    Batch [4600/7058] Loss: 0.8033 (Class: 0.5142, Duration: 2.8211)\n",
      "    Batch [4700/7058] Loss: 0.7277 (Class: 0.4389, Duration: 2.8173)\n",
      "    Batch [4800/7058] Loss: 0.9307 (Class: 0.5843, Duration: 3.3792)\n",
      "    Batch [4900/7058] Loss: 0.6535 (Class: 0.4083, Duration: 2.3928)\n",
      "    Batch [5000/7058] Loss: 0.8651 (Class: 0.5570, Duration: 3.0063)\n",
      "    Batch [5100/7058] Loss: 0.7460 (Class: 0.5000, Duration: 2.4006)\n",
      "    Batch [5200/7058] Loss: 0.6400 (Class: 0.3954, Duration: 2.3859)\n",
      "    Batch [5300/7058] Loss: 0.7651 (Class: 0.5412, Duration: 2.1841)\n",
      "    Batch [5400/7058] Loss: 0.7386 (Class: 0.4385, Duration: 2.9276)\n",
      "    Batch [5500/7058] Loss: 0.6889 (Class: 0.4248, Duration: 2.5760)\n",
      "    Batch [5600/7058] Loss: 0.6454 (Class: 0.3947, Duration: 2.4456)\n",
      "    Batch [5700/7058] Loss: 0.8076 (Class: 0.5259, Duration: 2.7483)\n",
      "    Batch [5800/7058] Loss: 0.7350 (Class: 0.4228, Duration: 3.0462)\n",
      "    Batch [5900/7058] Loss: 0.7853 (Class: 0.4278, Duration: 3.4873)\n",
      "    Batch [6000/7058] Loss: 0.7358 (Class: 0.4815, Duration: 2.4814)\n",
      "    Batch [6100/7058] Loss: 0.7344 (Class: 0.4087, Duration: 3.1780)\n",
      "    Batch [6200/7058] Loss: 0.7507 (Class: 0.4341, Duration: 3.0886)\n",
      "    Batch [6300/7058] Loss: 0.8226 (Class: 0.4921, Duration: 3.2242)\n",
      "    Batch [6400/7058] Loss: 0.7939 (Class: 0.5492, Duration: 2.3871)\n",
      "    Batch [6500/7058] Loss: 0.6158 (Class: 0.3610, Duration: 2.4857)\n",
      "    Batch [6600/7058] Loss: 0.7699 (Class: 0.4874, Duration: 2.7557)\n",
      "    Batch [6700/7058] Loss: 0.7093 (Class: 0.4019, Duration: 2.9990)\n",
      "    Batch [6800/7058] Loss: 0.6756 (Class: 0.4081, Duration: 2.6098)\n",
      "    Batch [6900/7058] Loss: 0.6586 (Class: 0.4626, Duration: 1.9122)\n",
      "    Batch [7000/7058] Loss: 0.7143 (Class: 0.4524, Duration: 2.5559)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.8 GB (+0.2 GB)\n",
      "  Loaded 100/255 files, Memory: 2.7 GB (+1.2 GB)\n",
      "  Loaded 150/255 files, Memory: 3.7 GB (+2.1 GB)\n",
      "  Loaded 200/255 files, Memory: 4.8 GB (+3.3 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.3 GB)\n",
      "Loaded 800392 samples. Memory usage: 6.9 GB (+5.3 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 1.0478 (Class: 0.6830, Duration: 3.5594)\n",
      "    Batch [100/6960] Loss: 0.9642 (Class: 0.6096, Duration: 3.4597)\n",
      "    Batch [200/6960] Loss: 0.7773 (Class: 0.4638, Duration: 3.0591)\n",
      "    Batch [300/6960] Loss: 0.8207 (Class: 0.5201, Duration: 2.9324)\n",
      "    Batch [400/6960] Loss: 0.9186 (Class: 0.5875, Duration: 3.2300)\n",
      "    Batch [500/6960] Loss: 0.7807 (Class: 0.4575, Duration: 3.1526)\n",
      "    Batch [600/6960] Loss: 0.7365 (Class: 0.4569, Duration: 2.7275)\n",
      "    Batch [700/6960] Loss: 0.9545 (Class: 0.6038, Duration: 3.4213)\n",
      "    Batch [800/6960] Loss: 0.7150 (Class: 0.4272, Duration: 2.8076)\n",
      "    Batch [900/6960] Loss: 0.7620 (Class: 0.4826, Duration: 2.7252)\n",
      "    Batch [1000/6960] Loss: 0.8295 (Class: 0.5320, Duration: 2.9021)\n",
      "    Batch [1100/6960] Loss: 0.8878 (Class: 0.4876, Duration: 3.9043)\n",
      "    Batch [1200/6960] Loss: 0.8720 (Class: 0.5077, Duration: 3.5535)\n",
      "    Batch [1300/6960] Loss: 0.7488 (Class: 0.4723, Duration: 2.6976)\n",
      "    Batch [1400/6960] Loss: 0.7386 (Class: 0.4603, Duration: 2.7149)\n",
      "    Batch [1500/6960] Loss: 0.8324 (Class: 0.4393, Duration: 3.8353)\n",
      "    Batch [1600/6960] Loss: 0.7409 (Class: 0.4412, Duration: 2.9236)\n",
      "    Batch [1700/6960] Loss: 0.7182 (Class: 0.4015, Duration: 3.0903)\n",
      "    Batch [1800/6960] Loss: 0.7879 (Class: 0.4821, Duration: 2.9841)\n",
      "    Batch [1900/6960] Loss: 0.7366 (Class: 0.4095, Duration: 3.1913)\n",
      "    Batch [2000/6960] Loss: 0.6680 (Class: 0.4711, Duration: 1.9209)\n",
      "    Batch [2100/6960] Loss: 0.7268 (Class: 0.4681, Duration: 2.5242)\n",
      "    Batch [2200/6960] Loss: 0.7000 (Class: 0.3552, Duration: 3.3638)\n",
      "    Batch [2300/6960] Loss: 0.7758 (Class: 0.4605, Duration: 3.0760)\n",
      "    Batch [2400/6960] Loss: 0.8730 (Class: 0.5222, Duration: 3.4226)\n",
      "    Batch [2500/6960] Loss: 0.7781 (Class: 0.5122, Duration: 2.5950)\n",
      "    Batch [2600/6960] Loss: 0.8998 (Class: 0.5257, Duration: 3.6496)\n",
      "    Batch [2700/6960] Loss: 0.8104 (Class: 0.5210, Duration: 2.8239)\n",
      "    Batch [2800/6960] Loss: 0.9049 (Class: 0.6247, Duration: 2.7337)\n",
      "    Batch [2900/6960] Loss: 0.8144 (Class: 0.5265, Duration: 2.8092)\n",
      "    Batch [3000/6960] Loss: 0.7298 (Class: 0.3852, Duration: 3.3617)\n",
      "    Batch [3100/6960] Loss: 0.6766 (Class: 0.4151, Duration: 2.5519)\n",
      "    Batch [3200/6960] Loss: 0.7886 (Class: 0.4434, Duration: 3.3673)\n",
      "    Batch [3300/6960] Loss: 0.8111 (Class: 0.4809, Duration: 3.2211)\n",
      "    Batch [3400/6960] Loss: 0.6540 (Class: 0.3881, Duration: 2.5938)\n",
      "    Batch [3500/6960] Loss: 0.6203 (Class: 0.3909, Duration: 2.2386)\n",
      "    Batch [3600/6960] Loss: 0.6741 (Class: 0.4288, Duration: 2.3930)\n",
      "    Batch [3700/6960] Loss: 0.8053 (Class: 0.5151, Duration: 2.8310)\n",
      "    Batch [3800/6960] Loss: 0.7133 (Class: 0.4270, Duration: 2.7934)\n",
      "    Batch [3900/6960] Loss: 0.6919 (Class: 0.4113, Duration: 2.7371)\n",
      "    Batch [4000/6960] Loss: 0.7456 (Class: 0.4557, Duration: 2.8287)\n",
      "    Batch [4100/6960] Loss: 0.5510 (Class: 0.3097, Duration: 2.3536)\n",
      "    Batch [4200/6960] Loss: 0.7340 (Class: 0.3716, Duration: 3.5361)\n",
      "    Batch [4300/6960] Loss: 0.7550 (Class: 0.4161, Duration: 3.3069)\n",
      "    Batch [4400/6960] Loss: 0.7316 (Class: 0.3899, Duration: 3.3335)\n",
      "    Batch [4500/6960] Loss: 0.5881 (Class: 0.3821, Duration: 2.0099)\n",
      "    Batch [4600/6960] Loss: 0.7369 (Class: 0.3765, Duration: 3.5158)\n",
      "    Batch [4700/6960] Loss: 0.7348 (Class: 0.4532, Duration: 2.7477)\n",
      "    Batch [4800/6960] Loss: 0.8192 (Class: 0.4441, Duration: 3.6595)\n",
      "    Batch [4900/6960] Loss: 0.6465 (Class: 0.3727, Duration: 2.6717)\n",
      "    Batch [5000/6960] Loss: 0.7034 (Class: 0.4476, Duration: 2.4957)\n",
      "    Batch [5100/6960] Loss: 0.7726 (Class: 0.5410, Duration: 2.2599)\n",
      "    Batch [5200/6960] Loss: 0.6839 (Class: 0.4249, Duration: 2.5266)\n",
      "    Batch [5300/6960] Loss: 0.5329 (Class: 0.3257, Duration: 2.0215)\n",
      "    Batch [5400/6960] Loss: 0.6849 (Class: 0.4483, Duration: 2.3084)\n",
      "    Batch [5500/6960] Loss: 0.6673 (Class: 0.3563, Duration: 3.0334)\n",
      "    Batch [5600/6960] Loss: 0.6398 (Class: 0.3819, Duration: 2.5165)\n",
      "    Batch [5700/6960] Loss: 0.6393 (Class: 0.3822, Duration: 2.5083)\n",
      "    Batch [5800/6960] Loss: 0.5109 (Class: 0.3432, Duration: 1.6357)\n",
      "    Batch [5900/6960] Loss: 0.6515 (Class: 0.3890, Duration: 2.5604)\n",
      "    Batch [6000/6960] Loss: 0.7490 (Class: 0.4132, Duration: 3.2759)\n",
      "    Batch [6100/6960] Loss: 0.5792 (Class: 0.3564, Duration: 2.1732)\n",
      "    Batch [6200/6960] Loss: 0.8759 (Class: 0.5934, Duration: 2.7556)\n",
      "    Batch [6300/6960] Loss: 0.8508 (Class: 0.4834, Duration: 3.5842)\n",
      "    Batch [6400/6960] Loss: 0.6972 (Class: 0.4073, Duration: 2.8278)\n",
      "    Batch [6500/6960] Loss: 0.6553 (Class: 0.4290, Duration: 2.2069)\n",
      "    Batch [6600/6960] Loss: 0.6252 (Class: 0.3927, Duration: 2.2683)\n",
      "    Batch [6700/6960] Loss: 0.6381 (Class: 0.3570, Duration: 2.7433)\n",
      "    Batch [6800/6960] Loss: 0.7857 (Class: 0.4506, Duration: 3.2688)\n",
      "    Batch [6900/6960] Loss: 0.5631 (Class: 0.3683, Duration: 1.9005)\n",
      "\n",
      "Train Loss: 0.7696, Train Acc: 79.93%\n",
      "Val Loss: 1.2648, Val Acc: 71.44%\n",
      "Val Duration MSE: 4.1769 (RMSE: 2.0438)\n",
      "Class Accuracies - Bull: 75.95%, Flat: 65.86%, Bear: 72.38%\n",
      "Learning Rate: 9.95e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  -> New best model saved! (best balanced accuracy)\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.6 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.6 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.9 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 4.7 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 6.7 GB (+5.4 GB)\n",
      "\n",
      "Epoch 14/80\n",
      "Duration weight: 0.107\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 0.9546 (Class: 0.6099, Duration: 3.2251)\n",
      "    Batch [100/7058] Loss: 0.7068 (Class: 0.3834, Duration: 3.0264)\n",
      "    Batch [200/7058] Loss: 0.7260 (Class: 0.4647, Duration: 2.4451)\n",
      "    Batch [300/7058] Loss: 0.8576 (Class: 0.5168, Duration: 3.1895)\n",
      "    Batch [400/7058] Loss: 0.7883 (Class: 0.4643, Duration: 3.0310)\n",
      "    Batch [500/7058] Loss: 0.8605 (Class: 0.6108, Duration: 2.3371)\n",
      "    Batch [600/7058] Loss: 0.7076 (Class: 0.4153, Duration: 2.7350)\n",
      "    Batch [700/7058] Loss: 0.8301 (Class: 0.5551, Duration: 2.5734)\n",
      "    Batch [800/7058] Loss: 0.8552 (Class: 0.4809, Duration: 3.5017)\n",
      "    Batch [900/7058] Loss: 0.7767 (Class: 0.4381, Duration: 3.1686)\n",
      "    Batch [1000/7058] Loss: 0.7315 (Class: 0.5013, Duration: 2.1541)\n",
      "    Batch [1100/7058] Loss: 0.7793 (Class: 0.4243, Duration: 3.3216)\n",
      "    Batch [1200/7058] Loss: 0.8271 (Class: 0.4898, Duration: 3.1556)\n",
      "    Batch [1300/7058] Loss: 0.6071 (Class: 0.3651, Duration: 2.2635)\n",
      "    Batch [1400/7058] Loss: 0.9490 (Class: 0.5537, Duration: 3.6981)\n",
      "    Batch [1500/7058] Loss: 0.7734 (Class: 0.4535, Duration: 2.9928)\n",
      "    Batch [1600/7058] Loss: 0.6464 (Class: 0.3832, Duration: 2.4629)\n",
      "    Batch [1700/7058] Loss: 0.7805 (Class: 0.5311, Duration: 2.3335)\n",
      "    Batch [1800/7058] Loss: 0.6209 (Class: 0.3908, Duration: 2.1530)\n",
      "    Batch [1900/7058] Loss: 0.7973 (Class: 0.4807, Duration: 2.9628)\n",
      "    Batch [2000/7058] Loss: 0.7603 (Class: 0.4289, Duration: 3.1013)\n",
      "    Batch [2100/7058] Loss: 0.6340 (Class: 0.4364, Duration: 1.8487)\n",
      "    Batch [2200/7058] Loss: 0.7662 (Class: 0.4939, Duration: 2.5481)\n",
      "    Batch [2300/7058] Loss: 0.9302 (Class: 0.6575, Duration: 2.5509)\n",
      "    Batch [2400/7058] Loss: 0.7462 (Class: 0.4198, Duration: 3.0542)\n",
      "    Batch [2500/7058] Loss: 0.7389 (Class: 0.4361, Duration: 2.8330)\n",
      "    Batch [2600/7058] Loss: 0.6557 (Class: 0.3516, Duration: 2.8455)\n",
      "    Batch [2700/7058] Loss: 0.9532 (Class: 0.5048, Duration: 4.1954)\n",
      "    Batch [2800/7058] Loss: 0.7843 (Class: 0.4371, Duration: 3.2486)\n",
      "    Batch [2900/7058] Loss: 0.6804 (Class: 0.3695, Duration: 2.9085)\n",
      "    Batch [3000/7058] Loss: 0.7215 (Class: 0.3806, Duration: 3.1899)\n",
      "    Batch [3100/7058] Loss: 0.6789 (Class: 0.4437, Duration: 2.2003)\n",
      "    Batch [3200/7058] Loss: 0.6776 (Class: 0.4217, Duration: 2.3940)\n",
      "    Batch [3300/7058] Loss: 0.6951 (Class: 0.3602, Duration: 3.1333)\n",
      "    Batch [3400/7058] Loss: 0.7112 (Class: 0.4657, Duration: 2.2979)\n",
      "    Batch [3500/7058] Loss: 0.6793 (Class: 0.3945, Duration: 2.6653)\n",
      "    Batch [3600/7058] Loss: 0.7838 (Class: 0.5410, Duration: 2.2721)\n",
      "    Batch [3700/7058] Loss: 0.7323 (Class: 0.4440, Duration: 2.6976)\n",
      "    Batch [3800/7058] Loss: 0.6800 (Class: 0.4186, Duration: 2.4465)\n",
      "    Batch [3900/7058] Loss: 0.6337 (Class: 0.3038, Duration: 3.0869)\n",
      "    Batch [4000/7058] Loss: 0.7701 (Class: 0.4078, Duration: 3.3902)\n",
      "    Batch [4100/7058] Loss: 0.7144 (Class: 0.3890, Duration: 3.0449)\n",
      "    Batch [4200/7058] Loss: 0.7427 (Class: 0.3875, Duration: 3.3237)\n",
      "    Batch [4300/7058] Loss: 0.6359 (Class: 0.3980, Duration: 2.2262)\n",
      "    Batch [4400/7058] Loss: 0.6536 (Class: 0.3819, Duration: 2.5419)\n",
      "    Batch [4500/7058] Loss: 0.8178 (Class: 0.4961, Duration: 3.0096)\n",
      "    Batch [4600/7058] Loss: 0.6728 (Class: 0.4598, Duration: 1.9926)\n",
      "    Batch [4700/7058] Loss: 0.8438 (Class: 0.4724, Duration: 3.4757)\n",
      "    Batch [4800/7058] Loss: 0.7799 (Class: 0.4792, Duration: 2.8135)\n",
      "    Batch [4900/7058] Loss: 0.7104 (Class: 0.3767, Duration: 3.1231)\n",
      "    Batch [5000/7058] Loss: 0.8185 (Class: 0.4404, Duration: 3.5378)\n",
      "    Batch [5100/7058] Loss: 0.7270 (Class: 0.4688, Duration: 2.4159)\n",
      "    Batch [5200/7058] Loss: 0.6718 (Class: 0.3700, Duration: 2.8242)\n",
      "    Batch [5300/7058] Loss: 0.7691 (Class: 0.4718, Duration: 2.7817)\n",
      "    Batch [5400/7058] Loss: 0.7581 (Class: 0.4281, Duration: 3.0879)\n",
      "    Batch [5500/7058] Loss: 0.7109 (Class: 0.3805, Duration: 3.0915)\n",
      "    Batch [5600/7058] Loss: 0.8432 (Class: 0.5101, Duration: 3.1168)\n",
      "    Batch [5700/7058] Loss: 0.6709 (Class: 0.3834, Duration: 2.6906)\n",
      "    Batch [5800/7058] Loss: 0.6780 (Class: 0.3432, Duration: 3.1329)\n",
      "    Batch [5900/7058] Loss: 0.7770 (Class: 0.4558, Duration: 3.0053)\n",
      "    Batch [6000/7058] Loss: 0.7490 (Class: 0.4693, Duration: 2.6178)\n",
      "    Batch [6100/7058] Loss: 0.7975 (Class: 0.4366, Duration: 3.3769)\n",
      "    Batch [6200/7058] Loss: 0.8025 (Class: 0.5182, Duration: 2.6602)\n",
      "    Batch [6300/7058] Loss: 0.7136 (Class: 0.4375, Duration: 2.5831)\n",
      "    Batch [6400/7058] Loss: 0.6174 (Class: 0.3287, Duration: 2.7011)\n",
      "    Batch [6500/7058] Loss: 0.7728 (Class: 0.4334, Duration: 3.1749)\n",
      "    Batch [6600/7058] Loss: 0.7640 (Class: 0.3422, Duration: 3.9465)\n",
      "    Batch [6700/7058] Loss: 0.5774 (Class: 0.3118, Duration: 2.4848)\n",
      "    Batch [6800/7058] Loss: 0.6447 (Class: 0.3196, Duration: 3.0423)\n",
      "    Batch [6900/7058] Loss: 0.7747 (Class: 0.4201, Duration: 3.3186)\n",
      "    Batch [7000/7058] Loss: 0.7937 (Class: 0.5120, Duration: 2.6361)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.7 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 2.7 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 3.7 GB (+2.4 GB)\n",
      "  Loaded 200/255 files, Memory: 4.8 GB (+3.6 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.6 GB)\n",
      "Loaded 800392 samples. Memory usage: 6.6 GB (+5.3 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 0.9060 (Class: 0.5145, Duration: 3.6632)\n",
      "    Batch [100/6960] Loss: 0.9079 (Class: 0.5044, Duration: 3.7756)\n",
      "    Batch [200/6960] Loss: 0.7262 (Class: 0.4895, Duration: 2.2146)\n",
      "    Batch [300/6960] Loss: 0.6580 (Class: 0.4459, Duration: 1.9850)\n",
      "    Batch [400/6960] Loss: 0.7215 (Class: 0.4125, Duration: 2.8911)\n",
      "    Batch [500/6960] Loss: 0.8237 (Class: 0.5482, Duration: 2.5783)\n",
      "    Batch [600/6960] Loss: 0.9351 (Class: 0.5334, Duration: 3.7591)\n",
      "    Batch [700/6960] Loss: 0.7133 (Class: 0.4615, Duration: 2.3561)\n",
      "    Batch [800/6960] Loss: 0.6120 (Class: 0.3760, Duration: 2.2086)\n",
      "    Batch [900/6960] Loss: 0.7179 (Class: 0.4125, Duration: 2.8575)\n",
      "    Batch [1000/6960] Loss: 0.8594 (Class: 0.5887, Duration: 2.5323)\n",
      "    Batch [1100/6960] Loss: 0.9091 (Class: 0.5271, Duration: 3.5740)\n",
      "    Batch [1200/6960] Loss: 0.7382 (Class: 0.4883, Duration: 2.3379)\n",
      "    Batch [1300/6960] Loss: 0.4671 (Class: 0.2629, Duration: 1.9108)\n",
      "    Batch [1400/6960] Loss: 0.6884 (Class: 0.3870, Duration: 2.8198)\n",
      "    Batch [1500/6960] Loss: 0.6464 (Class: 0.3951, Duration: 2.3510)\n",
      "    Batch [1600/6960] Loss: 0.7852 (Class: 0.4752, Duration: 2.9006)\n",
      "    Batch [1700/6960] Loss: 0.8521 (Class: 0.5909, Duration: 2.4436)\n",
      "    Batch [1800/6960] Loss: 0.6912 (Class: 0.4593, Duration: 2.1692)\n",
      "    Batch [1900/6960] Loss: 0.6917 (Class: 0.4623, Duration: 2.1466)\n",
      "    Batch [2000/6960] Loss: 0.7302 (Class: 0.3904, Duration: 3.1793)\n",
      "    Batch [2100/6960] Loss: 0.7515 (Class: 0.4262, Duration: 3.0437)\n",
      "    Batch [2200/6960] Loss: 0.6854 (Class: 0.3968, Duration: 2.6999)\n",
      "    Batch [2300/6960] Loss: 0.6255 (Class: 0.3568, Duration: 2.5139)\n",
      "    Batch [2400/6960] Loss: 0.7000 (Class: 0.3923, Duration: 2.8785)\n",
      "    Batch [2500/6960] Loss: 0.7442 (Class: 0.4476, Duration: 2.7760)\n",
      "    Batch [2600/6960] Loss: 0.7081 (Class: 0.3938, Duration: 2.9401)\n",
      "    Batch [2700/6960] Loss: 0.7353 (Class: 0.4635, Duration: 2.5431)\n",
      "    Batch [2800/6960] Loss: 0.6726 (Class: 0.4279, Duration: 2.2892)\n",
      "    Batch [2900/6960] Loss: 0.7972 (Class: 0.4394, Duration: 3.3476)\n",
      "    Batch [3000/6960] Loss: 0.7080 (Class: 0.4044, Duration: 2.8411)\n",
      "    Batch [3100/6960] Loss: 0.7050 (Class: 0.4326, Duration: 2.5491)\n",
      "    Batch [3200/6960] Loss: 0.6097 (Class: 0.3458, Duration: 2.4696)\n",
      "    Batch [3300/6960] Loss: 0.7155 (Class: 0.4068, Duration: 2.8889)\n",
      "    Batch [3400/6960] Loss: 0.6539 (Class: 0.3581, Duration: 2.7677)\n",
      "    Batch [3500/6960] Loss: 0.6286 (Class: 0.4007, Duration: 2.1325)\n",
      "    Batch [3600/6960] Loss: 0.5446 (Class: 0.3261, Duration: 2.0445)\n",
      "    Batch [3700/6960] Loss: 0.6695 (Class: 0.4031, Duration: 2.4920)\n",
      "    Batch [3800/6960] Loss: 0.7479 (Class: 0.4114, Duration: 3.1485)\n",
      "    Batch [3900/6960] Loss: 0.6844 (Class: 0.3741, Duration: 2.9034)\n",
      "    Batch [4000/6960] Loss: 0.6626 (Class: 0.3901, Duration: 2.5500)\n",
      "    Batch [4100/6960] Loss: 0.6502 (Class: 0.4187, Duration: 2.1663)\n",
      "    Batch [4200/6960] Loss: 0.5846 (Class: 0.3270, Duration: 2.4100)\n",
      "    Batch [4300/6960] Loss: 0.4956 (Class: 0.2707, Duration: 2.1037)\n",
      "    Batch [4400/6960] Loss: 0.5402 (Class: 0.2935, Duration: 2.3077)\n",
      "    Batch [4500/6960] Loss: 0.7093 (Class: 0.3774, Duration: 3.1059)\n",
      "    Batch [4600/6960] Loss: 0.6284 (Class: 0.3163, Duration: 2.9204)\n",
      "    Batch [4700/6960] Loss: 0.6318 (Class: 0.3850, Duration: 2.3089)\n",
      "    Batch [4800/6960] Loss: 0.6213 (Class: 0.4008, Duration: 2.0632)\n",
      "    Batch [4900/6960] Loss: 0.5976 (Class: 0.2763, Duration: 3.0064)\n",
      "    Batch [5000/6960] Loss: 0.6139 (Class: 0.3750, Duration: 2.2350)\n",
      "    Batch [5100/6960] Loss: 0.6841 (Class: 0.3859, Duration: 2.7904)\n",
      "    Batch [5200/6960] Loss: 0.6258 (Class: 0.3327, Duration: 2.7424)\n",
      "    Batch [5300/6960] Loss: 0.6544 (Class: 0.4125, Duration: 2.2627)\n",
      "    Batch [5400/6960] Loss: 0.8317 (Class: 0.4873, Duration: 3.2231)\n",
      "    Batch [5500/6960] Loss: 0.6629 (Class: 0.4160, Duration: 2.3096)\n",
      "    Batch [5600/6960] Loss: 0.7412 (Class: 0.4707, Duration: 2.5316)\n",
      "    Batch [5700/6960] Loss: 0.6791 (Class: 0.3976, Duration: 2.6341)\n",
      "    Batch [5800/6960] Loss: 0.5605 (Class: 0.2977, Duration: 2.4588)\n",
      "    Batch [5900/6960] Loss: 0.6642 (Class: 0.3773, Duration: 2.6843)\n",
      "    Batch [6000/6960] Loss: 0.6167 (Class: 0.2740, Duration: 3.2062)\n",
      "    Batch [6100/6960] Loss: 0.6828 (Class: 0.4446, Duration: 2.2296)\n",
      "    Batch [6200/6960] Loss: 0.7585 (Class: 0.4021, Duration: 3.3353)\n",
      "    Batch [6300/6960] Loss: 0.5691 (Class: 0.4119, Duration: 1.4708)\n",
      "    Batch [6400/6960] Loss: 0.5513 (Class: 0.3429, Duration: 1.9498)\n",
      "    Batch [6500/6960] Loss: 0.5883 (Class: 0.3661, Duration: 2.0792)\n",
      "    Batch [6600/6960] Loss: 0.5333 (Class: 0.2845, Duration: 2.3283)\n",
      "    Batch [6700/6960] Loss: 0.6255 (Class: 0.3947, Duration: 2.1597)\n",
      "    Batch [6800/6960] Loss: 0.7739 (Class: 0.4187, Duration: 3.3233)\n",
      "    Batch [6900/6960] Loss: 0.6544 (Class: 0.3875, Duration: 2.4976)\n",
      "\n",
      "Train Loss: 0.7355, Train Acc: 81.46%\n",
      "Val Loss: 1.3953, Val Acc: 70.66%\n",
      "Val Duration MSE: 4.2783 (RMSE: 2.0684)\n",
      "Class Accuracies - Bull: 76.56%, Flat: 59.94%, Bear: 76.95%\n",
      "Learning Rate: 9.92e-05\n",
      "GPU Memory: 0.42 GB allocated\n",
      "  Resetting to first half for next epoch...\n",
      "Loading first half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.6 GB (+0.3 GB)\n",
      "  Loaded 100/255 files, Memory: 2.6 GB (+1.4 GB)\n",
      "  Loaded 150/255 files, Memory: 3.9 GB (+2.6 GB)\n",
      "  Loaded 200/255 files, Memory: 4.7 GB (+3.4 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.5 GB)\n",
      "Loaded 811636 samples. Memory usage: 6.7 GB (+5.4 GB)\n",
      "\n",
      "Epoch 15/80\n",
      "Duration weight: 0.111\n",
      "--------------------------------------------------\n",
      "  Training on first half (811636 samples)...\n",
      "    Batch [0/7058] Loss: 1.1182 (Class: 0.7158, Duration: 3.6178)\n",
      "    Batch [100/7058] Loss: 0.9044 (Class: 0.5399, Duration: 3.2764)\n",
      "    Batch [200/7058] Loss: 0.8904 (Class: 0.5338, Duration: 3.2055)\n",
      "    Batch [300/7058] Loss: 0.7210 (Class: 0.4118, Duration: 2.7793)\n",
      "    Batch [400/7058] Loss: 0.8972 (Class: 0.4942, Duration: 3.6226)\n",
      "    Batch [500/7058] Loss: 0.7812 (Class: 0.4765, Duration: 2.7389)\n",
      "    Batch [600/7058] Loss: 0.7453 (Class: 0.4845, Duration: 2.3445)\n",
      "    Batch [700/7058] Loss: 0.7120 (Class: 0.4278, Duration: 2.5544)\n",
      "    Batch [800/7058] Loss: 0.6864 (Class: 0.3913, Duration: 2.6530)\n",
      "    Batch [900/7058] Loss: 0.7670 (Class: 0.4996, Duration: 2.4036)\n",
      "    Batch [1000/7058] Loss: 0.7580 (Class: 0.4342, Duration: 2.9104)\n",
      "    Batch [1100/7058] Loss: 0.8161 (Class: 0.4842, Duration: 2.9839)\n",
      "    Batch [1200/7058] Loss: 0.9306 (Class: 0.5851, Duration: 3.1049)\n",
      "    Batch [1300/7058] Loss: 0.8750 (Class: 0.5262, Duration: 3.1349)\n",
      "    Batch [1400/7058] Loss: 0.6092 (Class: 0.3651, Duration: 2.1945)\n",
      "    Batch [1500/7058] Loss: 0.6585 (Class: 0.3822, Duration: 2.4842)\n",
      "    Batch [1600/7058] Loss: 0.6909 (Class: 0.4293, Duration: 2.3511)\n",
      "    Batch [1700/7058] Loss: 0.7011 (Class: 0.4245, Duration: 2.4859)\n",
      "    Batch [1800/7058] Loss: 0.8030 (Class: 0.4569, Duration: 3.1108)\n",
      "    Batch [1900/7058] Loss: 0.8758 (Class: 0.5126, Duration: 3.2645)\n",
      "    Batch [2000/7058] Loss: 0.6440 (Class: 0.3849, Duration: 2.3286)\n",
      "    Batch [2100/7058] Loss: 0.6146 (Class: 0.3623, Duration: 2.2680)\n",
      "    Batch [2200/7058] Loss: 0.6724 (Class: 0.3825, Duration: 2.6063)\n",
      "    Batch [2300/7058] Loss: 0.6564 (Class: 0.3916, Duration: 2.3796)\n",
      "    Batch [2400/7058] Loss: 0.7788 (Class: 0.5111, Duration: 2.4069)\n",
      "    Batch [2500/7058] Loss: 0.7761 (Class: 0.4857, Duration: 2.6101)\n",
      "    Batch [2600/7058] Loss: 0.5532 (Class: 0.3375, Duration: 1.9392)\n",
      "    Batch [2700/7058] Loss: 0.7845 (Class: 0.4922, Duration: 2.6270)\n",
      "    Batch [2800/7058] Loss: 0.6871 (Class: 0.3298, Duration: 3.2111)\n",
      "    Batch [2900/7058] Loss: 0.6978 (Class: 0.4282, Duration: 2.4237)\n",
      "    Batch [3000/7058] Loss: 0.8714 (Class: 0.5162, Duration: 3.1933)\n",
      "    Batch [3100/7058] Loss: 0.7983 (Class: 0.4472, Duration: 3.1556)\n",
      "    Batch [3200/7058] Loss: 0.7791 (Class: 0.5132, Duration: 2.3902)\n",
      "    Batch [3300/7058] Loss: 0.7591 (Class: 0.3803, Duration: 3.4049)\n",
      "    Batch [3400/7058] Loss: 0.6063 (Class: 0.4247, Duration: 1.6323)\n",
      "    Batch [3500/7058] Loss: 0.6012 (Class: 0.3256, Duration: 2.4770)\n",
      "    Batch [3600/7058] Loss: 0.7827 (Class: 0.5237, Duration: 2.3283)\n",
      "    Batch [3700/7058] Loss: 0.7159 (Class: 0.4562, Duration: 2.3347)\n",
      "    Batch [3800/7058] Loss: 0.7429 (Class: 0.4192, Duration: 2.9098)\n",
      "    Batch [3900/7058] Loss: 0.6509 (Class: 0.3619, Duration: 2.5977)\n",
      "    Batch [4000/7058] Loss: 0.8777 (Class: 0.4940, Duration: 3.4494)\n",
      "    Batch [4100/7058] Loss: 0.5554 (Class: 0.3302, Duration: 2.0239)\n",
      "    Batch [4200/7058] Loss: 0.7880 (Class: 0.4921, Duration: 2.6592)\n",
      "    Batch [4300/7058] Loss: 0.5932 (Class: 0.3199, Duration: 2.4565)\n",
      "    Batch [4400/7058] Loss: 0.6701 (Class: 0.3293, Duration: 3.0628)\n",
      "    Batch [4500/7058] Loss: 0.8340 (Class: 0.4900, Duration: 3.0923)\n",
      "    Batch [4600/7058] Loss: 0.6912 (Class: 0.4098, Duration: 2.5297)\n",
      "    Batch [4700/7058] Loss: 0.5499 (Class: 0.3377, Duration: 1.9079)\n",
      "    Batch [4800/7058] Loss: 0.6273 (Class: 0.3764, Duration: 2.2555)\n",
      "    Batch [4900/7058] Loss: 0.6177 (Class: 0.2782, Duration: 3.0516)\n",
      "    Batch [5000/7058] Loss: 0.6314 (Class: 0.3435, Duration: 2.5872)\n",
      "    Batch [5100/7058] Loss: 0.7449 (Class: 0.4436, Duration: 2.7085)\n",
      "    Batch [5200/7058] Loss: 0.8251 (Class: 0.3913, Duration: 3.8998)\n",
      "    Batch [5300/7058] Loss: 0.7146 (Class: 0.4334, Duration: 2.5276)\n",
      "    Batch [5400/7058] Loss: 0.5392 (Class: 0.3488, Duration: 1.7110)\n",
      "    Batch [5500/7058] Loss: 0.7065 (Class: 0.3266, Duration: 3.4147)\n",
      "    Batch [5600/7058] Loss: 0.6152 (Class: 0.3577, Duration: 2.3146)\n",
      "    Batch [5700/7058] Loss: 0.6091 (Class: 0.3678, Duration: 2.1694)\n",
      "    Batch [5800/7058] Loss: 0.7172 (Class: 0.4134, Duration: 2.7300)\n",
      "    Batch [5900/7058] Loss: 0.8062 (Class: 0.4689, Duration: 3.0326)\n",
      "    Batch [6000/7058] Loss: 0.5580 (Class: 0.3228, Duration: 2.1144)\n",
      "    Batch [6100/7058] Loss: 0.6245 (Class: 0.3793, Duration: 2.2034)\n",
      "    Batch [6200/7058] Loss: 0.6580 (Class: 0.3574, Duration: 2.7018)\n",
      "    Batch [6300/7058] Loss: 0.6937 (Class: 0.4122, Duration: 2.5302)\n",
      "    Batch [6400/7058] Loss: 0.6081 (Class: 0.3451, Duration: 2.3645)\n",
      "    Batch [6500/7058] Loss: 0.6989 (Class: 0.3484, Duration: 3.1503)\n",
      "    Batch [6600/7058] Loss: 0.6612 (Class: 0.3392, Duration: 2.8951)\n",
      "    Batch [6700/7058] Loss: 0.6989 (Class: 0.3491, Duration: 3.1442)\n",
      "    Batch [6800/7058] Loss: 0.7897 (Class: 0.4285, Duration: 3.2464)\n",
      "    Batch [6900/7058] Loss: 0.6404 (Class: 0.3595, Duration: 2.5247)\n",
      "    Batch [7000/7058] Loss: 0.6375 (Class: 0.3770, Duration: 2.3422)\n",
      "  Switching to second half of data...\n",
      "Loading second half: 255 files from train set...\n",
      "  Loaded 50/255 files, Memory: 1.7 GB (+0.4 GB)\n",
      "  Loaded 100/255 files, Memory: 2.7 GB (+1.5 GB)\n",
      "  Loaded 150/255 files, Memory: 3.7 GB (+2.4 GB)\n",
      "  Loaded 200/255 files, Memory: 4.8 GB (+3.6 GB)\n",
      "  Loaded 250/255 files, Memory: 5.8 GB (+4.6 GB)\n",
      "Loaded 800392 samples. Memory usage: 6.6 GB (+5.3 GB)\n",
      "  Training on second half (800392 samples)...\n",
      "    Batch [0/6960] Loss: 0.9127 (Class: 0.5288, Duration: 3.4507)\n",
      "    Batch [100/6960] Loss: 0.8112 (Class: 0.5383, Duration: 2.4533)\n",
      "    Batch [200/6960] Loss: 0.7412 (Class: 0.4081, Duration: 2.9937)\n",
      "    Batch [300/6960] Loss: 0.6728 (Class: 0.3864, Duration: 2.5744)\n",
      "    Batch [400/6960] Loss: 0.8096 (Class: 0.5077, Duration: 2.7137)\n",
      "    Batch [500/6960] Loss: 0.8470 (Class: 0.5223, Duration: 2.9185)\n",
      "    Batch [600/6960] Loss: 0.8816 (Class: 0.4956, Duration: 3.4699)\n",
      "    Batch [700/6960] Loss: 0.6788 (Class: 0.4127, Duration: 2.3923)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Train with data half switching\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m train_loss, train_acc = \u001b[43mtrain_epoch_with_halves\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLASS_WEIGHT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     33\u001b[39m val_loss, val_acc, class_accuracies, val_duration_mse = validate(\n\u001b[32m     34\u001b[39m     model, val_loader, criterion_class, criterion_duration, \n\u001b[32m     35\u001b[39m     DEVICE, CLASS_WEIGHT, duration_weight\n\u001b[32m     36\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mtrain_epoch_with_halves\u001b[39m\u001b[34m(model, train_dataset, batch_size, criterion_class, criterion_duration, optimizer, device, class_weight, duration_weight, epoch)\u001b[39m\n\u001b[32m     90\u001b[39m loss_duration = criterion_duration(duration_pred, target_duration)\n\u001b[32m     92\u001b[39m loss = class_weight * loss_class + duration_weight * loss_duration\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     96\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/trends/.venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/trends/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/trends/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_duration_mse': [],\n",
    "    'learning_rate': [],\n",
    "    'duration_weight': []\n",
    "}\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_balanced_accuracy = 0\n",
    "best_duration_mse = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Gradually increase duration weight\n",
    "    progress = epoch / NUM_EPOCHS\n",
    "    duration_weight = DURATION_WEIGHT_START + (DURATION_WEIGHT_END - DURATION_WEIGHT_START) * progress\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"Duration weight: {duration_weight:.3f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train with data half switching\n",
    "    train_loss, train_acc = train_epoch_with_halves(\n",
    "        model, train_dataset, BATCH_SIZE, criterion_class, criterion_duration, \n",
    "        optimizer, DEVICE, CLASS_WEIGHT, duration_weight, epoch\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, class_accuracies, val_duration_mse = validate(\n",
    "        model, val_loader, criterion_class, criterion_duration, \n",
    "        DEVICE, CLASS_WEIGHT, duration_weight\n",
    "    )\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Val Duration MSE: {val_duration_mse:.4f} (RMSE: {np.sqrt(val_duration_mse):.4f})\")\n",
    "    print(f\"Class Accuracies - Bull: {class_accuracies['bull']:.2f}%, \"\n",
    "          f\"Flat: {class_accuracies['flat']:.2f}%, Bear: {class_accuracies['bear']:.2f}%\")\n",
    "    print(f\"Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    if DEVICE.type == 'cuda':\n",
    "        print(f\"GPU Memory: {round(torch.cuda.memory_allocated(0)/1024**3,2)} GB allocated\")\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_duration_mse'].append(val_duration_mse)\n",
    "    history['learning_rate'].append(current_lr)\n",
    "    history['duration_weight'].append(duration_weight)\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    balanced_accuracy = (class_accuracies['bull'] + class_accuracies['flat'] + class_accuracies['bear']) / 3\n",
    "    \n",
    "    # Save best model based on multiple criteria\n",
    "    save_model = False\n",
    "    save_reason = \"\"\n",
    "    \n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        save_model = True\n",
    "        save_reason = \"best overall accuracy\"\n",
    "    \n",
    "    if balanced_accuracy > best_balanced_accuracy and val_duration_mse < 15:\n",
    "        best_balanced_accuracy = balanced_accuracy\n",
    "        save_model = True\n",
    "        save_reason = \"best balanced accuracy\"\n",
    "    \n",
    "    if val_duration_mse < best_duration_mse and val_acc > 65:\n",
    "        best_duration_mse = val_duration_mse\n",
    "        save_model = True\n",
    "        save_reason = \"best duration MSE\"\n",
    "    \n",
    "    if save_model:\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'val_duration_mse': val_duration_mse,\n",
    "            'class_accuracies': class_accuracies,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'duration_weight': duration_weight,\n",
    "            'lookback_window': LOOKBACK_WINDOW\n",
    "        }, 'checkpoints/best_model.pth')\n",
    "        print(f\"  -> New best model saved! ({save_reason})\")\n",
    "    \n",
    "    # Switch back to first half for next epoch\n",
    "    if train_dataset.half == 'second':\n",
    "        print(\"  Resetting to first half for next epoch...\")\n",
    "        train_dataset.switch_half()\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "print(f\"Best balanced accuracy: {best_balanced_accuracy:.2f}%\")\n",
    "print(f\"Best duration MSE: {best_duration_mse:.4f} (RMSE: {np.sqrt(best_duration_mse):.4f})\")\n",
    "print(f\"Best model saved at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history['train_acc'], label='Train Accuracy')\n",
    "axes[0, 1].plot(history['val_acc'], label='Val Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Duration MSE\n",
    "axes[1, 0].plot(history['val_duration_mse'], label='Val Duration MSE', color='green')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('MSE')\n",
    "axes[1, 0].set_title('Validation Duration MSE')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Learning Rate and Duration Weight\n",
    "ax1 = axes[1, 1]\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(history['learning_rate'], label='Learning Rate', color='blue')\n",
    "ax2.plot(history['duration_weight'], label='Duration Weight', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Learning Rate', color='blue')\n",
    "ax2.set_ylabel('Duration Weight', color='red')\n",
    "ax1.set_title('Learning Rate and Duration Weight Schedule')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax1.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from epoch 13\n",
      "Best model metrics:\n",
      "  Validation Accuracy: 71.44%\n",
      "  Validation Loss: 1.2648\n",
      "  Duration MSE: 4.1769\n",
      "  Balanced Accuracy: 71.40%\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('checkpoints/best_model.pth', map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "print(f\"Best model metrics:\")\n",
    "print(f\"  Validation Accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "print(f\"  Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"  Duration MSE: {checkpoint['val_duration_mse']:.4f}\")\n",
    "print(f\"  Balanced Accuracy: {checkpoint['balanced_accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation Performance:\n",
      "Overall Accuracy: 71.44%\n",
      "\n",
      "Per-Class Accuracy:\n",
      "  Bull: 75.95%\n",
      "  Flat: 65.86%\n",
      "  Bear: 72.38%\n",
      "\n",
      "Duration Prediction:\n",
      "  MSE: 4.1769\n",
      "  RMSE: 2.0438 (≈ 3.8 days error)\n"
     ]
    }
   ],
   "source": [
    "# Run final validation\n",
    "val_loss, val_acc, class_accuracies, val_duration_mse = validate(\n",
    "    model, val_loader, criterion_class, criterion_duration, \n",
    "    DEVICE, CLASS_WEIGHT, checkpoint['duration_weight']\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Validation Performance:\")\n",
    "print(f\"Overall Accuracy: {val_acc:.2f}%\")\n",
    "print(f\"\\nPer-Class Accuracy:\")\n",
    "print(f\"  Bull: {class_accuracies['bull']:.2f}%\")\n",
    "print(f\"  Flat: {class_accuracies['flat']:.2f}%\")\n",
    "print(f\"  Bear: {class_accuracies['bear']:.2f}%\")\n",
    "print(f\"\\nDuration Prediction:\")\n",
    "print(f\"  MSE: {val_duration_mse:.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(val_duration_mse):.4f} (≈ {np.sqrt(val_duration_mse) / 0.545:.1f} days error)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training summary saved to training_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Save training summary\n",
    "summary = {\n",
    "    'config': {\n",
    "        'lookback_window': LOOKBACK_WINDOW,\n",
    "        'n_features': N_FEATURES,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'model_params': sum(p.numel() for p in model.parameters())\n",
    "    },\n",
    "    'best_results': {\n",
    "        'epoch': best_epoch,\n",
    "        'val_accuracy': best_val_accuracy,\n",
    "        'balanced_accuracy': best_balanced_accuracy,\n",
    "        'duration_mse': best_duration_mse,\n",
    "        'duration_rmse': np.sqrt(best_duration_mse)\n",
    "    },\n",
    "    'final_performance': {\n",
    "        'overall_accuracy': val_acc,\n",
    "        'class_accuracies': class_accuracies,\n",
    "        'duration_mse': val_duration_mse\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nTraining summary saved to training_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
